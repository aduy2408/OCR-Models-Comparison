{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Models Evaluation on French and English Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:09:50.266069Z",
     "iopub.status.busy": "2025-07-09T17:09:50.265712Z",
     "iopub.status.idle": "2025-07-09T17:09:50.269456Z",
     "shell.execute_reply": "2025-07-09T17:09:50.268822Z",
     "shell.execute_reply.started": "2025-07-09T17:09:50.266043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install pytesseract easyocr paddlepaddle paddleocr kraken pillow opencv-python\n",
    "# !pip install google-cloud-vision  \n",
    "# !pip install lxml beautifulsoup4  # For XML parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surya-ocr\n",
      "  Downloading surya_ocr-0.14.6-py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.3/177.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pypdfium2==4.30.0\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3.0.0,>=2.5.3\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops<0.9.0,>=0.8.1\n",
      "  Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filetype<2.0.0,>=1.2.0\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.1.0\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs<5.0.0,>=4.3.6 in /home/duyle/.pyenv/versions/3.10/lib/python3.10/site-packages (from surya-ocr) (4.3.8)\n",
      "Collecting pillow<11.0.0,>=10.2.0\n",
      "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=8.1.8 in /home/duyle/.pyenv/versions/3.10/lib/python3.10/site-packages (from surya-ocr) (8.1.8)\n",
      "Collecting opencv-python-headless<5.0.0.0,>=4.11.0.86\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch<3.0.0,>=2.7.0\n",
      "  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.51.2\n",
      "  Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv<2.0.0,>=1.0.0\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting pre-commit<5.0.0,>=4.2.0\n",
      "  Downloading pre_commit-4.2.0-py2.py3-none-any.whl (220 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.7/220.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2.3.0,>=2\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nodeenv>=0.11.1\n",
      "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Collecting cfgv>=2.0.0\n",
      "  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Collecting virtualenv>=20.10.0\n",
      "  Downloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting identify>=1.0.0\n",
      "  Downloading identify-2.6.12-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/duyle/.pyenv/versions/3.10/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.3->surya-ocr) (4.14.0)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.5.1.17\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.6.77\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.3.1\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.11.1.6\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/duyle/.pyenv/versions/3.10/lib/python3.10/site-packages (from torch<3.0.0,>=2.7.0->surya-ocr) (3.1.6)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.0.4\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /home/duyle/.pyenv/versions/3.10/lib/python3.10/site-packages (from triton==3.3.1->torch<3.0.0,>=2.7.0->surya-ocr) (65.5.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0\n",
      "  Downloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/duyle/.pyenv/versions/3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.51.2->surya-ocr) (25.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting distlib<1,>=0.3.7\n",
      "  Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/duyle/.pyenv/versions/3.10/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.7.0->surya-ocr) (3.0.2)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.7.9-py3-none-any.whl (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, filetype, distlib, urllib3, typing-inspection, triton, tqdm, sympy, safetensors, regex, pyyaml, python-dotenv, pypdfium2, pydantic-core, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nodeenv, networkx, idna, identify, hf-xet, fsspec, filelock, einops, charset_normalizer, cfgv, certifi, annotated-types, virtualenv, requests, pydantic, opencv-python-headless, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, pydantic-settings, pre-commit, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, surya-ocr\n",
      "Successfully installed annotated-types-0.7.0 certifi-2025.7.9 cfgv-3.4.0 charset_normalizer-3.4.2 distlib-0.3.9 einops-0.8.1 filelock-3.18.0 filetype-1.2.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface-hub-0.33.2 identify-2.6.12 idna-3.10 mpmath-1.3.0 networkx-3.4.2 nodeenv-1.9.1 numpy-2.2.6 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opencv-python-headless-4.12.0.88 pillow-10.4.0 pre-commit-4.2.0 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.10.1 pypdfium2-4.30.0 python-dotenv-1.1.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 surya-ocr-0.14.6 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 tqdm-4.67.1 transformers-4.53.1 triton-3.3.1 typing-inspection-0.4.1 urllib3-2.5.0 virtualenv-20.31.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install surya-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading text_recognition model to /home/duyle/.cache/datalab/models/text_recognition/2025_05_16: 100%|██████████| 10/10 [00:45<00:00,  4.55s/it]\n",
      "Downloading text_detection model to /home/duyle/.cache/datalab/models/text_detection/2025_05_07: 100%|██████████| 6/6 [00:49<00:00,  8.27s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:03<00:00,  3.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m recognition_predictor \u001b[38;5;241m=\u001b[39m RecognitionPredictor()\n\u001b[1;32m      7\u001b[0m detection_predictor \u001b[38;5;241m=\u001b[39m DetectionPredictor()\n\u001b[0;32m----> 9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mrecognition_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_predictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetection_predictor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/surya/recognition/__init__.py:857\u001b[0m, in \u001b[0;36mRecognitionPredictor.__call__\u001b[0;34m(self, images, task_names, det_predictor, detection_batch_size, recognition_batch_size, highres_images, bboxes, polygons, input_text, sort_lines, math_mode, return_words, drop_repeated_text)\u001b[0m\n\u001b[1;32m    854\u001b[0m flat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [flat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_names\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m predicted_tokens, batch_bboxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecognition_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecognition_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmath_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath_mode\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# Get text and bboxes in structured form\u001b[39;00m\n\u001b[1;32m    862\u001b[0m bbox_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbbox_size\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/surya/recognition/__init__.py:566\u001b[0m, in \u001b[0;36mRecognitionPredictor.prediction_loop\u001b[0;34m(self, flat, recognition_batch_size, math_mode)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     updated_inputs, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;66;03m# TODO Find a cleaner way of popping from the dict\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     predicted_tokens_cpu \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpreds\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/surya/recognition/__init__.py:332\u001b[0m, in \u001b[0;36mRecognitionPredictor.decode\u001b[0;34m(self, current_inputs)\u001b[0m\n\u001b[1;32m    329\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m current_inputs\u001b[38;5;241m.\u001b[39mposition_ids\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mINFERENCE_MODE():\n\u001b[0;32m--> 332\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m processed_output: ContinuousBatchOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_outputs(outputs)\n\u001b[1;32m    343\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(attention_mask, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/surya/common/surya/__init__.py:330\u001b[0m, in \u001b[0;36mSuryaModel.forward\u001b[0;34m(self, input_ids, image_tiles, grid_thw, inputs_embeds, attention_mask, position_ids, past_key_values, output_hidden_states, output_attentions, use_cache, logits_to_keep, encoder_chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    329\u001b[0m bbox_logits \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_head(hidden_states))\n\u001b[0;32m--> 330\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SuryaModelOutput(\n\u001b[1;32m    333\u001b[0m     bbox_logits\u001b[38;5;241m=\u001b[39mbbox_logits,\n\u001b[1;32m    334\u001b[0m     lm_logits\u001b[38;5;241m=\u001b[39mlm_logits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mpast_key_values,\n\u001b[1;32m    338\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from surya.recognition import RecognitionPredictor\n",
    "from surya.detection import DetectionPredictor\n",
    "\n",
    "image = Image.open('//home/duyle/Documents/Case-Study2/English_OCR_dataset/images/91939637.png')\n",
    "recognition_predictor = RecognitionPredictor()\n",
    "detection_predictor = DetectionPredictor()\n",
    "\n",
    "predictions = recognition_predictor([image], det_predictor=detection_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:34:01.583340Z",
     "iopub.status.busy": "2025-07-09T17:34:01.582750Z",
     "iopub.status.idle": "2025-07-09T17:34:01.602998Z",
     "shell.execute_reply": "2025-07-09T17:34:01.602004Z",
     "shell.execute_reply.started": "2025-07-09T17:34:01.583317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddleocr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1002129978.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# import easyocr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpaddleocr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPaddleOCR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# import kraken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paddleocr'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OCR Libraries\n",
    "# import pytesseract\n",
    "# import easyocr\n",
    "\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# import kraken\n",
    "# from kraken import pageseg\n",
    "# from kraken import rpred\n",
    "\n",
    "\n",
    "# from google.cloud import vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-09T17:37:19.210211Z",
     "iopub.status.busy": "2025-07-09T17:37:19.209368Z",
     "iopub.status.idle": "2025-07-09T17:38:41.367921Z",
     "shell.execute_reply": "2025-07-09T17:38:41.367068Z",
     "shell.execute_reply.started": "2025-07-09T17:37:19.210174Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-doctr[torch]\n",
      "  Downloading python_doctr-1.0.0-py3-none-any.whl.metadata (32 kB)\n",
      "\u001b[33mWARNING: python-doctr 1.0.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (0.21.0+cu124)\n",
      "Requirement already satisfied: onnx<3.0.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (1.18.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (1.26.4)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (1.15.3)\n",
      "Requirement already satisfied: h5py<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (3.14.0)\n",
      "Requirement already satisfied: opencv-python<5.0.0,>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (4.11.0.86)\n",
      "Collecting pypdfium2<5.0.0,>=4.11.0 (from python-doctr[torch])\n",
      "  Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pyclipper<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (1.3.0.post6)\n",
      "Requirement already satisfied: shapely<3.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (2.1.1)\n",
      "Collecting langdetect<2.0.0,>=1.0.9 (from python-doctr[torch])\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from python-doctr[torch])\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (0.33.1)\n",
      "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (11.2.1)\n",
      "Requirement already satisfied: defusedxml>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (0.7.1)\n",
      "Collecting anyascii>=0.3.2 (from python-doctr[torch])\n",
      "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting validators>=0.18.0 (from python-doctr[torch])\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from python-doctr[torch]) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (1.1.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect<2.0.0,>=1.0.9->python-doctr[torch]) (1.17.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.16.0->python-doctr[torch]) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.16.0->python-doctr[torch]) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.16.0->python-doctr[torch]) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.16.0->python-doctr[torch]) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.16.0->python-doctr[torch]) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.16.0->python-doctr[torch]) (2.4.1)\n",
      "Collecting protobuf>=4.25.1 (from onnx<3.0.0,>=1.12.0->python-doctr[torch])\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0.0,>=2.0.0->python-doctr[torch])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.0.0->python-doctr[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.0.0->python-doctr[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->python-doctr[torch]) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.16.0->python-doctr[torch]) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.16.0->python-doctr[torch]) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.16.0->python-doctr[torch]) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.16.0->python-doctr[torch]) (2024.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2025.6.15)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.16.0->python-doctr[torch]) (2024.2.0)\n",
      "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_doctr-1.0.0-py3-none-any.whl (288 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3e0afb6e1e5e64c88b0f8cf2c554543c5c367c19017a18b9a39ca7e12dfff709\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: validators, rapidfuzz, pypdfium2, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, langdetect, anyascii, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, python-doctr\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anyascii-0.3.3 langdetect-1.0.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 protobuf-6.31.1 pypdfium2-4.30.1 python-doctr-1.0.0 rapidfuzz-3.13.0 validators-0.35.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-doctr[torch]  # or python-doctr[tf] for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:40:09.860362Z",
     "iopub.status.busy": "2025-07-09T17:40:09.858874Z",
     "iopub.status.idle": "2025-07-09T17:40:09.863723Z",
     "shell.execute_reply": "2025-07-09T17:40:09.863039Z",
     "shell.execute_reply.started": "2025-07-09T17:40:09.860332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "doctr_available = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:09.333206Z",
     "iopub.status.busy": "2025-07-09T17:22:09.332616Z",
     "iopub.status.idle": "2025-07-09T17:22:09.336753Z",
     "shell.execute_reply": "2025-07-09T17:22:09.336022Z",
     "shell.execute_reply.started": "2025-07-09T17:22:09.333181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/json-data/active-sun-455914-a1-141238abf96c.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:10.538204Z",
     "iopub.status.busy": "2025-07-09T17:22:10.537590Z",
     "iopub.status.idle": "2025-07-09T17:22:10.541890Z",
     "shell.execute_reply": "2025-07-09T17:22:10.540992Z",
     "shell.execute_reply.started": "2025-07-09T17:22:10.538176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "ENGLISH_DATASET_PATH = \"/kaggle/input/ocr-dataset/English_OCR_dataset/English_OCR_dataset\"\n",
    "FRENCH_DATASET_PATH = \"/kaggle/input/ocr-dataset/French_OCR_dataset/French_OCR_dataset\"\n",
    "\n",
    "SAMPLES_PER_DATASET = 10\n",
    "TOTAL_SAMPLES = SAMPLES_PER_DATASET * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Sampling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:15.364037Z",
     "iopub.status.busy": "2025-07-09T17:22:15.363494Z",
     "iopub.status.idle": "2025-07-09T17:22:15.371435Z",
     "shell.execute_reply": "2025-07-09T17:22:15.370645Z",
     "shell.execute_reply.started": "2025-07-09T17:22:15.364013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_french_samples(dataset_path, num_samples):\n",
    "    samples = []\n",
    "    \n",
    "    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    valid_pairs = []\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        base_name = img_file.replace('_default.jpg', '')  \n",
    "        xml_file = None\n",
    "        \n",
    "        possible_xml = [\n",
    "            f\"{base_name}_default.xml\",  \n",
    "            f\"{base_name[:-1]}g_default.xml\",\n",
    "        ]\n",
    "        \n",
    "        for xml_name in possible_xml:\n",
    "            if os.path.exists(os.path.join(dataset_path, xml_name)):\n",
    "                xml_file = xml_name\n",
    "                break\n",
    "        \n",
    "        if xml_file:\n",
    "            valid_pairs.append((img_file, xml_file))\n",
    "    \n",
    "    \n",
    "    selected_pairs = random.sample(valid_pairs, min(num_samples, len(valid_pairs)))\n",
    "    \n",
    "    for img_file, xml_file in selected_pairs:\n",
    "        samples.append({\n",
    "            'dataset': 'French',\n",
    "            'image_path': os.path.join(dataset_path, img_file),\n",
    "            'annotation_path': os.path.join(dataset_path, xml_file),\n",
    "            'image_name': img_file,\n",
    "            'annotation_name': xml_file\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def get_english_samples(dataset_path, num_samples):\n",
    "    \"\"\"Get random samples from English OCR dataset (JSON format)\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    images_path = os.path.join(dataset_path, 'images')\n",
    "    annotations_path = os.path.join(dataset_path, 'annotations')\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.png')]\n",
    "    selected_files = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    for img_file in selected_files:\n",
    "        # Find corresponding JSON file\n",
    "        base_name = img_file.replace('.png', '')\n",
    "        json_file = f\"{base_name}.json\"\n",
    "        \n",
    "        json_path = os.path.join(annotations_path, json_file)\n",
    "        if os.path.exists(json_path):\n",
    "            samples.append({\n",
    "                'dataset': 'English',\n",
    "                'image_path': os.path.join(images_path, img_file),\n",
    "                'annotation_path': json_path,\n",
    "                'image_name': img_file,\n",
    "                'annotation_name': json_file\n",
    "            })\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:15.981008Z",
     "iopub.status.busy": "2025-07-09T17:22:15.980416Z",
     "iopub.status.idle": "2025-07-09T17:22:15.986819Z",
     "shell.execute_reply": "2025-07-09T17:22:15.986030Z",
     "shell.execute_reply.started": "2025-07-09T17:22:15.980983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_text_from_french_xml(xml_path):\n",
    "    \"\"\"Extract ground truth text from French dataset XML file\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Find all String elements with CONTENT attribute\n",
    "        texts = []\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            if content:\n",
    "                texts.append(content)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML {xml_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_english_json(json_path):\n",
    "    \"\"\"Extract ground truth text from English dataset JSON file\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        texts = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON {json_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:17.061806Z",
     "iopub.status.busy": "2025-07-09T17:22:17.061512Z",
     "iopub.status.idle": "2025-07-09T17:22:17.069054Z",
     "shell.execute_reply": "2025-07-09T17:22:17.068328Z",
     "shell.execute_reply.started": "2025-07-09T17:22:17.061783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_boxes_from_french_xml(xml_path):\n",
    "    \"\"\"Extract bounding boxes and text from French dataset XML file\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        # Find all String elements with coordinates and content\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            hpos = string_elem.get('HPOS')\n",
    "            vpos = string_elem.get('VPOS')\n",
    "            width = string_elem.get('WIDTH')\n",
    "            height = string_elem.get('HEIGHT')\n",
    "            \n",
    "            if content and hpos and vpos and width and height:\n",
    "                # Convert to integers and create bounding box [x1, y1, x2, y2]\n",
    "                x1 = int(float(hpos))\n",
    "                y1 = int(float(vpos))\n",
    "                x2 = x1 + int(float(width))\n",
    "                y2 = y1 + int(float(height))\n",
    "                \n",
    "                boxes.append({\n",
    "                    'text': content,\n",
    "                    'box': [x1, y1, x2, y2]\n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML boxes {xml_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_boxes_from_english_json(json_path):\n",
    "    \"\"\"Extract bounding boxes and text from English dataset JSON file\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        boxes = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            box = form_item.get('box', [])\n",
    "            \n",
    "            if text and len(box) == 4:\n",
    "                boxes.append({\n",
    "                    'text': text,\n",
    "                    'box': box  # Already in [x1, y1, x2, y2] format\n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON boxes {json_path}: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset and Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:52.003725Z",
     "iopub.status.busy": "2025-07-09T17:22:52.003452Z",
     "iopub.status.idle": "2025-07-09T17:22:52.203751Z",
     "shell.execute_reply": "2025-07-09T17:22:52.203134Z",
     "shell.execute_reply.started": "2025-07-09T17:22:52.003702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French samples: 10\n",
      "English samples: 10\n",
      "Total samples: 20\n",
      "\n",
      "Sample 1 (French):\n",
      "Image: 12_0bb90_default.jpg\n",
      "Ground truth preview: leurs et comnmnutateurs électriques. 7.591. Madsen. Presse à cylindres pour la paille. 7.461. Magl...\n",
      "\n",
      "Sample 2 (French):\n",
      "Image: 14_e26ee_default.jpg\n",
      "Ground truth preview: vois postaux avec système de contrôle du timbre-poste. 7.284. Utley. Appareil pour clarifier et déco...\n",
      "\n",
      "Sample 3 (French):\n",
      "Image: 2_f1a3f_default.jpg\n",
      "Ground truth preview: 4°V 8072 ARGUS DES BREVETS D'INVENTION Abonnement un an : 24 francs. LISTE DES BREVETS déposes et d...\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get samples from both datasets\n",
    "french_samples = get_french_samples(FRENCH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "english_samples = get_english_samples(ENGLISH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "\n",
    "all_samples = french_samples + english_samples\n",
    "\n",
    "print(f\"French samples: {len(french_samples)}\")\n",
    "print(f\"English samples: {len(english_samples)}\")\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "\n",
    "# Load ground truth text and bounding boxes for all samples\n",
    "for sample in all_samples:\n",
    "    if sample['dataset'] == 'French':\n",
    "        sample['ground_truth'] = extract_text_from_french_xml(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_french_xml(sample['annotation_path'])\n",
    "    else:\n",
    "        sample['ground_truth'] = extract_text_from_english_json(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_english_json(sample['annotation_path'])\n",
    "\n",
    "# Display sample information\n",
    "for i, sample in enumerate(all_samples[:3]):\n",
    "    print(f\"\\nSample {i+1} ({sample['dataset']}):\")  \n",
    "    print(f\"Image: {sample['image_name']}\")\n",
    "    print(f\"Ground truth preview: {sample['ground_truth'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Model Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:45:48.527712Z",
     "iopub.status.busy": "2025-07-09T17:45:48.527406Z",
     "iopub.status.idle": "2025-07-09T17:45:48.539362Z",
     "shell.execute_reply": "2025-07-09T17:45:48.538323Z",
     "shell.execute_reply.started": "2025-07-09T17:45:48.527693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Global variables to store initialized models\n",
    "easyocr_reader = None\n",
    "google_client = None\n",
    "doctr_model = None\n",
    "\n",
    "# Check model availability\n",
    "models_available = {\n",
    "    'tesseract': True,\n",
    "    'easyocr': True,\n",
    "\n",
    "    'doctr': True,\n",
    "    'google_vision': vision is not None\n",
    "}\n",
    "\n",
    "def initialize_ocr_models():\n",
    "    \"\"\"Initialize all available OCR models\"\"\"\n",
    "    global easyocr_reader, paddleocr_reader, google_client\n",
    "    \n",
    "    print(\"Initializing OCR models...\")\n",
    "    \n",
    "    # Initialize EasyOCR\n",
    "    try:\n",
    "        if models_available['easyocr']:\n",
    "            easyocr_reader = easyocr.Reader(['en', 'fr'])\n",
    "            print(\"✓ EasyOCR initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ EasyOCR initialization failed: {e}\")\n",
    "        models_available['easyocr'] = False\n",
    "    \n",
    "    # Initialize DocTR\n",
    "    try:\n",
    "        if models_available['doctr']:\n",
    "            doctr_model = ocr_predictor(pretrained=True)\n",
    "            print(\"✓ DocTR initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ DocTR initialization failed: {e}\")\n",
    "        models_available['doctr'] = False\n",
    "    \n",
    "    # Initialize Google Cloud Vision\n",
    "    try:\n",
    "        if models_available['google_vision']:\n",
    "            google_client = vision.ImageAnnotatorClient()\n",
    "            print(\"✓ Google Cloud Vision initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Google Cloud Vision initialization failed: {e}\")\n",
    "        models_available['google_vision'] = False\n",
    "    \n",
    "    available_models = [k for k, v in models_available.items() if v]\n",
    "    print(f\"\\nAvailable models: {available_models}\")\n",
    "    return available_models\n",
    "\n",
    "def tesseract_ocr(image_path):\n",
    "    \"\"\"Run Tesseract OCR on an image\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Tesseract error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def easyocr_ocr(image_path):\n",
    "    \"\"\"Run EasyOCR on an image\"\"\"\n",
    "    global easyocr_reader\n",
    "    try:\n",
    "        if not easyocr_reader:\n",
    "            print(\"EasyOCR not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        results = easyocr_reader.readtext(image_path)\n",
    "        text = ' '.join([result[1] for result in results])\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"EasyOCR error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def doctr_ocr(image_path):\n",
    "    global doctr_model\n",
    "    try:\n",
    "        if not doctr_model:\n",
    "            print(\"DocTR not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Load document\n",
    "        doc = DocumentFile.from_images(image_path)\n",
    "        \n",
    "        # Run OCR\n",
    "        result = doctr_model(doc)\n",
    "        \n",
    "        # Extract text\n",
    "        text_parts = []\n",
    "        for page in result.pages:\n",
    "            for block in page.blocks:\n",
    "                for line in block.lines:\n",
    "                    for word in line.words:\n",
    "                        text_parts.append(word.value)\n",
    "        \n",
    "        return ' '.join(text_parts).strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"DocTR error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def google_vision_ocr(image_path):\n",
    "    \"\"\"Run Google Cloud Vision OCR on an image\"\"\"\n",
    "    global google_client\n",
    "    try:\n",
    "        if not google_client:\n",
    "            print(\"Google Cloud Vision not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        \n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        response = google_client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "        \n",
    "        if texts:\n",
    "            return texts[0].description.strip()\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Google Vision error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:33.477969Z",
     "iopub.status.busy": "2025-07-09T17:22:33.477438Z",
     "iopub.status.idle": "2025-07-09T17:22:33.494339Z",
     "shell.execute_reply": "2025-07-09T17:22:33.493570Z",
     "shell.execute_reply.started": "2025-07-09T17:22:33.477944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Bounding box evaluation functions\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n",
    "    # box format: [x1, y1, x2, y2]\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    if x1_inter >= x2_inter or y1_inter >= y2_inter:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def get_ocr_boxes_with_text(ocr_model_name, image_path):\n",
    "    \"\"\"Get bounding boxes and text from OCR models that support it\"\"\"\n",
    "    boxes = []\n",
    "    \n",
    "    try:\n",
    "        if ocr_model_name == 'easyocr':\n",
    "            global easyocr_reader\n",
    "            if easyocr_reader:\n",
    "                results = easyocr_reader.readtext(image_path)\n",
    "                for result in results:\n",
    "                    # EasyOCR returns: [bbox_points, text, confidence]\n",
    "                    bbox_points = result[0]\n",
    "                    text = result[1]\n",
    "                    # Convert bbox points to [x1, y1, x2, y2]\n",
    "                    x_coords = [point[0] for point in bbox_points]\n",
    "                    y_coords = [point[1] for point in bbox_points]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        elif ocr_model_name == 'paddleocr':\n",
    "            global paddleocr_reader\n",
    "            if paddleocr_reader:\n",
    "                results = paddleocr_reader.ocr(image_path)\n",
    "                for line in results:\n",
    "                    if line:\n",
    "                        for word_info in line:\n",
    "                            if len(word_info) > 1:\n",
    "                                # PaddleOCR returns: [bbox_points, (text, confidence)]\n",
    "                                bbox_points = word_info[0]\n",
    "                                text = word_info[1][0]\n",
    "                                # Convert bbox points to [x1, y1, x2, y2]\n",
    "                                x_coords = [point[0] for point in bbox_points]\n",
    "                                y_coords = [point[1] for point in bbox_points]\n",
    "                                x1, x2 = min(x_coords), max(x_coords)\n",
    "                                y1, y2 = min(y_coords), max(y_coords)\n",
    "                                boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        elif ocr_model_name == 'google_vision':\n",
    "            global google_client\n",
    "            if google_client:\n",
    "                with open(image_path, 'rb') as image_file:\n",
    "                    content = image_file.read()\n",
    "                \n",
    "                image = vision.Image(content=content)\n",
    "                response = google_client.text_detection(image=image)\n",
    "                texts = response.text_annotations\n",
    "                \n",
    "                # Skip the first annotation (full text) and process individual words\n",
    "                for annotation in texts[1:]:\n",
    "                    text = annotation.description\n",
    "                    vertices = annotation.bounding_poly.vertices\n",
    "                    \n",
    "                    # Convert vertices to [x1, y1, x2, y2]\n",
    "                    x_coords = [vertex.x for vertex in vertices]\n",
    "                    y_coords = [vertex.y for vertex in vertices]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        # For other models (tesseract, kraken), we only have text\n",
    "        # so we can't do spatial evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting boxes from {ocr_model_name}: {e}\")\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Evaluate spatial accuracy using bounding box matching\"\"\"\n",
    "    if not predicted_boxes or not ground_truth_boxes:\n",
    "        return {\n",
    "            'detection_precision': 0.0,\n",
    "            'detection_recall': 0.0,\n",
    "            'detection_f1': 0.0,\n",
    "            'avg_iou': 0.0,\n",
    "            'matched_pairs': 0,\n",
    "            'total_predicted': len(predicted_boxes),\n",
    "            'total_ground_truth': len(ground_truth_boxes)\n",
    "        }\n",
    "    \n",
    "    # Find best matches between predicted and ground truth boxes\n",
    "    matched_pairs = []\n",
    "    used_gt_indices = set()\n",
    "    \n",
    "    for pred_idx, pred_box in enumerate(predicted_boxes):\n",
    "        best_iou = 0.0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for gt_idx, gt_box in enumerate(ground_truth_boxes):\n",
    "            if gt_idx in used_gt_indices:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(pred_box['box'], gt_box['box'])\n",
    "            if iou > best_iou and iou >= iou_threshold:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_gt_idx != -1:\n",
    "            matched_pairs.append({\n",
    "                'pred_idx': pred_idx,\n",
    "                'gt_idx': best_gt_idx,\n",
    "                'iou': best_iou,\n",
    "                'pred_text': pred_box['text'],\n",
    "                'gt_text': ground_truth_boxes[best_gt_idx]['text']\n",
    "            })\n",
    "            used_gt_indices.add(best_gt_idx)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    num_matches = len(matched_pairs)\n",
    "    detection_precision = num_matches / len(predicted_boxes) if predicted_boxes else 0.0\n",
    "    detection_recall = num_matches / len(ground_truth_boxes) if ground_truth_boxes else 0.0\n",
    "    detection_f1 = 2 * (detection_precision * detection_recall) / (detection_precision + detection_recall) if (detection_precision + detection_recall) > 0 else 0.0\n",
    "    avg_iou = sum(pair['iou'] for pair in matched_pairs) / num_matches if num_matches > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'detection_precision': detection_precision,\n",
    "        'detection_recall': detection_recall,\n",
    "        'detection_f1': detection_f1,\n",
    "        'avg_iou': avg_iou,\n",
    "        'matched_pairs': num_matches,\n",
    "        'total_predicted': len(predicted_boxes),\n",
    "        'total_ground_truth': len(ground_truth_boxes),\n",
    "        'matches': matched_pairs\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_with_spatial(predicted_text, ground_truth_text, predicted_boxes, ground_truth_boxes):\n",
    "    \"\"\"Comprehensive OCR evaluation including both text and spatial metrics\"\"\"\n",
    "    # Text-based evaluation (existing)\n",
    "    text_metrics = evaluate_ocr_result(predicted_text, ground_truth_text)\n",
    "    \n",
    "    # Spatial evaluation (new)\n",
    "    spatial_metrics = evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes)\n",
    "    \n",
    "    # Combined metrics\n",
    "    combined_metrics = {\n",
    "        # Text metrics\n",
    "        'text_similarity': text_metrics['similarity'],\n",
    "        'text_precision': text_metrics['precision'],``\n",
    "        'text_recall': text_metrics['recall'],\n",
    "        'text_f1': text_metrics['f1'],\n",
    "        \n",
    "        # Spatial metrics\n",
    "        'detection_precision': spatial_metrics['detection_precision'],\n",
    "        'detection_recall': spatial_metrics['detection_recall'],\n",
    "        'detection_f1': spatial_metrics['detection_f1'],\n",
    "        'avg_iou': spatial_metrics['avg_iou'],\n",
    "        'matched_boxes': spatial_metrics['matched_pairs'],\n",
    "        'total_predicted_boxes': spatial_metrics['total_predicted'],\n",
    "        'total_ground_truth_boxes': spatial_metrics['total_ground_truth'],\n",
    "        \n",
    "        # Combined score (average of text F1 and detection F1)\n",
    "        'combined_f1': (text_metrics['f1'] + spatial_metrics['detection_f1']) / 2,\n",
    "        \n",
    "        # Additional info\n",
    "        'predicted_length': text_metrics.get('predicted_length', 0),\n",
    "        'ground_truth_length': text_metrics.get('ground_truth_length', 0),\n",
    "        'spatial_matches': spatial_metrics.get('matches', [])\n",
    "    }\n",
    "    \n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:34.801719Z",
     "iopub.status.busy": "2025-07-09T17:22:34.801045Z",
     "iopub.status.idle": "2025-07-09T17:22:34.808538Z",
     "shell.execute_reply": "2025-07-09T17:22:34.807808Z",
     "shell.execute_reply.started": "2025-07-09T17:22:34.801694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for comparison\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text.lower()\n",
    "\n",
    "def calculate_similarity(text1,text2):\n",
    "    \"\"\"Calculate similarity between two texts using SequenceMatcher\"\"\"\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "    return SequenceMatcher(None, clean1, clean2).ratio()\n",
    "\n",
    "def calculate_word_accuracy(predicted, ground_truth):\n",
    "    \"\"\"Calculate word-level precision, recall, and F1-score\"\"\"\n",
    "    pred_words = set(clean_text(predicted).split())\n",
    "    gt_words = set(clean_text(ground_truth).split())\n",
    "    \n",
    "    if not gt_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    if not pred_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    intersection = pred_words.intersection(gt_words)\n",
    "    \n",
    "    precision = len(intersection) / len(pred_words) if pred_words else 0.0\n",
    "    recall = len(intersection) / len(gt_words) if gt_words else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_result(predicted, ground_truth):\n",
    "    \"\"\"Comprehensive evaluation of OCR result\"\"\"\n",
    "    similarity = calculate_similarity(predicted, ground_truth)\n",
    "    word_metrics = calculate_word_accuracy(predicted, ground_truth)\n",
    "    \n",
    "    return {\n",
    "        'similarity': similarity,\n",
    "        'precision': word_metrics['precision'],\n",
    "        'recall': word_metrics['recall'],\n",
    "        'f1': word_metrics['f1'],\n",
    "        'predicted_length': len(predicted),\n",
    "        'ground_truth_length': len(ground_truth)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run OCR Evaluation on All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:46:05.900882Z",
     "iopub.status.busy": "2025-07-09T17:46:05.900604Z",
     "iopub.status.idle": "2025-07-09T17:46:05.911573Z",
     "shell.execute_reply": "2025-07-09T17:46:05.910675Z",
     "shell.execute_reply.started": "2025-07-09T17:46:05.900861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_single_ocr_evaluation(samples, ocr_model_name):\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'tesseract': tesseract_ocr,\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'doctr':doctr_ocr,\n",
    "        'google_vision': google_vision_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} imgs\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate results\n",
    "        metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        print(f\"  Similarity: {metrics['similarity']:.3f}, P: {metrics['precision']:.3f}, R: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "\n",
    "        \n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_single_ocr_evaluation_with_spatial(samples, ocr_model_name):\n",
    "    \"\"\"Run OCR evaluation with both text and spatial metrics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'tesseract': tesseract_ocr,\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'doctr':doctr_ocr,\n",
    "        'google_vision': google_vision_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    # Check if model supports spatial evaluation\n",
    "    spatial_supported = ocr_model_name in ['easyocr', 'paddleocr', 'google_vision']\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} samples...\")\n",
    "    if spatial_supported:\n",
    "        print(f\"Spatial evaluation supported\")\n",
    "    else:\n",
    "        print(f\"Text-only evaluation (no bounding boxes)\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ground_truth_boxes': sample.get('ground_truth_boxes', []),\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get OCR text\n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Get OCR boxes if supported\n",
    "        predicted_boxes = []\n",
    "        if spatial_supported:\n",
    "            predicted_boxes = get_ocr_boxes_with_text(ocr_model_name, sample['image_path'])\n",
    "        \n",
    "        # Evaluate with spatial metrics if available\n",
    "        if spatial_supported and sample.get('ground_truth_boxes'):\n",
    "            metrics = evaluate_ocr_with_spatial(\n",
    "                predicted_text, \n",
    "                sample['ground_truth'],\n",
    "                predicted_boxes,\n",
    "                sample['ground_truth_boxes']\n",
    "            )\n",
    "            evaluation_type = 'spatial'\n",
    "        else:\n",
    "            # Fall back to text-only evaluation\n",
    "            metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "            evaluation_type = 'text_only'\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'predicted_boxes': predicted_boxes,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics,\n",
    "            'evaluation_type': evaluation_type\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        if evaluation_type == 'spatial':\n",
    "            print(f\"  Text F1: {metrics['text_f1']:.3f}, Detection F1: {metrics['detection_f1']:.3f}, Combined: {metrics['combined_f1']:.3f}, IoU: {metrics['avg_iou']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Similarity: {metrics['similarity']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:42:42.634698Z",
     "iopub.status.busy": "2025-07-09T17:42:42.634402Z",
     "iopub.status.idle": "2025-07-09T17:42:48.186364Z",
     "shell.execute_reply": "2025-07-09T17:42:48.185571Z",
     "shell.execute_reply.started": "2025-07-09T17:42:42.634676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OCR models...\n",
      "✓ EasyOCR initialized\n",
      "✓ DocTR initialized\n",
      "✓ Google Cloud Vision initialized\n",
      "\n",
      "Available models: ['tesseract', 'easyocr', 'doctr', 'google_vision']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tesseract', 'easyocr', 'doctr', 'google_vision']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_ocr_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:44:58.674601Z",
     "iopub.status.busy": "2025-07-09T17:44:58.674330Z",
     "iopub.status.idle": "2025-07-09T17:44:58.692053Z",
     "shell.execute_reply": "2025-07-09T17:44:58.691277Z",
     "shell.execute_reply.started": "2025-07-09T17:44:58.674581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running doctr on 20 imgs\n",
      "Processing 1/20: 12_0bb90_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 2/20: 14_e26ee_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 4/20: 8_5eeab_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 5/20: 10_10fe5_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 7/20: 17_24e64_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 8/20: 11_ef202_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 9/20: 5_1df9d_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 10/20: 18_ac1da_default.jpg\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 11/20: 0060080406.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 12/20: 0001477983.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 13/20: 88547278_88547279.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 14/20: 92094746.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 15/20: 0001239897.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 16/20: 00836816.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 17/20: 00860012_00860014.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 18/20: 71108371.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 19/20: 01122115.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n",
      "Processing 20/20: 00836244.png\n",
      "DocTR not initialized. Run initialize_ocr_models() first.\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 0.00s\n"
     ]
    }
   ],
   "source": [
    "doctr_results = run_single_ocr_evaluation(all_samples, 'doctr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:22:56.427611Z",
     "iopub.status.busy": "2025-07-09T17:22:56.427039Z",
     "iopub.status.idle": "2025-07-09T17:24:39.818327Z",
     "shell.execute_reply": "2025-07-09T17:24:39.817655Z",
     "shell.execute_reply.started": "2025-07-09T17:22:56.427586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running easyocr on 20 imgs\n",
      "Processing 1/20: 12_0bb90_default.jpg\n",
      "  Similarity: 0.299, P: 0.658, R: 0.674, F1: 0.666, Time: 9.80s\n",
      "Processing 2/20: 14_e26ee_default.jpg\n",
      "  Similarity: 0.092, P: 0.849, R: 0.871, F1: 0.860, Time: 8.55s\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n",
      "  Similarity: 0.325, P: 0.435, R: 0.458, F1: 0.446, Time: 2.12s\n",
      "Processing 4/20: 8_5eeab_default.jpg\n",
      "  Similarity: 0.270, P: 0.668, R: 0.678, F1: 0.673, Time: 8.82s\n",
      "Processing 5/20: 10_10fe5_default.jpg\n",
      "  Similarity: 0.269, P: 0.704, R: 0.710, F1: 0.707, Time: 9.47s\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n",
      "  Similarity: 0.203, P: 0.847, R: 0.866, F1: 0.856, Time: 9.81s\n",
      "Processing 7/20: 17_24e64_default.jpg\n",
      "  Similarity: 0.148, P: 0.864, R: 0.875, F1: 0.869, Time: 11.13s\n",
      "Processing 8/20: 11_ef202_default.jpg\n",
      "  Similarity: 0.211, P: 0.712, R: 0.725, F1: 0.719, Time: 9.30s\n",
      "Processing 9/20: 5_1df9d_default.jpg\n",
      "  Similarity: 0.199, P: 0.748, R: 0.739, F1: 0.743, Time: 9.12s\n",
      "Processing 10/20: 18_ac1da_default.jpg\n",
      "  Similarity: 0.091, P: 0.814, R: 0.825, F1: 0.819, Time: 10.57s\n",
      "Processing 11/20: 0060080406.png\n",
      "  Similarity: 0.032, P: 0.406, R: 0.380, F1: 0.393, Time: 1.54s\n",
      "Processing 12/20: 0001477983.png\n",
      "  Similarity: 0.088, P: 0.638, R: 0.694, F1: 0.665, Time: 1.53s\n",
      "Processing 13/20: 88547278_88547279.png\n",
      "  Similarity: 0.395, P: 0.548, R: 0.617, F1: 0.580, Time: 1.51s\n",
      "Processing 14/20: 92094746.png\n",
      "  Similarity: 0.126, P: 0.549, R: 0.496, F1: 0.521, Time: 1.01s\n",
      "Processing 15/20: 0001239897.png\n",
      "  Similarity: 0.015, P: 0.071, R: 0.067, F1: 0.069, Time: 1.18s\n",
      "Processing 16/20: 00836816.png\n",
      "  Similarity: 0.072, P: 0.266, R: 0.240, F1: 0.252, Time: 1.48s\n",
      "Processing 17/20: 00860012_00860014.png\n",
      "  Similarity: 0.263, P: 0.587, R: 0.549, F1: 0.568, Time: 1.36s\n",
      "Processing 18/20: 71108371.png\n",
      "  Similarity: 0.190, P: 0.599, R: 0.639, F1: 0.618, Time: 1.55s\n",
      "Processing 19/20: 01122115.png\n",
      "  Similarity: 0.227, P: 0.607, R: 0.561, F1: 0.583, Time: 1.99s\n",
      "Processing 20/20: 00836244.png\n",
      "  Similarity: 0.107, P: 0.282, R: 0.228, F1: 0.252, Time: 1.39s\n"
     ]
    }
   ],
   "source": [
    "easyocr_results = run_single_ocr_evaluation(all_samples, 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:25:04.559920Z",
     "iopub.status.busy": "2025-07-09T17:25:04.559632Z",
     "iopub.status.idle": "2025-07-09T17:28:29.732669Z",
     "shell.execute_reply": "2025-07-09T17:28:29.731957Z",
     "shell.execute_reply.started": "2025-07-09T17:25:04.559900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running easyocr on 20 samples...\n",
      "Spatial evaluation supported\n",
      "Processing 1/20: 12_0bb90_default.jpg\n",
      "  Text F1: 0.666, Detection F1: 0.084, Combined: 0.375, IoU: 0.625\n",
      "Processing 2/20: 14_e26ee_default.jpg\n",
      "  Text F1: 0.860, Detection F1: 0.107, Combined: 0.483, IoU: 0.690\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n",
      "  Text F1: 0.446, Detection F1: 0.289, Combined: 0.368, IoU: 0.674\n",
      "Processing 4/20: 8_5eeab_default.jpg\n",
      "  Text F1: 0.673, Detection F1: 0.098, Combined: 0.385, IoU: 0.618\n",
      "Processing 5/20: 10_10fe5_default.jpg\n",
      "  Text F1: 0.707, Detection F1: 0.072, Combined: 0.389, IoU: 0.637\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n",
      "  Text F1: 0.856, Detection F1: 0.143, Combined: 0.499, IoU: 0.652\n",
      "Processing 7/20: 17_24e64_default.jpg\n",
      "  Text F1: 0.869, Detection F1: 0.143, Combined: 0.506, IoU: 0.679\n",
      "Processing 8/20: 11_ef202_default.jpg\n",
      "  Text F1: 0.719, Detection F1: 0.073, Combined: 0.396, IoU: 0.624\n",
      "Processing 9/20: 5_1df9d_default.jpg\n",
      "  Text F1: 0.743, Detection F1: 0.104, Combined: 0.424, IoU: 0.623\n",
      "Processing 10/20: 18_ac1da_default.jpg\n",
      "  Text F1: 0.819, Detection F1: 0.201, Combined: 0.510, IoU: 0.676\n",
      "Processing 11/20: 0060080406.png\n",
      "  Text F1: 0.393, Detection F1: 0.452, Combined: 0.423, IoU: 0.704\n",
      "Processing 12/20: 0001477983.png\n",
      "  Text F1: 0.665, Detection F1: 0.406, Combined: 0.536, IoU: 0.770\n",
      "Processing 13/20: 88547278_88547279.png\n",
      "  Text F1: 0.580, Detection F1: 0.448, Combined: 0.514, IoU: 0.743\n",
      "Processing 14/20: 92094746.png\n",
      "  Text F1: 0.521, Detection F1: 0.640, Combined: 0.580, IoU: 0.739\n",
      "Processing 15/20: 0001239897.png\n",
      "  Text F1: 0.069, Detection F1: 0.397, Combined: 0.233, IoU: 0.710\n",
      "Processing 16/20: 00836816.png\n",
      "  Text F1: 0.252, Detection F1: 0.186, Combined: 0.219, IoU: 0.745\n",
      "Processing 17/20: 00860012_00860014.png\n",
      "  Text F1: 0.568, Detection F1: 0.604, Combined: 0.586, IoU: 0.770\n",
      "Processing 18/20: 71108371.png\n",
      "  Text F1: 0.618, Detection F1: 0.648, Combined: 0.633, IoU: 0.743\n",
      "Processing 19/20: 01122115.png\n",
      "  Text F1: 0.583, Detection F1: 0.249, Combined: 0.416, IoU: 0.689\n",
      "Processing 20/20: 00836244.png\n",
      "  Text F1: 0.252, Detection F1: 0.376, Combined: 0.314, IoU: 0.689\n"
     ]
    }
   ],
   "source": [
    "spatial_easyocr_results = run_single_ocr_evaluation_with_spatial(all_samples, 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:28:29.734243Z",
     "iopub.status.busy": "2025-07-09T17:28:29.733944Z",
     "iopub.status.idle": "2025-07-09T17:29:56.542660Z",
     "shell.execute_reply": "2025-07-09T17:29:56.541976Z",
     "shell.execute_reply.started": "2025-07-09T17:28:29.734223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tesseract on 20 imgs\n",
      "Processing 1/20: 12_0bb90_default.jpg\n",
      "  Similarity: 0.279, P: 0.709, R: 0.744, F1: 0.726, Time: 7.86s\n",
      "Processing 2/20: 14_e26ee_default.jpg\n",
      "  Similarity: 0.347, P: 0.799, R: 0.848, F1: 0.823, Time: 7.17s\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n",
      "  Similarity: 0.188, P: 0.450, R: 0.458, F1: 0.454, Time: 3.02s\n",
      "Processing 4/20: 8_5eeab_default.jpg\n",
      "  Similarity: 0.387, P: 0.700, R: 0.739, F1: 0.719, Time: 7.58s\n",
      "Processing 5/20: 10_10fe5_default.jpg\n",
      "  Similarity: 0.808, P: 0.742, R: 0.753, F1: 0.747, Time: 7.66s\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n",
      "  Similarity: 0.372, P: 0.787, R: 0.857, F1: 0.820, Time: 8.00s\n",
      "Processing 7/20: 17_24e64_default.jpg\n",
      "  Similarity: 0.529, P: 0.757, R: 0.832, F1: 0.793, Time: 10.60s\n",
      "Processing 8/20: 11_ef202_default.jpg\n",
      "  Similarity: 0.605, P: 0.709, R: 0.746, F1: 0.727, Time: 7.34s\n",
      "Processing 9/20: 5_1df9d_default.jpg\n",
      "  Similarity: 0.689, P: 0.707, R: 0.751, F1: 0.728, Time: 7.46s\n",
      "Processing 10/20: 18_ac1da_default.jpg\n",
      "  Similarity: 0.228, P: 0.787, R: 0.827, F1: 0.806, Time: 8.85s\n",
      "Processing 11/20: 0060080406.png\n",
      "  Similarity: 0.163, P: 0.562, R: 0.514, F1: 0.537, Time: 0.91s\n",
      "Processing 12/20: 0001477983.png\n",
      "  Similarity: 0.158, P: 0.733, R: 0.792, F1: 0.761, Time: 1.32s\n",
      "Processing 13/20: 88547278_88547279.png\n",
      "  Similarity: 0.412, P: 0.391, R: 0.567, F1: 0.463, Time: 0.86s\n",
      "Processing 14/20: 92094746.png\n",
      "  Similarity: 0.091, P: 0.591, R: 0.602, F1: 0.596, Time: 1.02s\n",
      "Processing 15/20: 0001239897.png\n",
      "  Similarity: 0.140, P: 0.171, R: 0.146, F1: 0.158, Time: 0.66s\n",
      "Processing 16/20: 00836816.png\n",
      "  Similarity: 0.292, P: 0.729, R: 0.579, F1: 0.645, Time: 0.80s\n",
      "Processing 17/20: 00860012_00860014.png\n",
      "  Similarity: 0.346, P: 0.510, R: 0.490, F1: 0.500, Time: 2.41s\n",
      "Processing 18/20: 71108371.png\n",
      "  Similarity: 0.143, P: 0.750, R: 0.831, F1: 0.789, Time: 1.08s\n",
      "Processing 19/20: 01122115.png\n",
      "  Similarity: 0.156, P: 0.672, R: 0.636, F1: 0.654, Time: 1.21s\n",
      "Processing 20/20: 00836244.png\n",
      "  Similarity: 0.070, P: 0.396, R: 0.262, F1: 0.315, Time: 0.76s\n"
     ]
    }
   ],
   "source": [
    "tesseract_results = run_single_ocr_evaluation(all_samples, 'tesseract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:30:28.231912Z",
     "iopub.status.busy": "2025-07-09T17:30:28.231203Z",
     "iopub.status.idle": "2025-07-09T17:30:41.941061Z",
     "shell.execute_reply": "2025-07-09T17:30:41.940270Z",
     "shell.execute_reply.started": "2025-07-09T17:30:28.231890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running google_vision on 20 imgs\n",
      "Processing 1/20: 12_0bb90_default.jpg\n",
      "  Similarity: 0.436, P: 0.810, R: 0.803, F1: 0.806, Time: 0.81s\n",
      "Processing 2/20: 14_e26ee_default.jpg\n",
      "  Similarity: 0.976, P: 0.976, R: 0.971, F1: 0.974, Time: 1.37s\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n",
      "  Similarity: 0.303, P: 0.569, R: 0.559, F1: 0.564, Time: 0.54s\n",
      "Processing 4/20: 8_5eeab_default.jpg\n",
      "  Similarity: 0.431, P: 0.761, R: 0.757, F1: 0.759, Time: 0.90s\n",
      "Processing 5/20: 10_10fe5_default.jpg\n",
      "  Similarity: 0.898, P: 0.817, R: 0.802, F1: 0.809, Time: 1.27s\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n",
      "  Similarity: 0.386, P: 0.945, R: 0.958, F1: 0.951, Time: 0.88s\n",
      "Processing 7/20: 17_24e64_default.jpg\n",
      "  Similarity: 0.831, P: 0.961, R: 0.959, F1: 0.960, Time: 1.03s\n",
      "Processing 8/20: 11_ef202_default.jpg\n",
      "  Similarity: 0.901, P: 0.813, R: 0.809, F1: 0.811, Time: 1.33s\n",
      "Processing 9/20: 5_1df9d_default.jpg\n",
      "  Similarity: 0.801, P: 0.827, R: 0.804, F1: 0.815, Time: 0.79s\n",
      "Processing 10/20: 18_ac1da_default.jpg\n",
      "  Similarity: 0.809, P: 0.950, R: 0.941, F1: 0.945, Time: 0.99s\n",
      "Processing 11/20: 0060080406.png\n",
      "  Similarity: 0.326, P: 0.848, R: 0.824, F1: 0.836, Time: 0.71s\n",
      "Processing 12/20: 0001477983.png\n",
      "  Similarity: 0.174, P: 0.872, R: 0.908, F1: 0.890, Time: 0.29s\n",
      "Processing 13/20: 88547278_88547279.png\n",
      "  Similarity: 0.436, P: 0.584, R: 0.842, F1: 0.689, Time: 0.29s\n",
      "Processing 14/20: 92094746.png\n",
      "  Similarity: 0.105, P: 0.860, R: 0.814, F1: 0.836, Time: 0.24s\n",
      "Processing 15/20: 0001239897.png\n",
      "  Similarity: 0.054, P: 0.617, R: 0.562, F1: 0.588, Time: 0.20s\n",
      "Processing 16/20: 00836816.png\n",
      "  Similarity: 0.733, P: 0.795, R: 0.736, F1: 0.764, Time: 0.23s\n",
      "Processing 17/20: 00860012_00860014.png\n",
      "  Similarity: 0.022, P: 0.819, R: 0.797, F1: 0.808, Time: 0.27s\n",
      "Processing 18/20: 71108371.png\n",
      "  Similarity: 0.245, P: 0.935, R: 0.946, F1: 0.940, Time: 0.36s\n",
      "Processing 19/20: 01122115.png\n",
      "  Similarity: 0.153, P: 0.750, R: 0.674, F1: 0.710, Time: 0.73s\n",
      "Processing 20/20: 00836244.png\n",
      "  Similarity: 0.099, P: 0.621, R: 0.566, F1: 0.592, Time: 0.25s\n"
     ]
    }
   ],
   "source": [
    "gg_vision_results = run_single_ocr_evaluation(all_samples,'google_vision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:30:41.942458Z",
     "iopub.status.busy": "2025-07-09T17:30:41.942177Z",
     "iopub.status.idle": "2025-07-09T17:31:09.067211Z",
     "shell.execute_reply": "2025-07-09T17:31:09.066639Z",
     "shell.execute_reply.started": "2025-07-09T17:30:41.942431Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running google_vision on 20 samples...\n",
      "Spatial evaluation supported\n",
      "Processing 1/20: 12_0bb90_default.jpg\n",
      "  Text F1: 0.806, Detection F1: 0.000, Combined: 0.403, IoU: 0.000\n",
      "Processing 2/20: 14_e26ee_default.jpg\n",
      "  Text F1: 0.974, Detection F1: 0.000, Combined: 0.487, IoU: 0.000\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n",
      "  Text F1: 0.564, Detection F1: 0.032, Combined: 0.298, IoU: 0.551\n",
      "Processing 4/20: 8_5eeab_default.jpg\n",
      "  Text F1: 0.759, Detection F1: 0.000, Combined: 0.380, IoU: 0.000\n",
      "Processing 5/20: 10_10fe5_default.jpg\n",
      "  Text F1: 0.809, Detection F1: 0.000, Combined: 0.405, IoU: 0.000\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n",
      "  Text F1: 0.951, Detection F1: 0.000, Combined: 0.476, IoU: 0.000\n",
      "Processing 7/20: 17_24e64_default.jpg\n",
      "  Text F1: 0.960, Detection F1: 0.000, Combined: 0.480, IoU: 0.000\n",
      "Processing 8/20: 11_ef202_default.jpg\n",
      "  Text F1: 0.811, Detection F1: 0.000, Combined: 0.406, IoU: 0.000\n",
      "Processing 9/20: 5_1df9d_default.jpg\n",
      "  Text F1: 0.815, Detection F1: 0.000, Combined: 0.408, IoU: 0.000\n",
      "Processing 10/20: 18_ac1da_default.jpg\n",
      "  Text F1: 0.945, Detection F1: 0.000, Combined: 0.473, IoU: 0.000\n",
      "Processing 11/20: 0060080406.png\n",
      "  Text F1: 0.836, Detection F1: 0.163, Combined: 0.499, IoU: 0.649\n",
      "Processing 12/20: 0001477983.png\n",
      "  Text F1: 0.890, Detection F1: 0.137, Combined: 0.513, IoU: 0.723\n",
      "Processing 13/20: 88547278_88547279.png\n",
      "  Text F1: 0.689, Detection F1: 0.044, Combined: 0.367, IoU: 0.676\n",
      "Processing 14/20: 92094746.png\n",
      "  Text F1: 0.836, Detection F1: 0.123, Combined: 0.480, IoU: 0.609\n",
      "Processing 15/20: 0001239897.png\n",
      "  Text F1: 0.588, Detection F1: 0.121, Combined: 0.354, IoU: 0.611\n",
      "Processing 16/20: 00836816.png\n",
      "  Text F1: 0.764, Detection F1: 0.089, Combined: 0.427, IoU: 0.647\n",
      "Processing 17/20: 00860012_00860014.png\n",
      "  Text F1: 0.808, Detection F1: 0.158, Combined: 0.483, IoU: 0.667\n",
      "Processing 18/20: 71108371.png\n",
      "  Text F1: 0.940, Detection F1: 0.133, Combined: 0.537, IoU: 0.670\n",
      "Processing 19/20: 01122115.png\n",
      "  Text F1: 0.710, Detection F1: 0.073, Combined: 0.391, IoU: 0.575\n",
      "Processing 20/20: 00836244.png\n",
      "  Text F1: 0.592, Detection F1: 0.196, Combined: 0.394, IoU: 0.608\n"
     ]
    }
   ],
   "source": [
    "spatial_gg_vision_results = run_single_ocr_evaluation_with_spatial(all_samples,'google_vision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:33:07.796586Z",
     "iopub.status.busy": "2025-07-09T17:33:07.795804Z",
     "iopub.status.idle": "2025-07-09T17:33:08.016779Z",
     "shell.execute_reply": "2025-07-09T17:33:08.016055Z",
     "shell.execute_reply.started": "2025-07-09T17:33:07.796559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert all results\n",
    "all_ocr_results = {\n",
    "    'easyocr': easyocr_results,\n",
    "    'tesseract': tesseract_results,\n",
    "    'google_vision': gg_vision_results, \n",
    "    'spatial_easyocr': spatial_easyocr_results,\n",
    "    'spatial_google_vision': spatial_gg_vision_results\n",
    "}\n",
    "\n",
    "converted_results = convert_numpy_types(all_ocr_results)\n",
    "\n",
    "with open('all_ocr_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(converted_results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Spatial Evaluation with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bounding box extraction on a sample\n",
    "sample = all_samples[0]\n",
    "print(f\"Testing on: {sample['image_name']} ({sample['dataset']})\")\n",
    "print(f\"Ground truth boxes: {len(sample.get('ground_truth_boxes', []))}\")\n",
    "\n",
    "# Show first few ground truth boxes\n",
    "for i, box in enumerate(sample.get('ground_truth_boxes', [])[:3]):\n",
    "    print(f\"  Box {i+1}: {box['box']} -> '{box['text'][:50]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with spatial evaluation (EasyOCR supports bounding boxes)\n",
    "print(\"=== Testing EasyOCR with Spatial Evaluation ===\")\n",
    "easyocr_spatial_results = run_single_ocr_evaluation_with_spatial(all_samples[:3], 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text-only vs spatial evaluation\n",
    "print(\"\\n=== Comparison: Text-only vs Spatial Evaluation ===\")\n",
    "sample_result = easyocr_spatial_results[0]\n",
    "metrics = sample_result['ocr_result']['metrics']\n",
    "\n",
    "if sample_result['ocr_result']['evaluation_type'] == 'spatial':\n",
    "    print(f\"Sample: {sample_result['image_name']}\")\n",
    "    print(f\"Text Metrics:\")\n",
    "    print(f\"  - Text F1: {metrics['text_f1']:.3f}\")\n",
    "    print(f\"  - Text Precision: {metrics['text_precision']:.3f}\")\n",
    "    print(f\"  - Text Recall: {metrics['text_recall']:.3f}\")\n",
    "    print(f\"Spatial Metrics:\")\n",
    "    print(f\"  - Detection F1: {metrics['detection_f1']:.3f}\")\n",
    "    print(f\"  - Detection Precision: {metrics['detection_precision']:.3f}\")\n",
    "    print(f\"  - Detection Recall: {metrics['detection_recall']:.3f}\")\n",
    "    print(f\"  - Average IoU: {metrics['avg_iou']:.3f}\")\n",
    "    print(f\"  - Matched Boxes: {metrics['matched_boxes']}/{metrics['total_ground_truth_boxes']}\")\n",
    "    print(f\"Combined F1 Score: {metrics['combined_f1']:.3f}\")\n",
    "else:\n",
    "    print(\"Spatial evaluation not available for this sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Google Vision with spatial evaluation\n",
    "print(\"=== Testing Google Vision with Spatial Evaluation ===\")\n",
    "google_spatial_results = run_single_ocr_evaluation_with_spatial(all_samples[:2], 'google_vision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_dataframe(results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Convert results to pandas DataFrame for analysis\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for result in results:\n",
    "        for model_name, model_result in result['ocr_results'].items():\n",
    "            row = {\n",
    "                'sample_id': result['sample_id'],\n",
    "                'dataset': result['dataset'],\n",
    "                'image_name': result['image_name'],\n",
    "                'model': model_name,\n",
    "                'processing_time': model_result.get('processing_time', 0),\n",
    "                'similarity': model_result['metrics']['similarity'],\n",
    "                'precision': model_result['metrics']['precision'],\n",
    "                'recall': model_result['metrics']['recall'],\n",
    "                'f1': model_result['metrics']['f1'],\n",
    "                'predicted_length': model_result['metrics'].get('predicted_length', 0),\n",
    "                'ground_truth_length': model_result['metrics'].get('ground_truth_length', 0),\n",
    "                'has_error': 'error' in model_result\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = create_results_dataframe(evaluation_results)\n",
    "print(f\"Results DataFrame shape: {df_results.shape}\")\n",
    "print(f\"Available models: {df_results['model'].unique()}\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by model\n",
    "summary_stats = df_results.groupby('model').agg({\n",
    "    'similarity': ['mean', 'std', 'min', 'max'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'processing_time': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"=== OCR Model Performance Summary ===\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by dataset (French vs English)\n",
    "dataset_performance = df_results.groupby(['dataset', 'model']).agg({\n",
    "    'similarity': 'mean',\n",
    "    'f1': 'mean',\n",
    "    'processing_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"=== Performance by Dataset ===\")\n",
    "dataset_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. F1 Score comparison\n",
    "sns.boxplot(data=df_results, x='model', y='f1', ax=axes[0,0])\n",
    "axes[0,0].set_title('F1 Score by OCR Model')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Similarity comparison\n",
    "sns.boxplot(data=df_results, x='model', y='similarity', ax=axes[0,1])\n",
    "axes[0,1].set_title('Text Similarity by OCR Model')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Processing time comparison\n",
    "sns.boxplot(data=df_results, x='model', y='processing_time', ax=axes[1,0])\n",
    "axes[1,0].set_title('Processing Time by OCR Model')\n",
    "axes[1,0].set_ylabel('Time (seconds)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Performance by dataset\n",
    "sns.barplot(data=df_results, x='model', y='f1', hue='dataset', ax=axes[1,1])\n",
    "axes[1,1].set_title('F1 Score by Model and Dataset')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ranking and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall ranking\n",
    "model_ranking = df_results.groupby('model').agg({\n",
    "    'f1': 'mean',\n",
    "    'similarity': 'mean',\n",
    "    'precision': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'processing_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Add pricing information\n",
    "pricing_info = {\n",
    "    'tesseract': 'Free',\n",
    "    'easyocr': 'Free',\n",
    "    'paddleocr': 'Free',\n",
    "    'kraken': 'Free',\n",
    "    'google_vision': '$1.50 per 1,000 images'\n",
    "}\n",
    "\n",
    "model_ranking['pricing'] = model_ranking.index.map(pricing_info)\n",
    "\n",
    "# Sort by F1 score\n",
    "model_ranking_sorted = model_ranking.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"=== OCR Model Ranking (by F1 Score) ===\")\n",
    "model_ranking_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results report\n",
    "final_report = {\n",
    "    'evaluation_summary': {\n",
    "        'total_samples': len(all_samples),\n",
    "        'french_samples': len(french_samples),\n",
    "        'english_samples': len(english_samples),\n",
    "        'models_tested': list(df_results['model'].unique()),\n",
    "        'evaluation_date': pd.Timestamp.now().isoformat()\n",
    "    },\n",
    "    'model_performance': model_ranking_sorted.to_dict('index'),\n",
    "    'dataset_performance': dataset_performance.to_dict(),\n",
    "    'pricing_information': pricing_info,\n",
    "    'detailed_results': evaluation_results\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = 'ocr_evaluation_results.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7834418,
     "sourceId": 12421308,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7834439,
     "sourceId": 12421338,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
