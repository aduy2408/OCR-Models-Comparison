{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12421308,"sourceType":"datasetVersion","datasetId":7834418},{"sourceId":12421338,"sourceType":"datasetVersion","datasetId":7834439}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# OCR Models Evaluation on French and English Datasets\n","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport random\nimport xml.etree.ElementTree as ET\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport time\n# import matplotlib.pyplot as plt\n# import seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# # OCR Libraries\n# import pytesseract\n# import easyocr\n\n# from google.cloud import vision\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:40:52.781624Z","iopub.execute_input":"2025-07-09T18:40:52.782395Z","iopub.status.idle":"2025-07-09T18:40:52.786753Z","shell.execute_reply.started":"2025-07-09T18:40:52.782369Z","shell.execute_reply":"2025-07-09T18:40:52.786014Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"pip install python-doctr[torch]  # or python-doctr[tf] for TensorFlow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom doctr.io import DocumentFile\nfrom doctr.models import ocr_predictor\ndoctr_available = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install surya-ocr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade surya-ocr torch torchvision transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchvision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install surya-ocr torch torchvision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nfrom surya.recognition import RecognitionPredictor\nfrom surya.detection import DetectionPredictor\n\nimage = Image.open('/kaggle/input/ocr-dataset/English_OCR_dataset/English_OCR_dataset/images/0001123541.png')\nrecognition_predictor = RecognitionPredictor()\ndetection_predictor = DetectionPredictor()\n\npredictions = recognition_predictor([image], det_predictor=detection_predictor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:37:14.367199Z","iopub.execute_input":"2025-07-09T18:37:14.368196Z","iopub.status.idle":"2025-07-09T18:38:00.798413Z","shell.execute_reply.started":"2025-07-09T18:37:14.368161Z","shell.execute_reply":"2025-07-09T18:38:00.797657Z"}},"outputs":[{"name":"stderr","text":"2025-07-09 18:37:20.465173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752086240.644285     151 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752086240.696560     151 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nDownloading text_recognition model to /root/.cache/datalab/models/text_recognition/2025_05_16: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\nDownloading text_detection model to /root/.cache/datalab/models/text_detection/2025_05_07: 100%|██████████| 6/6 [00:02<00:00,  2.08it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\nRecognizing Text: 100%|██████████| 43/43 [00:03<00:00, 11.27it/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/json-data/active-sun-455914-a1-141238abf96c.json'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Configuration","metadata":{}},{"cell_type":"code","source":"# Dataset paths\nENGLISH_DATASET_PATH = \"/kaggle/input/ocr-dataset/English_OCR_dataset/English_OCR_dataset\"\nFRENCH_DATASET_PATH = \"/kaggle/input/ocr-dataset/French_OCR_dataset/French_OCR_dataset\"\n\nSAMPLES_PER_DATASET = 10\nTOTAL_SAMPLES = SAMPLES_PER_DATASET * 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:40:42.283732Z","iopub.execute_input":"2025-07-09T18:40:42.284658Z","iopub.status.idle":"2025-07-09T18:40:42.288994Z","shell.execute_reply.started":"2025-07-09T18:40:42.284630Z","shell.execute_reply":"2025-07-09T18:40:42.288138Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Data Loading and Sampling Functions","metadata":{}},{"cell_type":"code","source":"def get_french_samples(dataset_path, num_samples):\n    samples = []\n    \n    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n    \n    valid_pairs = []\n    \n    for img_file in image_files:\n        base_name = img_file.replace('_default.jpg', '')  \n        xml_file = None\n        \n        possible_xml = [\n            f\"{base_name}_default.xml\",  \n            f\"{base_name[:-1]}g_default.xml\",\n        ]\n        \n        for xml_name in possible_xml:\n            if os.path.exists(os.path.join(dataset_path, xml_name)):\n                xml_file = xml_name\n                break\n        \n        if xml_file:\n            valid_pairs.append((img_file, xml_file))\n    \n    \n    selected_pairs = random.sample(valid_pairs, min(num_samples, len(valid_pairs)))\n    \n    for img_file, xml_file in selected_pairs:\n        samples.append({\n            'dataset': 'French',\n            'image_path': os.path.join(dataset_path, img_file),\n            'annotation_path': os.path.join(dataset_path, xml_file),\n            'image_name': img_file,\n            'annotation_name': xml_file\n        })\n    \n    return samples\n\ndef get_english_samples(dataset_path, num_samples):\n    samples = []\n    \n    images_path = os.path.join(dataset_path, 'images')\n    annotations_path = os.path.join(dataset_path, 'annotations')\n    \n    # Get all image files\n    image_files = [f for f in os.listdir(images_path) if f.endswith('.png')]\n    selected_files = random.sample(image_files, min(num_samples, len(image_files)))\n    \n    for img_file in selected_files:\n        # Find corresponding json\n        base_name = img_file.replace('.png', '')\n        json_file = f\"{base_name}.json\"\n        \n        json_path = os.path.join(annotations_path, json_file)\n        if os.path.exists(json_path):\n            samples.append({\n                'dataset': 'English',\n                'image_path': os.path.join(images_path, img_file),\n                'annotation_path': json_path,\n                'image_name': img_file,\n                'annotation_name': json_file\n            })\n    \n    return samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:40:44.077937Z","iopub.execute_input":"2025-07-09T18:40:44.078234Z","iopub.status.idle":"2025-07-09T18:40:44.086916Z","shell.execute_reply.started":"2025-07-09T18:40:44.078210Z","shell.execute_reply":"2025-07-09T18:40:44.086006Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def extract_text_from_french_xml(xml_path):\n    \"\"\"Extract ground truth text from French dataset XML file\"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        \n        # Find all String elements with CONTENT attribute\n        texts = []\n        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n            content = string_elem.get('CONTENT')\n            if content:\n                texts.append(content)\n        \n        return ' '.join(texts)\n    except Exception as e:\n        print(f\"Error parsing XML {xml_path}: {e}\")\n        return \"\"\n\ndef extract_text_from_english_json(json_path):\n    \"\"\"Extract ground truth text from English dataset JSON file\"\"\"\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        \n        texts = []\n        for form_item in data.get('form', []):\n            text = form_item.get('text', '').strip()\n            if text:\n                texts.append(text)\n        \n        return ' '.join(texts)\n    except Exception as e:\n        print(f\"Error parsing JSON {json_path}: {e}\")\n        return \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:40:47.347888Z","iopub.execute_input":"2025-07-09T18:40:47.348654Z","iopub.status.idle":"2025-07-09T18:40:47.354849Z","shell.execute_reply.started":"2025-07-09T18:40:47.348623Z","shell.execute_reply":"2025-07-09T18:40:47.354013Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n\ndef extract_boxes_from_french_xml(xml_path):\n    \"\"\"Extract bounding boxes and text from French dataset XML file\"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        \n        boxes = []\n        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n            content = string_elem.get('CONTENT')\n            hpos = string_elem.get('HPOS')\n            vpos = string_elem.get('VPOS')\n            width = string_elem.get('WIDTH')\n            height = string_elem.get('HEIGHT')\n            \n            if content and hpos and vpos and width and height:\n                x1 = int(float(hpos))\n                y1 = int(float(vpos))\n                x2 = x1 + int(float(width))\n                y2 = y1 + int(float(height))\n                \n                boxes.append({\n                    'text': content,\n                    'box': [x1, y1, x2, y2]\n                })\n        \n        return boxes\n    except Exception as e:\n        print(f\"Error parsing XML boxes {xml_path}: {e}\")\n        return []\n\ndef extract_boxes_from_english_json(json_path):\n    \"\"\"Extract bounding boxes and text from English dataset JSON file\"\"\"\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        \n        boxes = []\n        for form_item in data.get('form', []):\n            text = form_item.get('text', '').strip()\n            box = form_item.get('box', [])\n            \n            if text and len(box) == 4:\n                boxes.append({\n                    'text': text,\n                    'box': box  # Already in [x1, y1, x2, y2] format\n                })\n        \n        return boxes\n    except Exception as e:\n        print(f\"Error parsing JSON boxes {json_path}: {e}\")\n        return []\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:40:48.706415Z","iopub.execute_input":"2025-07-09T18:40:48.706720Z","iopub.status.idle":"2025-07-09T18:40:48.714219Z","shell.execute_reply.started":"2025-07-09T18:40:48.706697Z","shell.execute_reply":"2025-07-09T18:40:48.713401Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Sample Dataset and Load Ground Truth","metadata":{}},{"cell_type":"code","source":"\n\nfrench_samples = get_french_samples(FRENCH_DATASET_PATH, SAMPLES_PER_DATASET)\nenglish_samples = get_english_samples(ENGLISH_DATASET_PATH, SAMPLES_PER_DATASET)\n\nall_samples = french_samples + english_samples\n\nprint(f\"French samples: {len(french_samples)}\")\nprint(f\"English samples: {len(english_samples)}\")\nprint(f\"Total samples: {len(all_samples)}\")\n\nfor sample in all_samples:\n    if sample['dataset'] == 'French':\n        sample['ground_truth'] = extract_text_from_french_xml(sample['annotation_path'])\n        sample['ground_truth_boxes'] = extract_boxes_from_french_xml(sample['annotation_path'])\n    else:\n        sample['ground_truth'] = extract_text_from_english_json(sample['annotation_path'])\n        sample['ground_truth_boxes'] = extract_boxes_from_english_json(sample['annotation_path'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:40:58.137535Z","iopub.execute_input":"2025-07-09T18:40:58.138077Z","iopub.status.idle":"2025-07-09T18:40:58.401976Z","shell.execute_reply.started":"2025-07-09T18:40:58.138050Z","shell.execute_reply":"2025-07-09T18:40:58.401301Z"}},"outputs":[{"name":"stdout","text":"French samples: 10\nEnglish samples: 10\nTotal samples: 20\n\nSample 1 (French):\nImage: 12_0bb90_default.jpg\nGround truth preview: leurs et comnmnutateurs électriques. 7.591. Madsen. Presse à cylindres pour la paille. 7.461. Magl...\n\nSample 2 (French):\nImage: 14_e26ee_default.jpg\nGround truth preview: vois postaux avec système de contrôle du timbre-poste. 7.284. Utley. Appareil pour clarifier et déco...\n\nSample 3 (French):\nImage: 2_f1a3f_default.jpg\nGround truth preview: 4°V 8072 ARGUS DES BREVETS D'INVENTION Abonnement un an : 24 francs. LISTE DES BREVETS déposes et d...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## OCR Model Implementations","metadata":{}},{"cell_type":"code","source":"easyocr_reader = None\ngoogle_client = None\ndoctr_model = None\nsurya_recognition_predictor = None\nsurya_detection_predictor = None\n\nmodels_available = {\n    'tesseract': True,\n    'easyocr': True,\n    'surya': True,\n    'doctr': True,\n    'google_vision':True\n}\n\ndef initialize_ocr_models():\n    global easyocr_reader, google_client, doctr_model, surya_recognition_predictor, surya_detection_predictor\n    \n    print(\"Initializing models\")\n    \n    try:\n        if models_available['easyocr']:\n            easyocr_reader = easyocr.Reader(['en', 'fr'])\n    except Exception as e:\n        print(f\"EasyOCR initialization failed: {e}\")\n        models_available['easyocr'] = False\n    \n    #DocTR\n    try:\n        if models_available['doctr']:\n            doctr_model = ocr_predictor(pretrained=True)\n    except Exception as e:\n        print(f\"DocTR initialization failed: {e}\")\n        models_available['doctr'] = False\n    #Surya\n    try:\n        if models_available['surya']:\n            surya_recognition_predictor = RecognitionPredictor()\n            surya_detection_predictor = DetectionPredictor()\n    except Exception as e:\n        print(f\"Surya OCR initialization failed: {e}\")\n        models_available['surya'] = False\n        \n    # Google Cloud Vision\n    try:\n        if models_available['google_vision']:\n            google_client = vision.ImageAnnotatorClient()\n    except Exception as e:\n        print(f\"Google Cloud Vision initialization failed: {e}\")\n        models_available['google_vision'] = False\n    \n    available_models = [k for k, v in models_available.items() if v]\n    return available_models\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:44:52.484249Z","iopub.execute_input":"2025-07-09T18:44:52.484895Z","iopub.status.idle":"2025-07-09T18:44:52.498831Z","shell.execute_reply.started":"2025-07-09T18:44:52.484869Z","shell.execute_reply":"2025-07-09T18:44:52.498109Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def tesseract_ocr(image_path):\n    try:\n        image = Image.open(image_path)\n        text = pytesseract.image_to_string(image)\n        return text.strip()\n    except Exception as e:\n        print(f\"Tesseract error on {image_path}: {e}\")\n        return \"\"\n\ndef easyocr_ocr(image_path):\n    global easyocr_reader\n    try:\n        if not easyocr_reader:\n            print(\"easyocr not initialized.\")\n            return \"\"\n        results = easyocr_reader.readtext(image_path)\n        text = ' '.join([result[1] for result in results])\n        return text.strip()\n    except Exception as e:\n        print(f\"easyocr error on {image_path}: {e}\")\n        return \"\"\n        \ndef surya_ocr(image_path):\n    global surya_recognition_predictor, surya_detection_predictor\n    try:\n        if not surya_recognition_predictor or not surya_detection_predictor:\n            print(\"Surya not initialized\")\n            return \"\"\n        \n        image = Image.open(image_path)\n        predictions = surya_recognition_predictor([image], det_predictor=surya_detection_predictor)\n        \n        # Extract text from predictions\n        text_parts = []\n        for text_line in predictions[0].text_lines:\n            text_parts.append(text_line.text)\n        \n        return ' '.join(text_parts).strip()\n    except Exception as e:\n        print(f\"Surya error: {e}\")\n        return \"\"\n        \ndef doctr_ocr(image_path):\n    global doctr_model\n    try:\n        if not doctr_model:\n            print(\"DocTR not initialized\")\n            return \"\"\n        \n        # Load document\n        doc = DocumentFile.from_images(image_path)\n        \n        # Run OCR\n        result = doctr_model(doc)\n        \n        # Extract text\n        text_parts = []\n        for page in result.pages:\n            for block in page.blocks:\n                for line in block.lines:\n                    for word in line.words:\n                        text_parts.append(word.value)\n        \n        return ' '.join(text_parts).strip()\n        \n    except Exception as e:\n        print(f\"doctr error on {image_path}: {e}\")\n        return \"\"\n\n\ndef google_vision_ocr(image_path):\n    global google_client\n    try:\n        if not google_client:\n            print(\"gg vsion not initialized\")\n            return \"\"\n        \n        with open(image_path, 'rb') as image_file:\n            content = image_file.read()\n        \n        image = vision.Image(content=content)\n        response = google_client.text_detection(image=image)\n        texts = response.text_annotations\n        \n        if texts:\n            return texts[0].description.strip()\n        return \"\"\n    except Exception as e:\n        print(f\"gg vision error on {image_path}: {e}\")\n        return \"\"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{}},{"cell_type":"markdown","source":"## Run OCR Evaluation on All Samples","metadata":{}},{"cell_type":"code","source":"\ndef run_single_ocr_evaluation(samples, ocr_model_name):\n    results = []\n    \n    ocr_functions = {\n        'tesseract': tesseract_ocr,\n        'easyocr': easyocr_ocr,\n        'doctr':doctr_ocr,\n        'surya':surya_ocr,\n        'google_vision': google_vision_ocr\n    }\n    \n    ocr_function = ocr_functions[ocr_model_name]\n    \n    print(f\"Running {ocr_model_name} on {len(samples)} imgs\")\n    \n    for i, sample in enumerate(samples):\n        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n        \n        sample_result = {\n            'sample_id': i,\n            'dataset': sample['dataset'],\n            'image_name': sample['image_name'],\n            'ground_truth': sample['ground_truth'],\n            'ocr_result': {}\n        }\n        \n        start_time = time.time()\n        \n        predicted_text = ocr_function(sample['image_path'])\n        processing_time = time.time() - start_time\n        \n        # Evaluate results\n        metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n        \n        sample_result['ocr_result'] = {\n            'model': ocr_model_name,\n            'predicted_text': predicted_text,\n            'processing_time': processing_time,\n            'metrics': metrics\n        }\n        print(f\"  Similarity: {metrics['similarity']:.3f}, P: {metrics['precision']:.3f}, R: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n        \n\n        results.append(sample_result)\n    \n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_single_ocr_evaluation_with_spatial(samples, ocr_model_name):\n    results = []\n    \n    ocr_functions = {\n        'easyocr': easyocr_ocr,\n        'doctr':doctr_ocr,\n        'google_vision': google_vision_ocr,\n        'surya':surya_ocr\n    }\n    \n    ocr_function = ocr_functions[ocr_model_name]\n    \n    spatial_supported = ocr_model_name in ['easyocr', 'doctr', 'google_vision','surya']\n    \n    print(f\"Running {ocr_model_name} on {len(samples)} samples\")\n    \n    for i, sample in enumerate(samples):\n        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n        \n        sample_result = {\n            'sample_id': i,\n            'dataset': sample['dataset'],\n            'image_name': sample['image_name'],\n            'ground_truth': sample['ground_truth'],\n            'ground_truth_boxes': sample.get('ground_truth_boxes', []),\n            'ocr_result': {}\n        }\n        \n        start_time = time.time()\n        \n        # Get OCR text\n        predicted_text = ocr_function(sample['image_path'])\n        processing_time = time.time() - start_time\n        \n        # Get OCR boxes\n        predicted_boxes = []\n        if spatial_supported:\n            predicted_boxes = get_ocr_boxes_with_text(ocr_model_name, sample['image_path'])\n        \n        if spatial_supported and sample.get('ground_truth_boxes'):\n            metrics = evaluate_ocr_with_spatial(\n                predicted_text, \n                sample['ground_truth'],\n                predicted_boxes,\n                sample['ground_truth_boxes']\n            )\n            evaluation_type = 'spatial'\n        else:\n            metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n            evaluation_type = 'text_only'\n        \n        sample_result['ocr_result'] = {\n            'model': ocr_model_name,\n            'predicted_text': predicted_text,\n            'predicted_boxes': predicted_boxes,\n            'processing_time': processing_time,\n            'metrics': metrics,\n            'evaluation_type': evaluation_type\n        }\n        \n        if evaluation_type == 'spatial':\n            print(f\"  Text F1: {metrics['text_f1']:.3f}, Detection F1: {metrics['detection_f1']:.3f}, Combined: {metrics['combined_f1']:.3f}, IoU: {metrics['avg_iou']:.3f}\")\n        else:\n            print(f\"  Similarity: {metrics['similarity']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n        \n        results.append(sample_result)\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:48:24.052179Z","iopub.execute_input":"2025-07-09T18:48:24.052420Z","iopub.status.idle":"2025-07-09T18:48:24.071868Z","shell.execute_reply.started":"2025-07-09T18:48:24.052397Z","shell.execute_reply":"2025-07-09T18:48:24.071166Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"def calculate_iou(box1, box2):\n    x1_inter = max(box1[0], box2[0])\n    y1_inter = max(box1[1], box2[1])\n    x2_inter = min(box1[2], box2[2])\n    y2_inter = min(box1[3], box2[3])\n    \n    if x1_inter >= x2_inter or y1_inter >= y2_inter:\n        return 0.0\n    \n    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n    \n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    \n    union = area1 + area2 - intersection\n    \n    return intersection / union if union > 0 else 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef get_ocr_boxes_with_text(ocr_model_name, image_path):\n    boxes = []\n    \n    try:\n        if ocr_model_name == 'easyocr':\n            global easyocr_reader\n            if easyocr_reader:\n                results = easyocr_reader.readtext(image_path)\n                for result in results:\n                    bbox_points = result[0]\n                    text = result[1]\n                    # Convert bbox points \n                    x_coords = [point[0] for point in bbox_points]\n                    y_coords = [point[1] for point in bbox_points]\n                    x1, x2 = min(x_coords), max(x_coords)\n                    y1, y2 = min(y_coords), max(y_coords)\n                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n        \n        elif ocr_model_name == 'doctr':  \n            global doctr_model\n            if doctr_model:\n                # Load document\n                doc = DocumentFile.from_images(image_path)\n                result = doctr_model(doc)\n                \n                from PIL import Image\n                img = Image.open(image_path)\n                img_width, img_height = img.size\n                \n                for page in result.pages:\n                    for block in page.blocks:\n                        for line in block.lines:\n                            for word in line.words:\n                                # Convert \n                                bbox = word.geometry\n                                x1 = int(bbox[0][0] * img_width)\n                                y1 = int(bbox[0][1] * img_height)\n                                x2 = int(bbox[1][0] * img_width)\n                                y2 = int(bbox[1][1] * img_height)\n                                \n                                boxes.append({'text': word.value,'box': [x1, y1, x2, y2]})\n\n        elif ocr_model_name == 'surya':  \n            from PIL import Image\n            global surya_recognition_predictor, surya_detection_predictor\n            if surya_recognition_predictor and surya_detection_predictor:\n                image = Image.open(image_path)\n                predictions = surya_recognition_predictor([image], det_predictor=surya_detection_predictor)\n                \n                for text_line in predictions[0].text_lines:\n                    text = text_line.text\n                    bbox = text_line.bbox\n                    boxes.append({'text': text, 'box': bbox})\n                    \n        elif ocr_model_name == 'google_vision':\n            global google_client\n            if google_client:\n                with open(image_path, 'rb') as image_file:\n                    content = image_file.read()\n                \n                image = vision.Image(content=content)\n                response = google_client.text_detection(image=image)\n                texts = response.text_annotations\n                \n                for annotation in texts[1:]:\n                    text = annotation.description\n                    vertices = annotation.bounding_poly.vertices\n                    \n                    # Convert vertices to [x1, y1, x2, y2]\n                    x_coords = [vertex.x for vertex in vertices]\n                    y_coords = [vertex.y for vertex in vertices]\n                    x1, x2 = min(x_coords), max(x_coords)\n                    y1, y2 = min(y_coords), max(y_coords)\n                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n\n    except Exception as e:\n        print(f\"Error getting boxes from {ocr_model_name}: {e}\")\n    \n    return boxes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\n\ndef evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes, iou_threshold=0.5):\n    \"\"\"Evaluate spatial accuracy using bounding box matching\"\"\"\n    if not predicted_boxes or not ground_truth_boxes:\n        return {\n            'detection_precision': 0.0,\n            'detection_recall': 0.0,\n            'detection_f1': 0.0,\n            'avg_iou': 0.0,\n            'matched_pairs': 0,\n            'total_predicted': len(predicted_boxes),\n            'total_ground_truth': len(ground_truth_boxes)\n        }\n    \n    # Find best matches between predicted and ground truth boxes\n    matched_pairs = []\n    used_gt_indices = set()\n    \n    for pred_idx, pred_box in enumerate(predicted_boxes):\n        best_iou = 0.0\n        best_gt_idx = -1\n        \n        for gt_idx, gt_box in enumerate(ground_truth_boxes):\n            if gt_idx in used_gt_indices:\n                continue\n            \n            iou = calculate_iou(pred_box['box'], gt_box['box'])\n            if iou > best_iou and iou >= iou_threshold:\n                best_iou = iou\n                best_gt_idx = gt_idx\n        \n        if best_gt_idx != -1:\n            matched_pairs.append({\n                'pred_idx': pred_idx,\n                'gt_idx': best_gt_idx,\n                'iou': best_iou,\n                'pred_text': pred_box['text'],\n                'gt_text': ground_truth_boxes[best_gt_idx]['text']\n            })\n            used_gt_indices.add(best_gt_idx)\n    \n    # Calculate metrics\n    num_matches = len(matched_pairs)\n    detection_precision = num_matches / len(predicted_boxes) if predicted_boxes else 0.0\n    detection_recall = num_matches / len(ground_truth_boxes) if ground_truth_boxes else 0.0\n    detection_f1 = 2 * (detection_precision * detection_recall) / (detection_precision + detection_recall) if (detection_precision + detection_recall) > 0 else 0.0\n    avg_iou = sum(pair['iou'] for pair in matched_pairs) / num_matches if num_matches > 0 else 0.0\n    \n    return {\n        'detection_precision': detection_precision,\n        'detection_recall': detection_recall,\n        'detection_f1': detection_f1,\n        'avg_iou': avg_iou,\n        'matched_pairs': num_matches,\n        'total_predicted': len(predicted_boxes),\n        'total_ground_truth': len(ground_truth_boxes),\n        'matches': matched_pairs\n    }\n\ndef evaluate_ocr_with_spatial(predicted_text, ground_truth_text, predicted_boxes, ground_truth_boxes):\n    # Text based evaluation \n    text_metrics = evaluate_ocr_result(predicted_text, ground_truth_text)\n    \n    # Spatial evaluation \n    spatial_metrics = evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes)\n    \n    # Combined metrics\n    combined_metrics = {\n        # Text metrics\n        'text_similarity': text_metrics['similarity'],\n        'text_precision': text_metrics['precision'],\n        'text_recall': text_metrics['recall'],\n        'text_f1': text_metrics['f1'],\n        \n        # Spatial metrics\n        'detection_precision': spatial_metrics['detection_precision'],\n        'detection_recall': spatial_metrics['detection_recall'],\n        'detection_f1': spatial_metrics['detection_f1'],\n        'avg_iou': spatial_metrics['avg_iou'],\n        'matched_boxes': spatial_metrics['matched_pairs'],\n        'total_predicted_boxes': spatial_metrics['total_predicted'],\n        'total_ground_truth_boxes': spatial_metrics['total_ground_truth'],\n        \n        # Combined score (average of text F1 and detection F1)\n        'combined_f1': (text_metrics['f1'] + spatial_metrics['detection_f1']) / 2,\n        \n        # Additional info\n        'predicted_length': text_metrics.get('predicted_length', 0),\n        'ground_truth_length': text_metrics.get('ground_truth_length', 0),\n        'spatial_matches': spatial_metrics.get('matches', [])\n    }\n    \n    return combined_metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from difflib import SequenceMatcher\nimport re\n\ndef clean_text(text):\n    text = re.sub(r'\\s+', ' ', text.strip())\n    return text.lower()\n\ndef calculate_similarity(text1,text2):\n    clean1 = clean_text(text1)\n    clean2 = clean_text(text2)\n    return SequenceMatcher(None, clean1, clean2).ratio()\n\ndef calculate_word_accuracy(predicted, ground_truth):\n    pred_words = set(clean_text(predicted).split())\n    gt_words = set(clean_text(ground_truth).split())\n    \n    if not gt_words:\n        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n    \n    if not pred_words:\n        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n    \n    intersection = pred_words.intersection(gt_words)\n    \n    precision = len(intersection) / len(pred_words) if pred_words else 0.0\n    recall = len(intersection) / len(gt_words) if gt_words else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\ndef evaluate_ocr_result(predicted, ground_truth):\n    similarity = calculate_similarity(predicted, ground_truth)\n    word_metrics = calculate_word_accuracy(predicted, ground_truth)\n    \n    return {\n        'similarity': similarity,\n        'precision': word_metrics['precision'],\n        'recall': word_metrics['recall'],\n        'f1': word_metrics['f1'],\n        'predicted_length': len(predicted),\n        'ground_truth_length': len(ground_truth)\n    }","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"initialize_ocr_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:45:03.227035Z","iopub.execute_input":"2025-07-09T18:45:03.227343Z","iopub.status.idle":"2025-07-09T18:45:05.536758Z","shell.execute_reply.started":"2025-07-09T18:45:03.227323Z","shell.execute_reply":"2025-07-09T18:45:05.535985Z"}},"outputs":[{"name":"stdout","text":"Initializing OCR models...\n✗ EasyOCR initialization failed: name 'easyocr' is not defined\n✗ DocTR initialization failed: name 'ocr_predictor' is not defined\n✓ Surya OCR initialized\n✗ Google Cloud Vision initialization failed: name 'vision' is not defined\n\nAvailable models: ['tesseract', 'surya']\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['tesseract', 'surya']"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"surya_results = run_single_ocr_evaluation(all_samples,'surya')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:46:03.505674Z","iopub.execute_input":"2025-07-09T18:46:03.505956Z","iopub.status.idle":"2025-07-09T18:48:24.030970Z","shell.execute_reply.started":"2025-07-09T18:46:03.505935Z","shell.execute_reply":"2025-07-09T18:48:24.030148Z"}},"outputs":[{"name":"stdout","text":"Running surya on 20 imgs\nProcessing 1/20: 12_0bb90_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\nRecognizing Text: 100%|██████████| 122/122 [00:06<00:00, 17.64it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.335, P: 0.817, R: 0.801, F1: 0.809, Time: 8.89s\nProcessing 2/20: 14_e26ee_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\nRecognizing Text: 100%|██████████| 127/127 [00:07<00:00, 16.49it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.140, P: 0.976, R: 0.971, F1: 0.974, Time: 9.24s\nProcessing 3/20: 2_f1a3f_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\nRecognizing Text: 100%|██████████| 22/22 [00:03<00:00,  5.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.272, P: 0.557, R: 0.576, F1: 0.567, Time: 5.05s\nProcessing 4/20: 8_5eeab_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\nRecognizing Text: 100%|██████████| 120/120 [00:07<00:00, 16.95it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.347, P: 0.777, R: 0.770, F1: 0.774, Time: 8.66s\nProcessing 5/20: 10_10fe5_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\nRecognizing Text: 100%|██████████| 125/125 [00:07<00:00, 17.59it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.366, P: 0.821, R: 0.802, F1: 0.811, Time: 8.60s\nProcessing 6/20: 16_e8fb7_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\nRecognizing Text: 100%|██████████| 115/115 [00:08<00:00, 13.46it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.228, P: 0.937, R: 0.952, F1: 0.944, Time: 10.07s\nProcessing 7/20: 17_24e64_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\nRecognizing Text: 100%|██████████| 123/123 [00:08<00:00, 14.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.168, P: 0.944, R: 0.959, F1: 0.952, Time: 9.78s\nProcessing 8/20: 11_ef202_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\nRecognizing Text: 100%|██████████| 124/124 [00:06<00:00, 18.50it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.233, P: 0.814, R: 0.812, F1: 0.813, Time: 8.20s\nProcessing 9/20: 5_1df9d_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\nRecognizing Text: 100%|██████████| 122/122 [00:08<00:00, 14.61it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.225, P: 0.837, R: 0.812, F1: 0.824, Time: 9.94s\nProcessing 10/20: 18_ac1da_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\nRecognizing Text: 100%|██████████| 122/122 [00:09<00:00, 13.03it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.165, P: 0.918, R: 0.931, F1: 0.925, Time: 11.39s\nProcessing 11/20: 0060080406.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\nRecognizing Text: 100%|██████████| 76/76 [00:07<00:00,  9.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.062, P: 0.662, R: 0.718, F1: 0.689, Time: 8.04s\nProcessing 12/20: 0001477983.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\nRecognizing Text: 100%|██████████| 42/42 [00:03<00:00, 10.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.185, P: 0.856, R: 0.896, F1: 0.876, Time: 4.22s\nProcessing 13/20: 88547278_88547279.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nRecognizing Text: 100%|██████████| 56/56 [00:08<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.471, P: 0.545, R: 0.808, F1: 0.651, Time: 8.92s\nProcessing 14/20: 92094746.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\nRecognizing Text: 100%|██████████| 35/35 [00:02<00:00, 13.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.060, P: 0.703, R: 0.735, F1: 0.719, Time: 2.89s\nProcessing 15/20: 0001239897.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\nRecognizing Text: 100%|██████████| 56/56 [00:03<00:00, 17.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.048, P: 0.494, R: 0.472, F1: 0.483, Time: 3.46s\nProcessing 16/20: 00836816.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\nRecognizing Text: 100%|██████████| 36/36 [00:04<00:00,  8.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.238, P: 0.699, R: 0.711, F1: 0.705, Time: 4.50s\nProcessing 17/20: 00860012_00860014.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\nRecognizing Text: 100%|██████████| 68/68 [00:04<00:00, 15.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.024, P: 0.722, R: 0.712, F1: 0.717, Time: 4.81s\nProcessing 18/20: 71108371.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nRecognizing Text: 100%|██████████| 63/63 [00:04<00:00, 14.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.236, P: 0.915, R: 0.910, F1: 0.912, Time: 4.74s\nProcessing 19/20: 01122115.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\nRecognizing Text: 100%|██████████| 62/62 [00:03<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.124, P: 0.683, R: 0.658, F1: 0.670, Time: 4.14s\nProcessing 20/20: 00836244.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\nRecognizing Text: 100%|██████████| 87/87 [00:04<00:00, 19.52it/s]","output_type":"stream"},{"name":"stdout","text":"  Similarity: 0.094, P: 0.532, R: 0.517, F1: 0.524, Time: 4.80s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"spatial_surya_results = run_single_ocr_evaluation_with_spatial(all_samples,'surya')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:50:15.921811Z","iopub.execute_input":"2025-07-09T18:50:15.922099Z","iopub.status.idle":"2025-07-09T18:54:56.753591Z","shell.execute_reply.started":"2025-07-09T18:50:15.922080Z","shell.execute_reply":"2025-07-09T18:54:56.752555Z"}},"outputs":[{"name":"stdout","text":"Running surya on 20 samples...\nProcessing 1/20: 12_0bb90_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"\n\nDetecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n\nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\u001b[A\u001b[A\n\n\nRecognizing Text:   0%|          | 0/122 [00:00<?, ?it/s]\u001b[A\u001b[A\n\nRecognizing Text:   1%|          | 1/122 [00:04<09:39,  4.79s/it]\u001b[A\u001b[A\n\nRecognizing Text:   6%|▌         | 7/122 [00:05<01:01,  1.88it/s]\u001b[A\u001b[A\n\nRecognizing Text:   7%|▋         | 9/122 [00:05<00:45,  2.51it/s]\u001b[A\u001b[A\n\nRecognizing Text:  12%|█▏        | 15/122 [00:05<00:19,  5.38it/s]\u001b[A\u001b[A\n\nRecognizing Text:  15%|█▍        | 18/122 [00:05<00:14,  6.95it/s]\u001b[A\u001b[A\n\nRecognizing Text:  17%|█▋        | 21/122 [00:05<00:11,  8.81it/s]\u001b[A\u001b[A\n\nRecognizing Text:  20%|█▉        | 24/122 [00:05<00:08, 10.91it/s]\u001b[A\u001b[A\n\nRecognizing Text:  22%|██▏       | 27/122 [00:05<00:07, 13.13it/s]\u001b[A\u001b[A\n\nRecognizing Text:  25%|██▌       | 31/122 [00:05<00:05, 16.83it/s]\u001b[A\u001b[A\n\nRecognizing Text:  31%|███       | 38/122 [00:05<00:03, 25.50it/s]\u001b[A\u001b[A\n\nRecognizing Text:  35%|███▌      | 43/122 [00:06<00:02, 26.36it/s]\u001b[A\u001b[A\n\nRecognizing Text:  39%|███▊      | 47/122 [00:06<00:02, 28.00it/s]\u001b[A\u001b[A\n\nRecognizing Text:  44%|████▍     | 54/122 [00:06<00:01, 35.67it/s]\u001b[A\u001b[A\n\nRecognizing Text:  58%|█████▊    | 71/122 [00:06<00:00, 63.68it/s]\u001b[A\u001b[A\n\nRecognizing Text:  78%|███████▊  | 95/122 [00:06<00:00, 101.48it/s]\u001b[A\u001b[A\n\nRecognizing Text:  90%|█████████ | 110/122 [00:06<00:00, 109.08it/s]\u001b[A\u001b[A\n\nRecognizing Text: 100%|██████████| 122/122 [00:06<00:00, 17.60it/s] \u001b[A\u001b[A\n\n\nDetecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n\nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\u001b[A\u001b[A\n\n\nRecognizing Text:   0%|          | 0/122 [00:00<?, ?it/s]\u001b[A\u001b[A\n\nRecognizing Text:   1%|          | 1/122 [00:04<09:40,  4.80s/it]\u001b[A\u001b[A\n\nRecognizing Text:   6%|▌         | 7/122 [00:05<01:01,  1.88it/s]\u001b[A\u001b[A\n\nRecognizing Text:   7%|▋         | 9/122 [00:05<00:45,  2.50it/s]\u001b[A\u001b[A\n\nRecognizing Text:  12%|█▏        | 15/122 [00:05<00:19,  5.37it/s]\u001b[A\u001b[A\n\nRecognizing Text:  15%|█▍        | 18/122 [00:05<00:14,  6.94it/s]\u001b[A\u001b[A\n\nRecognizing Text:  17%|█▋        | 21/122 [00:05<00:11,  8.80it/s]\u001b[A\u001b[A\n\nRecognizing Text:  20%|█▉        | 24/122 [00:05<00:08, 10.89it/s]\u001b[A\u001b[A\n\nRecognizing Text:  22%|██▏       | 27/122 [00:05<00:07, 13.11it/s]\u001b[A\u001b[A\n\nRecognizing Text:  25%|██▌       | 31/122 [00:05<00:05, 16.85it/s]\u001b[A\u001b[A\n\nRecognizing Text:  31%|███       | 38/122 [00:06<00:03, 25.51it/s]\u001b[A\u001b[A\n\nRecognizing Text:  35%|███▌      | 43/122 [00:06<00:02, 26.39it/s]\u001b[A\u001b[A\n\nRecognizing Text:  39%|███▊      | 47/122 [00:06<00:02, 28.08it/s]\u001b[A\u001b[A\n\nRecognizing Text:  44%|████▍     | 54/122 [00:06<00:01, 35.77it/s]\u001b[A\u001b[A\n\nRecognizing Text:  58%|█████▊    | 71/122 [00:06<00:00, 63.82it/s]\u001b[A\u001b[A\n\nRecognizing Text:  78%|███████▊  | 95/122 [00:06<00:00, 101.58it/s]\u001b[A\u001b[A\n\nRecognizing Text:  90%|█████████ | 110/122 [00:06<00:00, 109.15it/s]\u001b[A\u001b[A\n\nRecognizing Text: 100%|██████████| 122/122 [00:06<00:00, 17.58it/s] \u001b[A\u001b[A\nRecognizing Text:   0%|          | 0/22 [01:20<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.809, Detection F1: 0.343, Combined: 0.576, IoU: 0.569\nProcessing 2/20: 14_e26ee_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\nRecognizing Text: 100%|██████████| 127/127 [00:07<00:00, 16.52it/s] \nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\nRecognizing Text: 100%|██████████| 127/127 [00:07<00:00, 16.57it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.974, Detection F1: 0.768, Combined: 0.871, IoU: 0.622\nProcessing 3/20: 2_f1a3f_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\nRecognizing Text: 100%|██████████| 22/22 [00:03<00:00,  5.80it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\nRecognizing Text: 100%|██████████| 22/22 [00:03<00:00,  5.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.567, Detection F1: 0.622, Combined: 0.594, IoU: 0.594\nProcessing 4/20: 8_5eeab_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\nRecognizing Text: 100%|██████████| 120/120 [00:07<00:00, 16.89it/s] \nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\nRecognizing Text: 100%|██████████| 120/120 [00:07<00:00, 16.90it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.774, Detection F1: 0.492, Combined: 0.633, IoU: 0.569\nProcessing 5/20: 10_10fe5_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\nRecognizing Text: 100%|██████████| 125/125 [00:07<00:00, 17.59it/s] \nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\nRecognizing Text: 100%|██████████| 125/125 [00:07<00:00, 17.47it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.811, Detection F1: 0.258, Combined: 0.535, IoU: 0.551\nProcessing 6/20: 16_e8fb7_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\nRecognizing Text: 100%|██████████| 115/115 [00:08<00:00, 13.33it/s] \nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\nRecognizing Text: 100%|██████████| 115/115 [00:08<00:00, 13.29it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.944, Detection F1: 0.678, Combined: 0.811, IoU: 0.558\nProcessing 7/20: 17_24e64_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\nRecognizing Text: 100%|██████████| 123/123 [00:08<00:00, 14.96it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\nRecognizing Text: 100%|██████████| 123/123 [00:08<00:00, 14.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.952, Detection F1: 0.854, Combined: 0.903, IoU: 0.619\nProcessing 8/20: 11_ef202_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\nRecognizing Text: 100%|██████████| 124/124 [00:06<00:00, 18.34it/s] \nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\nRecognizing Text: 100%|██████████| 124/124 [00:06<00:00, 18.37it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.813, Detection F1: 0.178, Combined: 0.495, IoU: 0.557\nProcessing 9/20: 5_1df9d_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\nRecognizing Text: 100%|██████████| 122/122 [00:08<00:00, 14.57it/s] \nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\nRecognizing Text: 100%|██████████| 122/122 [00:08<00:00, 14.57it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.824, Detection F1: 0.813, Combined: 0.818, IoU: 0.628\nProcessing 10/20: 18_ac1da_default.jpg\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\nRecognizing Text: 100%|██████████| 122/122 [00:09<00:00, 13.02it/s] \nDetecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\nRecognizing Text: 100%|██████████| 122/122 [00:09<00:00, 13.02it/s] \n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.925, Detection F1: 0.959, Combined: 0.942, IoU: 0.662\nProcessing 11/20: 0060080406.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\nRecognizing Text: 100%|██████████| 76/76 [00:07<00:00,  9.80it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\nRecognizing Text: 100%|██████████| 76/76 [00:07<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.689, Detection F1: 0.622, Combined: 0.656, IoU: 0.704\nProcessing 12/20: 0001477983.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\nRecognizing Text: 100%|██████████| 42/42 [00:03<00:00, 10.74it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\nRecognizing Text: 100%|██████████| 42/42 [00:03<00:00, 10.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.876, Detection F1: 0.447, Combined: 0.661, IoU: 0.731\nProcessing 13/20: 88547278_88547279.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\nRecognizing Text: 100%|██████████| 56/56 [00:08<00:00,  6.50it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\nRecognizing Text: 100%|██████████| 56/56 [00:08<00:00,  6.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.651, Detection F1: 0.521, Combined: 0.586, IoU: 0.733\nProcessing 14/20: 92094746.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\nRecognizing Text: 100%|██████████| 35/35 [00:02<00:00, 13.39it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\nRecognizing Text: 100%|██████████| 35/35 [00:02<00:00, 13.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.719, Detection F1: 0.194, Combined: 0.456, IoU: 0.657\nProcessing 15/20: 0001239897.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\nRecognizing Text: 100%|██████████| 56/56 [00:03<00:00, 17.64it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\nRecognizing Text: 100%|██████████| 56/56 [00:03<00:00, 17.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.483, Detection F1: 0.621, Combined: 0.552, IoU: 0.771\nProcessing 16/20: 00836816.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\nRecognizing Text: 100%|██████████| 36/36 [00:04<00:00,  8.59it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\nRecognizing Text: 100%|██████████| 36/36 [00:04<00:00,  8.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.705, Detection F1: 0.585, Combined: 0.645, IoU: 0.729\nProcessing 17/20: 00860012_00860014.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\nRecognizing Text: 100%|██████████| 68/68 [00:04<00:00, 15.14it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\nRecognizing Text: 100%|██████████| 68/68 [00:04<00:00, 15.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.717, Detection F1: 0.744, Combined: 0.731, IoU: 0.717\nProcessing 18/20: 71108371.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\nRecognizing Text: 100%|██████████| 63/63 [00:04<00:00, 14.21it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\nRecognizing Text: 100%|██████████| 63/63 [00:04<00:00, 14.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.912, Detection F1: 0.694, Combined: 0.803, IoU: 0.757\nProcessing 19/20: 01122115.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\nRecognizing Text: 100%|██████████| 62/62 [00:03<00:00, 16.21it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\nRecognizing Text: 100%|██████████| 62/62 [00:03<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.670, Detection F1: 0.179, Combined: 0.425, IoU: 0.557\nProcessing 20/20: 00836244.png\n","output_type":"stream"},{"name":"stderr","text":"Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\nRecognizing Text: 100%|██████████| 87/87 [00:04<00:00, 19.56it/s]\nDetecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\nRecognizing Text: 100%|██████████| 87/87 [00:04<00:00, 19.42it/s]","output_type":"stream"},{"name":"stdout","text":"  Text F1: 0.524, Detection F1: 0.859, Combined: 0.692, IoU: 0.699\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"doctr_results = run_single_ocr_evaluation(all_samples, 'doctr')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spatial_doctr_results = run_single_ocr_evaluation_with_spatial(all_samples,'doctr')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"easyocr_results = run_single_ocr_evaluation(all_samples, 'easyocr')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spatial_easyocr_results = run_single_ocr_evaluation_with_spatial(all_samples, 'easyocr')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tesseract_results = run_single_ocr_evaluation(all_samples, 'tesseract')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gg_vision_results = run_single_ocr_evaluation(all_samples,'google_vision')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"spatial_gg_vision_results = run_single_ocr_evaluation_with_spatial(all_samples,'google_vision')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\n\ndef convert_numpy_types(obj):\n    if isinstance(obj, dict):\n        return {key: convert_numpy_types(value) for key, value in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_numpy_types(item) for item in obj]\n    elif isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\n# Convert all results\nall_ocr_results = {\n    # 'easyocr': easyocr_results,\n    # 'tesseract': tesseract_results,\n    # 'google_vision': gg_vision_results, \n    # 'spatial_easyocr': spatial_easyocr_results,\n    # 'spatial_google_vision': spatial_gg_vision_results,\n    # 'doctr': doctr_results,\n    # 'spatial_doctr': spatial_doctr_results\n    'surya':surya_results,\n    'spatial_surya':spatial_surya_results\n}\n\nconverted_results = convert_numpy_types(all_ocr_results)\n\nwith open('all_ocr_results_surya.json', 'w', encoding='utf-8') as f:\n    json.dump(converted_results, f, indent=2, ensure_ascii=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:56:38.439223Z","iopub.execute_input":"2025-07-09T18:56:38.439791Z","iopub.status.idle":"2025-07-09T18:56:38.508440Z","shell.execute_reply.started":"2025-07-09T18:56:38.439750Z","shell.execute_reply":"2025-07-09T18:56:38.507875Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Test Spatial Evaluation with Bounding Boxes","metadata":{}}]}