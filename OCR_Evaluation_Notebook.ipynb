{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Models Evaluation on French and English Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (0.3.13)\n",
      "Requirement already satisfied: easyocr in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: paddlepaddle in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: paddleocr in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: kraken in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (5.3.0)\n",
      "Requirement already satisfied: pillow in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (10.2.0)\n",
      "Requirement already satisfied: opencv-python in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: torch in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (2.4.1)\n",
      "Requirement already satisfied: torchvision>=0.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (0.19.1)\n",
      "Requirement already satisfied: opencv-python-headless in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (4.11.0.86)\n",
      "Requirement already satisfied: scipy in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (1.13.1)\n",
      "Requirement already satisfied: numpy in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (2.0.2)\n",
      "Requirement already satisfied: scikit-image in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (0.24.0)\n",
      "Requirement already satisfied: python-bidi in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (0.6.6)\n",
      "Requirement already satisfied: PyYAML in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (6.0.2)\n",
      "Requirement already satisfied: Shapely in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (2.0.7)\n",
      "Requirement already satisfied: pyclipper in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from easyocr) (1.11.1.4)\n",
      "Requirement already satisfied: httpx in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlepaddle) (0.27.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlepaddle) (5.29.5)\n",
      "Requirement already satisfied: decorator in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlepaddle) (5.1.1)\n",
      "Requirement already satisfied: opt_einsum==3.3.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: networkx in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlepaddle) (3.4.2)\n",
      "Requirement already satisfied: typing_extensions in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlepaddle) (4.12.2)\n",
      "Requirement already satisfied: paddlex>=3.1.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.1.2)\n",
      "Requirement already satisfied: jsonschema in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (4.20.0)\n",
      "Requirement already satisfied: lxml in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (5.3.1)\n",
      "Requirement already satisfied: requests in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (2.32.3)\n",
      "Requirement already satisfied: click>=8.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (8.1.8)\n",
      "Requirement already satisfied: regex in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (2024.11.6)\n",
      "Requirement already satisfied: coremltools~=8.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (8.3.0)\n",
      "Requirement already satisfied: jinja2~=3.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (3.1.5)\n",
      "Requirement already satisfied: scikit-learn~=1.5.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (1.5.2)\n",
      "Requirement already satisfied: pyarrow in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (20.0.0)\n",
      "Requirement already satisfied: lightning~=2.4.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (2.4.0)\n",
      "Requirement already satisfied: torchmetrics>=1.1.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (1.7.4)\n",
      "Requirement already satisfied: threadpoolctl~=3.5.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (3.5.0)\n",
      "Requirement already satisfied: importlib-resources>=1.3.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (6.5.2)\n",
      "Requirement already satisfied: rich in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from kraken) (13.9.4)\n",
      "Requirement already satisfied: sympy in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from coremltools~=8.1->kraken) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from coremltools~=8.1->kraken) (4.67.1)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from coremltools~=8.1->kraken) (25.1.0)\n",
      "Requirement already satisfied: cattrs in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from coremltools~=8.1->kraken) (25.1.1)\n",
      "Requirement already satisfied: pyaml in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from coremltools~=8.1->kraken) (25.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from jinja2~=3.0->kraken) (2.1.5)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (2025.5.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from lightning~=2.4.0->kraken) (0.14.3)\n",
      "Requirement already satisfied: pytorch-lightning in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from lightning~=2.4.0->kraken) (2.5.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (3.11.14)\n",
      "Requirement already satisfied: setuptools in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning~=2.4.0->kraken) (65.5.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-image->easyocr) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-image->easyocr) (2025.2.18)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-learn~=1.5.0->kraken) (1.4.2)\n",
      "Requirement already satisfied: filelock in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (3.18.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from torch->easyocr) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->easyocr) (12.6.85)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning~=2.4.0->kraken) (3.10)\n",
      "Requirement already satisfied: chardet in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.2.0)\n",
      "Requirement already satisfied: colorlog in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (6.9.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.33.0)\n",
      "Requirement already satisfied: pandas>=1.3 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.3.0)\n",
      "Requirement already satisfied: prettytable in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.16.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.8.2)\n",
      "Requirement already satisfied: ruamel.yaml in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.18.14)\n",
      "Requirement already satisfied: ujson in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.10.0)\n",
      "Requirement already satisfied: ftfy in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (6.3.1)\n",
      "Requirement already satisfied: imagesize in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.4.1)\n",
      "Requirement already satisfied: langchain>=0.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.26)\n",
      "Requirement already satisfied: langchain-community>=0.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.27)\n",
      "Requirement already satisfied: langchain-core in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.68)\n",
      "Requirement already satisfied: langchain-openai>=0.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.27)\n",
      "Requirement already satisfied: openai>=1.63 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.93.2)\n",
      "Requirement already satisfied: opencv-contrib-python==4.10.0.84 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.10.0.84)\n",
      "Requirement already satisfied: openpyxl in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.1.5)\n",
      "Requirement already satisfied: premailer in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.10.0)\n",
      "Requirement already satisfied: pypdfium2>=4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.30.1)\n",
      "Requirement already satisfied: tokenizers>=0.19 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.21.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.13.3)\n",
      "Requirement already satisfied: einops in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.8.1)\n",
      "Requirement already satisfied: GPUtil>=1.4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.4.0)\n",
      "Requirement already satisfied: tiktoken in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.4.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain-core->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain-core->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pydantic>=2->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pydantic>=2->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests->kraken) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests->kraken) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests->kraken) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.1.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.6.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.4.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.63->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from httpx->paddlepaddle) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain>=0.2->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.17.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from huggingface_hub->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from beautifulsoup4->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.6)\n",
      "Requirement already satisfied: wcwidth in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from ftfy->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.2.13)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from jsonschema->kraken) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from jsonschema->kraken) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from jsonschema->kraken) (0.23.1)\n",
      "Requirement already satisfied: et-xmlfile in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from openpyxl->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.0.0)\n",
      "Requirement already satisfied: cssselect in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (1.3.0)\n",
      "Requirement already satisfied: cssutils in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (2.11.1)\n",
      "Requirement already satisfied: cachetools in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (5.5.0)\n",
      "Requirement already satisfied: more-itertools in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from cssutils->premailer->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (10.6.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from rich->kraken) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from rich->kraken) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->kraken) (0.1.2)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from ruamel.yaml->paddlex>=3.1.0->paddlex[ie,multimodal,ocr,trans]>=3.1.0->paddleocr) (0.2.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from sympy->coremltools~=8.1->kraken) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-vision in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (3.10.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-cloud-vision) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-cloud-vision) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-cloud-vision) (5.29.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2025.1.31)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.6.1)\n",
      "Requirement already satisfied: lxml in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (5.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: matplotlib in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pytesseract easyocr paddlepaddle paddleocr kraken pillow opencv-python\n",
    "!pip install google-cloud-vision  # Only if you have Google Cloud credentials\n",
    "!pip install lxml beautifulsoup4  # For XML parsing\n",
    "!pip install matplotlib seaborn pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OCR Libraries\n",
    "import pytesseract\n",
    "import easyocr\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "\n",
    "import kraken\n",
    "from kraken import pageseg\n",
    "from kraken import rpred\n",
    "\n",
    "\n",
    "from google.cloud import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify this line if you placed credentials in project directory:\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/duyle/Documents/Case-Study2/active-sun-455914-a1-141238abf96c.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French dataset path: /home/duyle/Documents/Case-Study2/French_OCR_dataset/\n",
      "English dataset path: /home/duyle/Documents/Case-Study2/English_OCR_dataset/\n",
      "Total samples to process: 20\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "FRENCH_DATASET_PATH = \"/home/duyle/Documents/Case-Study2/French_OCR_dataset/\"\n",
    "ENGLISH_DATASET_PATH = \"/home/duyle/Documents/Case-Study2/English_OCR_dataset/\"\n",
    "\n",
    "# Number of samples per dataset\n",
    "SAMPLES_PER_DATASET = 10\n",
    "TOTAL_SAMPLES = SAMPLES_PER_DATASET * 2\n",
    "\n",
    "print(f\"French dataset path: {FRENCH_DATASET_PATH}\")\n",
    "print(f\"English dataset path: {ENGLISH_DATASET_PATH}\")\n",
    "print(f\"Total samples to process: {TOTAL_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Sampling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_french_samples(dataset_path, num_samples):\n",
    "    samples = []\n",
    "    \n",
    "    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    valid_pairs = []\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        base_name = img_file.replace('_default.jpg', '')  \n",
    "        xml_file = None\n",
    "        \n",
    "        possible_xml = [\n",
    "            f\"{base_name}_default.xml\",  \n",
    "            f\"{base_name[:-1]}g_default.xml\",\n",
    "        ]\n",
    "        \n",
    "        for xml_name in possible_xml:\n",
    "            if os.path.exists(os.path.join(dataset_path, xml_name)):\n",
    "                xml_file = xml_name\n",
    "                break\n",
    "        \n",
    "        if xml_file:\n",
    "            valid_pairs.append((img_file, xml_file))\n",
    "    \n",
    "    \n",
    "    selected_pairs = random.sample(valid_pairs, min(num_samples, len(valid_pairs)))\n",
    "    \n",
    "    for img_file, xml_file in selected_pairs:\n",
    "        samples.append({\n",
    "            'dataset': 'French',\n",
    "            'image_path': os.path.join(dataset_path, img_file),\n",
    "            'annotation_path': os.path.join(dataset_path, xml_file),\n",
    "            'image_name': img_file,\n",
    "            'annotation_name': xml_file\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def get_english_samples(dataset_path, num_samples):\n",
    "    \"\"\"Get random samples from English OCR dataset (JSON format)\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    images_path = os.path.join(dataset_path, 'images')\n",
    "    annotations_path = os.path.join(dataset_path, 'annotations')\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.png')]\n",
    "    selected_files = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    for img_file in selected_files:\n",
    "        # Find corresponding JSON file\n",
    "        base_name = img_file.replace('.png', '')\n",
    "        json_file = f\"{base_name}.json\"\n",
    "        \n",
    "        json_path = os.path.join(annotations_path, json_file)\n",
    "        if os.path.exists(json_path):\n",
    "            samples.append({\n",
    "                'dataset': 'English',\n",
    "                'image_path': os.path.join(images_path, img_file),\n",
    "                'annotation_path': json_path,\n",
    "                'image_name': img_file,\n",
    "                'annotation_name': json_file\n",
    "            })\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_french_xml(xml_path):\n",
    "    \"\"\"Extract ground truth text from French dataset XML file\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Find all String elements with CONTENT attribute\n",
    "        texts = []\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            if content:\n",
    "                texts.append(content)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML {xml_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_english_json(json_path):\n",
    "    \"\"\"Extract ground truth text from English dataset JSON file\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        texts = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON {json_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_boxes_from_french_xml(xml_path):\n",
    "    \"\"\"Extract bounding boxes and text from French dataset XML file\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        # Find all String elements with coordinates and content\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            hpos = string_elem.get('HPOS')\n",
    "            vpos = string_elem.get('VPOS')\n",
    "            width = string_elem.get('WIDTH')\n",
    "            height = string_elem.get('HEIGHT')\n",
    "            \n",
    "            if content and hpos and vpos and width and height:\n",
    "                # Convert to integers and create bounding box [x1, y1, x2, y2]\n",
    "                x1 = int(float(hpos))\n",
    "                y1 = int(float(vpos))\n",
    "                x2 = x1 + int(float(width))\n",
    "                y2 = y1 + int(float(height))\n",
    "                \n",
    "                boxes.append({\n",
    "                    'text': content,\n",
    "                    'box': [x1, y1, x2, y2]\n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML boxes {xml_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_boxes_from_english_json(json_path):\n",
    "    \"\"\"Extract bounding boxes and text from English dataset JSON file\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        boxes = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            box = form_item.get('box', [])\n",
    "            \n",
    "            if text and len(box) == 4:\n",
    "                boxes.append({\n",
    "                    'text': text,\n",
    "                    'box': box  # Already in [x1, y1, x2, y2] format\n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON boxes {json_path}: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset and Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French samples: 10\n",
      "English samples: 10\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get samples from both datasets\n",
    "french_samples = get_french_samples(FRENCH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "english_samples = get_english_samples(ENGLISH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "\n",
    "all_samples = french_samples + english_samples\n",
    "\n",
    "print(f\"French samples: {len(french_samples)}\")\n",
    "print(f\"English samples: {len(english_samples)}\")\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "\n",
    "# Load ground truth text and bounding boxes for all samples\n",
    "for sample in all_samples:\n",
    "    if sample['dataset'] == 'French':\n",
    "        sample['ground_truth'] = extract_text_from_french_xml(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_french_xml(sample['annotation_path'])\n",
    "    else:\n",
    "        sample['ground_truth'] = extract_text_from_english_json(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_english_json(sample['annotation_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Model Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to store initialized models\n",
    "easyocr_reader = None\n",
    "paddleocr_reader = None\n",
    "google_client = None\n",
    "\n",
    "# Check model availability\n",
    "models_available = {\n",
    "    'tesseract': True,\n",
    "    'easyocr': True,\n",
    "    'paddleocr': PaddleOCR is not None,\n",
    "    'kraken': kraken is not None,\n",
    "    'google_vision': vision is not None\n",
    "}\n",
    "\n",
    "def initialize_ocr_models():\n",
    "    \"\"\"Initialize all available OCR models\"\"\"\n",
    "    global easyocr_reader, paddleocr_reader, google_client\n",
    "    \n",
    "    print(\"Initializing OCR models...\")\n",
    "    \n",
    "    # Initialize EasyOCR\n",
    "    try:\n",
    "        if models_available['easyocr']:\n",
    "            easyocr_reader = easyocr.Reader(['en', 'fr'])\n",
    "            print(\"✓ EasyOCR initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ EasyOCR initialization failed: {e}\")\n",
    "        models_available['easyocr'] = False\n",
    "    \n",
    "    # Initialize PaddleOCR\n",
    "    try:\n",
    "        if models_available['paddleocr']:\n",
    "            paddleocr_reader = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "            print(\"✓ PaddleOCR initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ PaddleOCR initialization failed: {e}\")\n",
    "        models_available['paddleocr'] = False\n",
    "    \n",
    "    # Initialize Google Cloud Vision\n",
    "    try:\n",
    "        if models_available['google_vision']:\n",
    "            google_client = vision.ImageAnnotatorClient()\n",
    "            print(\"✓ Google Cloud Vision initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Google Cloud Vision initialization failed: {e}\")\n",
    "        models_available['google_vision'] = False\n",
    "    \n",
    "    available_models = [k for k, v in models_available.items() if v]\n",
    "    print(f\"\\nAvailable models: {available_models}\")\n",
    "    return available_models\n",
    "\n",
    "def tesseract_ocr(image_path):\n",
    "    \"\"\"Run Tesseract OCR on an image\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Tesseract error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def easyocr_ocr(image_path):\n",
    "    \"\"\"Run EasyOCR on an image\"\"\"\n",
    "    global easyocr_reader\n",
    "    try:\n",
    "        if not easyocr_reader:\n",
    "            print(\"EasyOCR not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        results = easyocr_reader.readtext(image_path)\n",
    "        text = ' '.join([result[1] for result in results])\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"EasyOCR error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def paddleocr_ocr(image_path):\n",
    "    \"\"\"Run PaddleOCR on an image\"\"\"\n",
    "    global paddleocr_reader\n",
    "    try:\n",
    "        if not paddleocr_reader:\n",
    "            print(\"PaddleOCR not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        results = paddleocr_reader.ocr(image_path, cls=True)\n",
    "        text_parts = []\n",
    "        for line in results:\n",
    "            if line:\n",
    "                for word_info in line:\n",
    "                    if len(word_info) > 1:\n",
    "                        text_parts.append(word_info[1][0])\n",
    "        return ' '.join(text_parts).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"PaddleOCR error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def kraken_ocr(image_path):\n",
    "    \"\"\"Run Kraken OCR on an image\"\"\"\n",
    "    try:\n",
    "        if not kraken:\n",
    "            return \"Kraken not available\"\n",
    "        # This is a simplified implementation\n",
    "        # Kraken requires more complex setup with models\n",
    "        return \"Kraken requires model setup\"\n",
    "    except Exception as e:\n",
    "        print(f\"Kraken error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def google_vision_ocr(image_path):\n",
    "    \"\"\"Run Google Cloud Vision OCR on an image\"\"\"\n",
    "    global google_client\n",
    "    try:\n",
    "        if not google_client:\n",
    "            print(\"Google Cloud Vision not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        \n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        response = google_client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "        \n",
    "        if texts:\n",
    "            return texts[0].description.strip()\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Google Vision error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box evaluation functions\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n",
    "    # box format: [x1, y1, x2, y2]\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    if x1_inter >= x2_inter or y1_inter >= y2_inter:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def get_ocr_boxes_with_text(ocr_model_name, image_path):\n",
    "    \"\"\"Get bounding boxes and text from OCR models that support it\"\"\"\n",
    "    boxes = []\n",
    "    \n",
    "    try:\n",
    "        if ocr_model_name == 'easyocr':\n",
    "            global easyocr_reader\n",
    "            if easyocr_reader:\n",
    "                results = easyocr_reader.readtext(image_path)\n",
    "                for result in results:\n",
    "                    # EasyOCR returns: [bbox_points, text, confidence]\n",
    "                    bbox_points = result[0]\n",
    "                    text = result[1]\n",
    "                    # Convert bbox points to [x1, y1, x2, y2]\n",
    "                    x_coords = [point[0] for point in bbox_points]\n",
    "                    y_coords = [point[1] for point in bbox_points]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        elif ocr_model_name == 'paddleocr':\n",
    "            global paddleocr_reader\n",
    "            if paddleocr_reader:\n",
    "                results = paddleocr_reader.ocr(image_path)\n",
    "                for line in results:\n",
    "                    if line:\n",
    "                        for word_info in line:\n",
    "                            if len(word_info) > 1:\n",
    "                                # PaddleOCR returns: [bbox_points, (text, confidence)]\n",
    "                                bbox_points = word_info[0]\n",
    "                                text = word_info[1][0]\n",
    "                                # Convert bbox points to [x1, y1, x2, y2]\n",
    "                                x_coords = [point[0] for point in bbox_points]\n",
    "                                y_coords = [point[1] for point in bbox_points]\n",
    "                                x1, x2 = min(x_coords), max(x_coords)\n",
    "                                y1, y2 = min(y_coords), max(y_coords)\n",
    "                                boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        elif ocr_model_name == 'google_vision':\n",
    "            global google_client\n",
    "            if google_client:\n",
    "                with open(image_path, 'rb') as image_file:\n",
    "                    content = image_file.read()\n",
    "                \n",
    "                image = vision.Image(content=content)\n",
    "                response = google_client.text_detection(image=image)\n",
    "                texts = response.text_annotations\n",
    "                \n",
    "                # Skip the first annotation (full text) and process individual words\n",
    "                for annotation in texts[1:]:\n",
    "                    text = annotation.description\n",
    "                    vertices = annotation.bounding_poly.vertices\n",
    "                    \n",
    "                    # Convert vertices to [x1, y1, x2, y2]\n",
    "                    x_coords = [vertex.x for vertex in vertices]\n",
    "                    y_coords = [vertex.y for vertex in vertices]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        # For other models (tesseract, kraken), we only have text\n",
    "        # so we can't do spatial evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting boxes from {ocr_model_name}: {e}\")\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Evaluate spatial accuracy using bounding box matching\"\"\"\n",
    "    if not predicted_boxes or not ground_truth_boxes:\n",
    "        return {\n",
    "            'detection_precision': 0.0,\n",
    "            'detection_recall': 0.0,\n",
    "            'detection_f1': 0.0,\n",
    "            'avg_iou': 0.0,\n",
    "            'matched_pairs': 0,\n",
    "            'total_predicted': len(predicted_boxes),\n",
    "            'total_ground_truth': len(ground_truth_boxes)\n",
    "        }\n",
    "    \n",
    "    # Find best matches between predicted and ground truth boxes\n",
    "    matched_pairs = []\n",
    "    used_gt_indices = set()\n",
    "    \n",
    "    for pred_idx, pred_box in enumerate(predicted_boxes):\n",
    "        best_iou = 0.0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for gt_idx, gt_box in enumerate(ground_truth_boxes):\n",
    "            if gt_idx in used_gt_indices:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(pred_box['box'], gt_box['box'])\n",
    "            if iou > best_iou and iou >= iou_threshold:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_gt_idx != -1:\n",
    "            matched_pairs.append({\n",
    "                'pred_idx': pred_idx,\n",
    "                'gt_idx': best_gt_idx,\n",
    "                'iou': best_iou,\n",
    "                'pred_text': pred_box['text'],\n",
    "                'gt_text': ground_truth_boxes[best_gt_idx]['text']\n",
    "            })\n",
    "            used_gt_indices.add(best_gt_idx)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    num_matches = len(matched_pairs)\n",
    "    detection_precision = num_matches / len(predicted_boxes) if predicted_boxes else 0.0\n",
    "    detection_recall = num_matches / len(ground_truth_boxes) if ground_truth_boxes else 0.0\n",
    "    detection_f1 = 2 * (detection_precision * detection_recall) / (detection_precision + detection_recall) if (detection_precision + detection_recall) > 0 else 0.0\n",
    "    avg_iou = sum(pair['iou'] for pair in matched_pairs) / num_matches if num_matches > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'detection_precision': detection_precision,\n",
    "        'detection_recall': detection_recall,\n",
    "        'detection_f1': detection_f1,\n",
    "        'avg_iou': avg_iou,\n",
    "        'matched_pairs': num_matches,\n",
    "        'total_predicted': len(predicted_boxes),\n",
    "        'total_ground_truth': len(ground_truth_boxes),\n",
    "        'matches': matched_pairs\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_with_spatial(predicted_text, ground_truth_text, predicted_boxes, ground_truth_boxes):\n",
    "    \"\"\"Comprehensive OCR evaluation including both text and spatial metrics\"\"\"\n",
    "    # Text-based evaluation (existing)\n",
    "    text_metrics = evaluate_ocr_result(predicted_text, ground_truth_text)\n",
    "    \n",
    "    # Spatial evaluation (new)\n",
    "    spatial_metrics = evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes)\n",
    "    \n",
    "    # Combined metrics\n",
    "    combined_metrics = {\n",
    "        # Text metrics\n",
    "        'text_similarity': text_metrics['similarity'],\n",
    "        'text_precision': text_metrics['precision'],\n",
    "        'text_recall': text_metrics['recall'],\n",
    "        'text_f1': text_metrics['f1'],\n",
    "        \n",
    "        # Spatial metrics\n",
    "        'detection_precision': spatial_metrics['detection_precision'],\n",
    "        'detection_recall': spatial_metrics['detection_recall'],\n",
    "        'detection_f1': spatial_metrics['detection_f1'],\n",
    "        'avg_iou': spatial_metrics['avg_iou'],\n",
    "        'matched_boxes': spatial_metrics['matched_pairs'],\n",
    "        'total_predicted_boxes': spatial_metrics['total_predicted'],\n",
    "        'total_ground_truth_boxes': spatial_metrics['total_ground_truth'],\n",
    "        \n",
    "        # Combined score (average of text F1 and detection F1)\n",
    "        'combined_f1': (text_metrics['f1'] + spatial_metrics['detection_f1']) / 2,\n",
    "        \n",
    "        # Additional info\n",
    "        'predicted_length': text_metrics.get('predicted_length', 0),\n",
    "        'ground_truth_length': text_metrics.get('ground_truth_length', 0),\n",
    "        'spatial_matches': spatial_metrics.get('matches', [])\n",
    "    }\n",
    "    \n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for comparison\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text.lower()\n",
    "\n",
    "def calculate_similarity(text1,text2):\n",
    "    \"\"\"Calculate similarity between two texts using SequenceMatcher\"\"\"\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "    return SequenceMatcher(None, clean1, clean2).ratio()\n",
    "\n",
    "def calculate_word_accuracy(predicted, ground_truth):\n",
    "    \"\"\"Calculate word-level precision, recall, and F1-score\"\"\"\n",
    "    pred_words = set(clean_text(predicted).split())\n",
    "    gt_words = set(clean_text(ground_truth).split())\n",
    "    \n",
    "    if not gt_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    if not pred_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    intersection = pred_words.intersection(gt_words)\n",
    "    \n",
    "    precision = len(intersection) / len(pred_words) if pred_words else 0.0\n",
    "    recall = len(intersection) / len(gt_words) if gt_words else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_result(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
    "    \"\"\"Comprehensive evaluation of OCR result\"\"\"\n",
    "    similarity = calculate_similarity(predicted, ground_truth)\n",
    "    word_metrics = calculate_word_accuracy(predicted, ground_truth)\n",
    "    \n",
    "    return {\n",
    "        'similarity': similarity,\n",
    "        'precision': word_metrics['precision'],\n",
    "        'recall': word_metrics['recall'],\n",
    "        'f1': word_metrics['f1'],\n",
    "        'predicted_length': len(predicted),\n",
    "        'ground_truth_length': len(ground_truth)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run OCR Evaluation on All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_single_ocr_evaluation(samples, ocr_model_name):\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'tesseract': tesseract_ocr,\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'paddleocr': paddleocr_ocr,\n",
    "        'kraken': kraken_ocr,\n",
    "        'google_vision': google_vision_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} imgs\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate results\n",
    "        metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        print(f\"  Similarity: {metrics['similarity']:.3f}, P: {metrics['precision']:.3f}, R: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "\n",
    "        \n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_single_ocr_evaluation_with_spatial(samples, ocr_model_name):\n",
    "    \"\"\"Run OCR evaluation with both text and spatial metrics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'tesseract': tesseract_ocr,\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'paddleocr': paddleocr_ocr,\n",
    "        'kraken': kraken_ocr,\n",
    "        'google_vision': google_vision_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    # Check if model supports spatial evaluation\n",
    "    spatial_supported = ocr_model_name in ['easyocr', 'paddleocr', 'google_vision']\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} samples...\")\n",
    "    if spatial_supported:\n",
    "        print(f\"Spatial evaluation supported\")\n",
    "    else:\n",
    "        print(f\"Text-only evaluation (no bounding boxes)\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ground_truth_boxes': sample.get('ground_truth_boxes', []),\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get OCR text\n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Get OCR boxes if supported\n",
    "        predicted_boxes = []\n",
    "        if spatial_supported:\n",
    "            predicted_boxes = get_ocr_boxes_with_text(ocr_model_name, sample['image_path'])\n",
    "        \n",
    "        # Evaluate with spatial metrics if available\n",
    "        if spatial_supported and sample.get('ground_truth_boxes'):\n",
    "            metrics = evaluate_ocr_with_spatial(\n",
    "                predicted_text, \n",
    "                sample['ground_truth'],\n",
    "                predicted_boxes,\n",
    "                sample['ground_truth_boxes']\n",
    "            )\n",
    "            evaluation_type = 'spatial'\n",
    "        else:\n",
    "            # Fall back to text-only evaluation\n",
    "            metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "            evaluation_type = 'text_only'\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'predicted_boxes': predicted_boxes,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics,\n",
    "            'evaluation_type': evaluation_type\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        if evaluation_type == 'spatial':\n",
    "            print(f\"  Text F1: {metrics['text_f1']:.3f}, Detection F1: {metrics['detection_f1']:.3f}, Combined: {metrics['combined_f1']:.3f}, IoU: {metrics['avg_iou']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Similarity: {metrics['similarity']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OCR models...\n",
      "✓ EasyOCR initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "which: no ccache in (/home/duyle/.pyenv/versions/3.10.16/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/home/duyle/.local/share/flatpak/exports/bin:/var/lib/flatpak/exports/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin:/home/duyle/.local/share/bin)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 9327.58it/s]\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 7632.95it/s]\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 61908.55it/s]\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 67650.06it/s]\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 11174.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PaddleOCR initialized\n",
      "✓ Google Cloud Vision initialized\n",
      "\n",
      "Available models: ['tesseract', 'easyocr', 'paddleocr', 'kraken', 'google_vision']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tesseract', 'easyocr', 'paddleocr', 'kraken', 'google_vision']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_ocr_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running easyocr on 20 samples...\n",
      "Processing sample 1/20: 14_e26ee_default.jpg\n",
      "  Similarity: 0.092, P: 0.843, R: 0.865, F1: 0.854, Time: 24.16s\n",
      "Processing sample 2/20: 4_938dc_default.jpg\n",
      "  Similarity: 0.181, P: 0.697, R: 0.707, F1: 0.702, Time: 26.73s\n",
      "Processing sample 3/20: 18_ac1da_default.jpg\n",
      "  Similarity: 0.091, P: 0.809, R: 0.820, F1: 0.815, Time: 28.32s\n",
      "Processing sample 4/20: 17_24e64_default.jpg\n",
      "  Similarity: 0.148, P: 0.859, R: 0.868, F1: 0.864, Time: 29.21s\n",
      "Processing sample 5/20: 16_e8fb7_default.jpg\n",
      "  Similarity: 0.204, P: 0.841, R: 0.860, F1: 0.850, Time: 27.78s\n",
      "Processing sample 6/20: 8_5eeab_default.jpg\n",
      "  Similarity: 0.270, P: 0.666, R: 0.678, F1: 0.672, Time: 26.87s\n",
      "Processing sample 7/20: 13_e13eb_default.jpg\n",
      "  Similarity: 0.224, P: 0.690, R: 0.696, F1: 0.693, Time: 28.69s\n",
      "Processing sample 8/20: 12_0bb90_default.jpg\n",
      "  Similarity: 0.292, P: 0.649, R: 0.667, F1: 0.658, Time: 32.25s\n",
      "Processing sample 9/20: 7_f0ec7_default.jpg\n",
      "  Similarity: 0.034, P: 0.678, R: 0.680, F1: 0.679, Time: 33.86s\n",
      "Processing sample 10/20: 5_1df9d_default.jpg\n",
      "  Similarity: 0.195, P: 0.735, R: 0.726, F1: 0.731, Time: 32.28s\n",
      "Processing sample 11/20: 71202511.png\n",
      "  Similarity: 0.268, P: 0.585, R: 0.568, F1: 0.576, Time: 5.46s\n",
      "Processing sample 12/20: 0011856542.png\n",
      "  Similarity: 0.471, P: 0.552, R: 0.544, F1: 0.548, Time: 4.21s\n",
      "Processing sample 13/20: 88547278_88547279.png\n",
      "  Similarity: 0.396, P: 0.563, R: 0.633, F1: 0.596, Time: 7.32s\n",
      "Processing sample 14/20: 0060080406.png\n",
      "  Similarity: 0.032, P: 0.429, R: 0.401, F1: 0.415, Time: 6.19s\n",
      "Processing sample 15/20: 0060270727.png\n",
      "  Similarity: 0.229, P: 0.525, R: 0.486, F1: 0.505, Time: 5.44s\n",
      "Processing sample 16/20: 00283813.png\n",
      "  Similarity: 0.098, P: 0.629, R: 0.682, F1: 0.655, Time: 9.36s\n",
      "Processing sample 17/20: 80728670.png\n",
      "  Similarity: 0.356, P: 0.704, R: 0.633, F1: 0.667, Time: 5.05s\n",
      "Processing sample 18/20: 93455715.png\n",
      "  Similarity: 0.065, P: 0.700, R: 0.563, F1: 0.624, Time: 4.43s\n",
      "Processing sample 19/20: 716552.png\n",
      "  Similarity: 0.030, P: 0.385, R: 0.374, F1: 0.380, Time: 6.54s\n",
      "Processing sample 20/20: 11875011.png\n",
      "  Similarity: 0.518, P: 0.861, R: 0.868, F1: 0.865, Time: 7.08s\n"
     ]
    }
   ],
   "source": [
    "easyocr_results = run_single_ocr_evaluation(all_samples, 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running easyocr on 20 samples...\n",
      "Spatial evaluation supported\n",
      "Processing 1/20: 14_e26ee_default.jpg\n",
      "  Text F1: 0.854, Detection F1: 0.107, Combined: 0.480, IoU: 0.690\n",
      "Processing 2/20: 4_938dc_default.jpg\n",
      "  Text F1: 0.702, Detection F1: 0.127, Combined: 0.415, IoU: 0.652\n",
      "Processing 3/20: 18_ac1da_default.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spatial_results_easyocr \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_ocr_evaluation_with_spatial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43measyocr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 95\u001b[0m, in \u001b[0;36mrun_single_ocr_evaluation_with_spatial\u001b[0;34m(samples, ocr_model_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m predicted_boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spatial_supported:\n\u001b[0;32m---> 95\u001b[0m     predicted_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mget_ocr_boxes_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Evaluate with spatial metrics if available\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spatial_supported \u001b[38;5;129;01mand\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mget_ocr_boxes_with_text\u001b[0;34m(ocr_model_name, image_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m easyocr_reader\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m easyocr_reader:\n\u001b[0;32m---> 30\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43measyocr_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# EasyOCR returns: [bbox_points, text, confidence]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         bbox_points \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/easyocr/easyocr.py:468\u001b[0m, in \u001b[0;36mReader.readtext\u001b[0;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[1;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 468\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_cv_grey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizontal_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocklist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/easyocr/easyocr.py:384\u001b[0m, in \u001b[0;36mReader.recognize\u001b[0;34m(self, img_cv_grey, horizontal_list, free_list, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, contrast_ths, adjust_contrast, filter_ths, y_ths, x_ths, reformat, output_format)\u001b[0m\n\u001b[1;32m    382\u001b[0m     f_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    383\u001b[0m     image_list, max_width \u001b[38;5;241m=\u001b[39m get_image_list(h_list, f_list, img_cv_grey, model_height \u001b[38;5;241m=\u001b[39m imgH)\n\u001b[0;32m--> 384\u001b[0m     result0 \u001b[38;5;241m=\u001b[39m \u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharacter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mignore_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result0\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m free_list:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/easyocr/recognition.py:206\u001b[0m, in \u001b[0;36mget_text\u001b[0;34m(character, imgH, imgW, recognizer, converter, image_list, ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths, workers, device)\u001b[0m\n\u001b[1;32m    201\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m    202\u001b[0m     test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(workers), collate_fn\u001b[38;5;241m=\u001b[39mAlignCollate_normal, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# predict first round\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m result1 \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecognizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_max_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mignore_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_group_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeamWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# predict second round\u001b[39;00m\n\u001b[1;32m    210\u001b[0m low_confident_idx \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result1) \u001b[38;5;28;01mif\u001b[39;00m (item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m contrast_ths)]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/easyocr/recognition.py:104\u001b[0m, in \u001b[0;36mrecognizer_predict\u001b[0;34m(model, converter, test_loader, batch_max_length, ignore_idx, char_group_idx, decoder, beamWidth, device)\u001b[0m\n\u001b[1;32m    102\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_tensors \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    105\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    106\u001b[0m         image \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/torch/autograd/profiler.py:705\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 705\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/torch/_ops.py:888\u001b[0m, in \u001b[0;36mTorchBindOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m--> 888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_must_dispatch_in_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;66;03m# When any inputs are FakeScriptObject, we need to\u001b[39;00m\n\u001b[1;32m    890\u001b[0m         \u001b[38;5;66;03m# skip c++ dispatcher and dispatch in python through _get_dispatch of python_dispatcher\u001b[39;00m\n\u001b[1;32m    891\u001b[0m         \u001b[38;5;66;03m# because C++ dispatcher will check the schema and cannot recognize FakeScriptObject.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;66;03m# 1. We only register the torchbind op temporarily as effectful op because we only want\u001b[39;00m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;66;03m#    the effect token functionalization logic to be applied during tracing. Otherwise, the behavior\u001b[39;00m\n\u001b[1;32m    896\u001b[0m         \u001b[38;5;66;03m#    of the eagerly executing the op might change after tracing.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;66;03m# 2. We don't want to register the op as effectful for all torchbind ops in ctor because this might\u001b[39;00m\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;66;03m#    cause unexpected behavior for some autograd.profiler ops e.g. profiler._record_function_exit._RecordFunction.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_register_as_effectful_op_temporarily():\n\u001b[1;32m    900\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_dispatch_in_python(\n\u001b[1;32m    901\u001b[0m                 args, kwargs, self_\u001b[38;5;241m.\u001b[39m_fallthrough_keys()\n\u001b[1;32m    902\u001b[0m             )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/torch/_ops.py:944\u001b[0m, in \u001b[0;36m_must_dispatch_in_python\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_must_dispatch_in_python\u001b[39m(args, kwargs):\n\u001b[0;32m--> 944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpytree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_any\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_class_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFakeScriptObject\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/torch/utils/_pytree.py:1187\u001b[0m, in \u001b[0;36mtree_any\u001b[0;34m(pred, tree, is_leaf)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtree_any\u001b[39m(\n\u001b[1;32m   1182\u001b[0m     pred: Callable[[Any], \u001b[38;5;28mbool\u001b[39m],\n\u001b[1;32m   1183\u001b[0m     tree: PyTree,\n\u001b[1;32m   1184\u001b[0m     is_leaf: Optional[Callable[[PyTree], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1185\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1186\u001b[0m     flat_args \u001b[38;5;241m=\u001b[39m tree_iter(tree, is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/torch/utils/_pytree.py:888\u001b[0m, in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;66;03m# Recursively flatten the children\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m child_pytrees:\n\u001b[0;32m--> 888\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m tree_iter(child, is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/torch/utils/_pytree.py:879\u001b[0m, in \u001b[0;36mtree_iter\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtree_iter\u001b[39m(\n\u001b[1;32m    875\u001b[0m     tree: PyTree,\n\u001b[1;32m    876\u001b[0m     is_leaf: Optional[Callable[[PyTree], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[Any]:\n\u001b[1;32m    878\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get an iterator over the leaves of a pytree.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_leaf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_leaf\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    880\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m tree\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spatial_results_easyocr = run_single_ocr_evaluation_with_spatial(all_samples, 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Spatial Evaluation with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bounding box extraction on a sample\n",
    "sample = all_samples[0]\n",
    "print(f\"Testing on: {sample['image_name']} ({sample['dataset']})\")\n",
    "print(f\"Ground truth boxes: {len(sample.get('ground_truth_boxes', []))}\")\n",
    "\n",
    "# Show first few ground truth boxes\n",
    "for i, box in enumerate(sample.get('ground_truth_boxes', [])[:3]):\n",
    "    print(f\"  Box {i+1}: {box['box']} -> '{box['text'][:50]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with spatial evaluation (EasyOCR supports bounding boxes)\n",
    "print(\"=== Testing EasyOCR with Spatial Evaluation ===\")\n",
    "easyocr_spatial_results = run_single_ocr_evaluation_with_spatial(all_samples[:3], 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text-only vs spatial evaluation\n",
    "print(\"\\n=== Comparison: Text-only vs Spatial Evaluation ===\")\n",
    "sample_result = easyocr_spatial_results[0]\n",
    "metrics = sample_result['ocr_result']['metrics']\n",
    "\n",
    "if sample_result['ocr_result']['evaluation_type'] == 'spatial':\n",
    "    print(f\"Sample: {sample_result['image_name']}\")\n",
    "    print(f\"Text Metrics:\")\n",
    "    print(f\"  - Text F1: {metrics['text_f1']:.3f}\")\n",
    "    print(f\"  - Text Precision: {metrics['text_precision']:.3f}\")\n",
    "    print(f\"  - Text Recall: {metrics['text_recall']:.3f}\")\n",
    "    print(f\"Spatial Metrics:\")\n",
    "    print(f\"  - Detection F1: {metrics['detection_f1']:.3f}\")\n",
    "    print(f\"  - Detection Precision: {metrics['detection_precision']:.3f}\")\n",
    "    print(f\"  - Detection Recall: {metrics['detection_recall']:.3f}\")\n",
    "    print(f\"  - Average IoU: {metrics['avg_iou']:.3f}\")\n",
    "    print(f\"  - Matched Boxes: {metrics['matched_boxes']}/{metrics['total_ground_truth_boxes']}\")\n",
    "    print(f\"Combined F1 Score: {metrics['combined_f1']:.3f}\")\n",
    "else:\n",
    "    print(\"Spatial evaluation not available for this sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Google Vision with spatial evaluation\n",
    "print(\"=== Testing Google Vision with Spatial Evaluation ===\")\n",
    "google_spatial_results = run_single_ocr_evaluation_with_spatial(all_samples[:2], 'google_vision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_dataframe(results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Convert results to pandas DataFrame for analysis\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for result in results:\n",
    "        for model_name, model_result in result['ocr_results'].items():\n",
    "            row = {\n",
    "                'sample_id': result['sample_id'],\n",
    "                'dataset': result['dataset'],\n",
    "                'image_name': result['image_name'],\n",
    "                'model': model_name,\n",
    "                'processing_time': model_result.get('processing_time', 0),\n",
    "                'similarity': model_result['metrics']['similarity'],\n",
    "                'precision': model_result['metrics']['precision'],\n",
    "                'recall': model_result['metrics']['recall'],\n",
    "                'f1': model_result['metrics']['f1'],\n",
    "                'predicted_length': model_result['metrics'].get('predicted_length', 0),\n",
    "                'ground_truth_length': model_result['metrics'].get('ground_truth_length', 0),\n",
    "                'has_error': 'error' in model_result\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = create_results_dataframe(evaluation_results)\n",
    "print(f\"Results DataFrame shape: {df_results.shape}\")\n",
    "print(f\"Available models: {df_results['model'].unique()}\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by model\n",
    "summary_stats = df_results.groupby('model').agg({\n",
    "    'similarity': ['mean', 'std', 'min', 'max'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'processing_time': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"=== OCR Model Performance Summary ===\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by dataset (French vs English)\n",
    "dataset_performance = df_results.groupby(['dataset', 'model']).agg({\n",
    "    'similarity': 'mean',\n",
    "    'f1': 'mean',\n",
    "    'processing_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"=== Performance by Dataset ===\")\n",
    "dataset_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. F1 Score comparison\n",
    "sns.boxplot(data=df_results, x='model', y='f1', ax=axes[0,0])\n",
    "axes[0,0].set_title('F1 Score by OCR Model')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Similarity comparison\n",
    "sns.boxplot(data=df_results, x='model', y='similarity', ax=axes[0,1])\n",
    "axes[0,1].set_title('Text Similarity by OCR Model')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Processing time comparison\n",
    "sns.boxplot(data=df_results, x='model', y='processing_time', ax=axes[1,0])\n",
    "axes[1,0].set_title('Processing Time by OCR Model')\n",
    "axes[1,0].set_ylabel('Time (seconds)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Performance by dataset\n",
    "sns.barplot(data=df_results, x='model', y='f1', hue='dataset', ax=axes[1,1])\n",
    "axes[1,1].set_title('F1 Score by Model and Dataset')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ranking and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall ranking\n",
    "model_ranking = df_results.groupby('model').agg({\n",
    "    'f1': 'mean',\n",
    "    'similarity': 'mean',\n",
    "    'precision': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'processing_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Add pricing information\n",
    "pricing_info = {\n",
    "    'tesseract': 'Free',\n",
    "    'easyocr': 'Free',\n",
    "    'paddleocr': 'Free',\n",
    "    'kraken': 'Free',\n",
    "    'google_vision': '$1.50 per 1,000 images'\n",
    "}\n",
    "\n",
    "model_ranking['pricing'] = model_ranking.index.map(pricing_info)\n",
    "\n",
    "# Sort by F1 score\n",
    "model_ranking_sorted = model_ranking.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"=== OCR Model Ranking (by F1 Score) ===\")\n",
    "model_ranking_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results report\n",
    "final_report = {\n",
    "    'evaluation_summary': {\n",
    "        'total_samples': len(all_samples),\n",
    "        'french_samples': len(french_samples),\n",
    "        'english_samples': len(english_samples),\n",
    "        'models_tested': list(df_results['model'].unique()),\n",
    "        'evaluation_date': pd.Timestamp.now().isoformat()\n",
    "    },\n",
    "    'model_performance': model_ranking_sorted.to_dict('index'),\n",
    "    'dataset_performance': dataset_performance.to_dict(),\n",
    "    'pricing_information': pricing_info,\n",
    "    'detailed_results': evaluation_results\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = 'ocr_evaluation_results.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
