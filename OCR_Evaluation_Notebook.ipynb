{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Models Evaluation on French and English Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install pytesseract easyocr paddlepaddle paddleocr kraken pillow opencv-python\n",
    "# !pip install google-cloud-vision  \n",
    "# !pip install lxml beautifulsoup4 \n",
    "# !pip install matplotlib seaborn pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OCR Libraries\n",
    "import pytesseract\n",
    "import easyocr\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "\n",
    "import kraken\n",
    "from kraken import pageseg\n",
    "from kraken import rpred\n",
    "\n",
    "\n",
    "from google.cloud import vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/duyle/Documents/Case-Study2/active-sun-455914-a1-141238abf96c.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French dataset path: /home/duyle/Documents/Case-Study2/French_OCR_dataset/\n",
      "English dataset path: /home/duyle/Documents/Case-Study2/English_OCR_dataset/\n",
      "Total samples to process: 20\n"
     ]
    }
   ],
   "source": [
    "FRENCH_DATASET_PATH = \"/home/duyle/Documents/Case-Study2/French_OCR_dataset/\"\n",
    "ENGLISH_DATASET_PATH = \"/home/duyle/Documents/Case-Study2/English_OCR_dataset/\"\n",
    "\n",
    "SAMPLES_PER_DATASET = 10\n",
    "TOTAL_SAMPLES = SAMPLES_PER_DATASET * 2\n",
    "\n",
    "print(f\"French dataset path: {FRENCH_DATASET_PATH}\")\n",
    "print(f\"English dataset path: {ENGLISH_DATASET_PATH}\")\n",
    "print(f\"Total samples to process: {TOTAL_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Sampling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_french_samples(dataset_path, num_samples):\n",
    "    samples = []\n",
    "    \n",
    "    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    valid_pairs = []\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        base_name = img_file.replace('_default.jpg', '')  \n",
    "        xml_file = None\n",
    "        \n",
    "        possible_xml = [\n",
    "            f\"{base_name}_default.xml\",  \n",
    "            f\"{base_name[:-1]}g_default.xml\",\n",
    "        ]\n",
    "        \n",
    "        for xml_name in possible_xml:\n",
    "            if os.path.exists(os.path.join(dataset_path, xml_name)):\n",
    "                xml_file = xml_name\n",
    "                break\n",
    "        \n",
    "        if xml_file:\n",
    "            valid_pairs.append((img_file, xml_file))\n",
    "    \n",
    "    \n",
    "    selected_pairs = random.sample(valid_pairs, min(num_samples, len(valid_pairs)))\n",
    "    \n",
    "    for img_file, xml_file in selected_pairs:\n",
    "        samples.append({\n",
    "            'dataset': 'French',\n",
    "            'image_path': os.path.join(dataset_path, img_file),\n",
    "            'annotation_path': os.path.join(dataset_path, xml_file),\n",
    "            'image_name': img_file,\n",
    "            'annotation_name': xml_file\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def get_english_samples(dataset_path, num_samples):\n",
    "    \"\"\"Get random samples from English OCR dataset (JSON format)\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    images_path = os.path.join(dataset_path, 'images')\n",
    "    annotations_path = os.path.join(dataset_path, 'annotations')\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.png')]\n",
    "    selected_files = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    for img_file in selected_files:\n",
    "        # Find corresponding JSON file\n",
    "        base_name = img_file.replace('.png', '')\n",
    "        json_file = f\"{base_name}.json\"\n",
    "        \n",
    "        json_path = os.path.join(annotations_path, json_file)\n",
    "        if os.path.exists(json_path):\n",
    "            samples.append({\n",
    "                'dataset': 'English',\n",
    "                'image_path': os.path.join(images_path, img_file),\n",
    "                'annotation_path': json_path,\n",
    "                'image_name': img_file,\n",
    "                'annotation_name': json_file\n",
    "            })\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_french_xml(xml_path):\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Find all String elements with CONTENT attribute\n",
    "        texts = []\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            if content:\n",
    "                texts.append(content)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML {xml_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_english_json(json_path):\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        texts = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON {json_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_boxes_from_french_xml(xml_path):\n",
    "    \"\"\"Extract bounding boxes and text from French dataset XML file\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        # Find all String elements with coordinates and content\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            hpos = string_elem.get('HPOS')\n",
    "            vpos = string_elem.get('VPOS')\n",
    "            width = string_elem.get('WIDTH')\n",
    "            height = string_elem.get('HEIGHT')\n",
    "            \n",
    "            if content and hpos and vpos and width and height:\n",
    "                x1 = int(float(hpos))\n",
    "                y1 = int(float(vpos))\n",
    "                x2 = x1 + int(float(width))\n",
    "                y2 = y1 + int(float(height))\n",
    "                \n",
    "                boxes.append({\n",
    "                    'text': content,\n",
    "                    'box': [x1, y1, x2, y2]\n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML boxes {xml_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_boxes_from_english_json(json_path):\n",
    "    \"\"\"Extract bounding boxes and text from English dataset JSON file\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        boxes = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            box = form_item.get('box', [])\n",
    "            \n",
    "            if text and len(box) == 4:\n",
    "                boxes.append({\n",
    "                    'text': text,\n",
    "                    'box': box \n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON boxes {json_path}: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset and Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French samples: 10\n",
      "English samples: 10\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get samples from both datasets\n",
    "french_samples = get_french_samples(FRENCH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "english_samples = get_english_samples(ENGLISH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "\n",
    "all_samples = french_samples + english_samples\n",
    "\n",
    "print(f\"French samples: {len(french_samples)}\")\n",
    "print(f\"English samples: {len(english_samples)}\")\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "\n",
    "# Load ground truth text and bounding boxes for all samples\n",
    "for sample in all_samples:\n",
    "    if sample['dataset'] == 'French':\n",
    "        sample['ground_truth'] = extract_text_from_french_xml(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_french_xml(sample['annotation_path'])\n",
    "    else:\n",
    "        sample['ground_truth'] = extract_text_from_english_json(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_english_json(sample['annotation_path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Model Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables to store initialized models\n",
    "easyocr_reader = None\n",
    "paddleocr_reader = None\n",
    "google_client = None\n",
    "\n",
    "# Check model availability\n",
    "models_available = {\n",
    "    'tesseract': True,\n",
    "    'easyocr': True,\n",
    "    'paddleocr': PaddleOCR is not None,\n",
    "    'kraken': kraken is not None,\n",
    "    'google_vision': vision is not None\n",
    "}\n",
    "\n",
    "def initialize_ocr_models():\n",
    "    \"\"\"Initialize all available OCR models\"\"\"\n",
    "    global easyocr_reader, paddleocr_reader, google_client\n",
    "    \n",
    "    print(\"Initializing OCR models...\")\n",
    "    \n",
    "    # Initialize EasyOCR\n",
    "    try:\n",
    "        if models_available['easyocr']:\n",
    "            easyocr_reader = easyocr.Reader(['en', 'fr'])\n",
    "            print(\"✓ EasyOCR initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ EasyOCR initialization failed: {e}\")\n",
    "        models_available['easyocr'] = False\n",
    "    \n",
    "    # Initialize PaddleOCR\n",
    "    try:\n",
    "        if models_available['paddleocr']:\n",
    "            paddleocr_reader = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "            print(\"✓ PaddleOCR initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ PaddleOCR initialization failed: {e}\")\n",
    "        models_available['paddleocr'] = False\n",
    "    \n",
    "    # Initialize Google Cloud Vision\n",
    "    try:\n",
    "        if models_available['google_vision']:\n",
    "            google_client = vision.ImageAnnotatorClient()\n",
    "            print(\"✓ Google Cloud Vision initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Google Cloud Vision initialization failed: {e}\")\n",
    "        models_available['google_vision'] = False\n",
    "    \n",
    "    available_models = [k for k, v in models_available.items() if v]\n",
    "    print(f\"\\nAvailable models: {available_models}\")\n",
    "    return available_models\n",
    "\n",
    "def tesseract_ocr(image_path):\n",
    "    \"\"\"Run Tesseract OCR on an image\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Tesseract error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def easyocr_ocr(image_path):\n",
    "    \"\"\"Run EasyOCR on an image\"\"\"\n",
    "    global easyocr_reader\n",
    "    try:\n",
    "        if not easyocr_reader:\n",
    "            print(\"EasyOCR not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        results = easyocr_reader.readtext(image_path)\n",
    "        text = ' '.join([result[1] for result in results])\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"EasyOCR error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def paddleocr_ocr(image_path):\n",
    "    \"\"\"Run PaddleOCR on an image with image resizing to prevent memory issues\"\"\"\n",
    "    global paddleocr_reader\n",
    "    try:\n",
    "        if not paddleocr_reader:\n",
    "            print(\"PaddleOCR not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Load and resize image to prevent memory issues\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Resize if image is too large\n",
    "        max_size = 3500  # Adjust this value as needed\n",
    "        if max(image.size) > max_size:\n",
    "            ratio = max_size / max(image.size)\n",
    "            new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Convert to numpy array for PaddleOCR\n",
    "            import numpy as np\n",
    "            image_array = np.array(image)\n",
    "            results = paddleocr_reader.ocr(image_array)\n",
    "        else:\n",
    "            results = paddleocr_reader.ocr(image_path)\n",
    "        \n",
    "        text_parts = []\n",
    "        for line in results:\n",
    "            if line:\n",
    "                for word_info in line:\n",
    "                    if len(word_info) > 1:\n",
    "                        text_parts.append(word_info[1][0])\n",
    "        \n",
    "        return ' '.join(text_parts).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"PaddleOCR error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def kraken_ocr(image_path):\n",
    "    \"\"\"Run Kraken OCR on an image\"\"\"\n",
    "    try:\n",
    "        if not kraken:\n",
    "            return \"Kraken not available\"\n",
    "        return \"Kraken requires model setup\"\n",
    "    except Exception as e:\n",
    "        print(f\"Kraken error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def google_vision_ocr(image_path):\n",
    "    \"\"\"Run Google Cloud Vision OCR on an image\"\"\"\n",
    "    global google_client\n",
    "    try:\n",
    "        if not google_client:\n",
    "            print(\"Google Cloud Vision not initialized. Run initialize_ocr_models() first.\")\n",
    "            return \"\"\n",
    "        \n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        response = google_client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "        \n",
    "        if texts:\n",
    "            return texts[0].description.strip()\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Google Vision error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box evaluation functions\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) between two bounding boxes\"\"\"\n",
    "    # box format: [x1, y1, x2, y2]\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    if x1_inter >= x2_inter or y1_inter >= y2_inter:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def get_ocr_boxes_with_text(ocr_model_name, image_path):\n",
    "    \"\"\"Get bounding boxes and text from OCR models that support it\"\"\"\n",
    "    boxes = []\n",
    "    \n",
    "    try:\n",
    "        if ocr_model_name == 'easyocr':\n",
    "            global easyocr_reader\n",
    "            if easyocr_reader:\n",
    "                results = easyocr_reader.readtext(image_path)\n",
    "                for result in results:\n",
    "                    # EasyOCR returns: [bbox_points, text, confidence]\n",
    "                    bbox_points = result[0]\n",
    "                    text = result[1]\n",
    "                    # Convert bbox points to [x1, y1, x2, y2]\n",
    "                    x_coords = [point[0] for point in bbox_points]\n",
    "                    y_coords = [point[1] for point in bbox_points]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        elif ocr_model_name == 'paddleocr':\n",
    "            global paddleocr_reader\n",
    "            if paddleocr_reader:\n",
    "                results = paddleocr_reader.ocr(image_path)\n",
    "                for line in results:\n",
    "                    if line:\n",
    "                        for word_info in line:\n",
    "                            if len(word_info) > 1:\n",
    "                                # PaddleOCR returns: [bbox_points, (text, confidence)]\n",
    "                                bbox_points = word_info[0]\n",
    "                                text = word_info[1][0]\n",
    "                                # Convert bbox points to [x1, y1, x2, y2]\n",
    "                                x_coords = [point[0] for point in bbox_points]\n",
    "                                y_coords = [point[1] for point in bbox_points]\n",
    "                                x1, x2 = min(x_coords), max(x_coords)\n",
    "                                y1, y2 = min(y_coords), max(y_coords)\n",
    "                                boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        elif ocr_model_name == 'google_vision':\n",
    "            global google_client\n",
    "            if google_client:\n",
    "                with open(image_path, 'rb') as image_file:\n",
    "                    content = image_file.read()\n",
    "                \n",
    "                image = vision.Image(content=content)\n",
    "                response = google_client.text_detection(image=image)\n",
    "                texts = response.text_annotations\n",
    "                \n",
    "                # Skip the first annotation (full text) and process individual words\n",
    "                for annotation in texts[1:]:\n",
    "                    text = annotation.description\n",
    "                    vertices = annotation.bounding_poly.vertices\n",
    "                    \n",
    "                    # Convert vertices to [x1, y1, x2, y2]\n",
    "                    x_coords = [vertex.x for vertex in vertices]\n",
    "                    y_coords = [vertex.y for vertex in vertices]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        # For other models (tesseract, kraken), we only have text\n",
    "        # so we can't do spatial evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting boxes from {ocr_model_name}: {e}\")\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "def evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Evaluate spatial accuracy using bounding box matching\"\"\"\n",
    "    if not predicted_boxes or not ground_truth_boxes:\n",
    "        return {\n",
    "            'detection_precision': 0.0,\n",
    "            'detection_recall': 0.0,\n",
    "            'detection_f1': 0.0,\n",
    "            'avg_iou': 0.0,\n",
    "            'matched_pairs': 0,\n",
    "            'total_predicted': len(predicted_boxes),\n",
    "            'total_ground_truth': len(ground_truth_boxes)\n",
    "        }\n",
    "    \n",
    "    # Find best matches between predicted and ground truth boxes\n",
    "    matched_pairs = []\n",
    "    used_gt_indices = set()\n",
    "    \n",
    "    for pred_idx, pred_box in enumerate(predicted_boxes):\n",
    "        best_iou = 0.0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for gt_idx, gt_box in enumerate(ground_truth_boxes):\n",
    "            if gt_idx in used_gt_indices:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(pred_box['box'], gt_box['box'])\n",
    "            if iou > best_iou and iou >= iou_threshold:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_gt_idx != -1:\n",
    "            matched_pairs.append({\n",
    "                'pred_idx': pred_idx,\n",
    "                'gt_idx': best_gt_idx,\n",
    "                'iou': best_iou,\n",
    "                'pred_text': pred_box['text'],\n",
    "                'gt_text': ground_truth_boxes[best_gt_idx]['text']\n",
    "            })\n",
    "            used_gt_indices.add(best_gt_idx)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    num_matches = len(matched_pairs)\n",
    "    detection_precision = num_matches / len(predicted_boxes) if predicted_boxes else 0.0\n",
    "    detection_recall = num_matches / len(ground_truth_boxes) if ground_truth_boxes else 0.0\n",
    "    detection_f1 = 2 * (detection_precision * detection_recall) / (detection_precision + detection_recall) if (detection_precision + detection_recall) > 0 else 0.0\n",
    "    avg_iou = sum(pair['iou'] for pair in matched_pairs) / num_matches if num_matches > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'detection_precision': detection_precision,\n",
    "        'detection_recall': detection_recall,\n",
    "        'detection_f1': detection_f1,\n",
    "        'avg_iou': avg_iou,\n",
    "        'matched_pairs': num_matches,\n",
    "        'total_predicted': len(predicted_boxes),\n",
    "        'total_ground_truth': len(ground_truth_boxes),\n",
    "        'matches': matched_pairs\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_with_spatial(predicted_text, ground_truth_text, predicted_boxes, ground_truth_boxes):\n",
    "    \"\"\"Comprehensive OCR evaluation including both text and spatial metrics\"\"\"\n",
    "    # Text-based evaluation (existing)\n",
    "    text_metrics = evaluate_ocr_result(predicted_text, ground_truth_text)\n",
    "    \n",
    "    # Spatial evaluation (new)\n",
    "    spatial_metrics = evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes)\n",
    "    \n",
    "    # Combined metrics\n",
    "    combined_metrics = {\n",
    "        # Text metrics\n",
    "        'text_similarity': text_metrics['similarity'],\n",
    "        'text_precision': text_metrics['precision'],\n",
    "        'text_recall': text_metrics['recall'],\n",
    "        'text_f1': text_metrics['f1'],\n",
    "        \n",
    "        # Spatial metrics\n",
    "        'detection_precision': spatial_metrics['detection_precision'],\n",
    "        'detection_recall': spatial_metrics['detection_recall'],\n",
    "        'detection_f1': spatial_metrics['detection_f1'],\n",
    "        'avg_iou': spatial_metrics['avg_iou'],\n",
    "        'matched_boxes': spatial_metrics['matched_pairs'],\n",
    "        'total_predicted_boxes': spatial_metrics['total_predicted'],\n",
    "        'total_ground_truth_boxes': spatial_metrics['total_ground_truth'],\n",
    "        \n",
    "        # Combined score (average of text F1 and detection F1)\n",
    "        'combined_f1': (text_metrics['f1'] + spatial_metrics['detection_f1']) / 2,\n",
    "        \n",
    "        # Additional info\n",
    "        'predicted_length': text_metrics.get('predicted_length', 0),\n",
    "        'ground_truth_length': text_metrics.get('ground_truth_length', 0),\n",
    "        'spatial_matches': spatial_metrics.get('matches', [])\n",
    "    }\n",
    "    \n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for comparison\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text.lower()\n",
    "\n",
    "def calculate_similarity(text1,text2):\n",
    "    \"\"\"Calculate similarity between two texts using SequenceMatcher\"\"\"\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "    return SequenceMatcher(None, clean1, clean2).ratio()\n",
    "\n",
    "def calculate_word_accuracy(predicted, ground_truth):\n",
    "    \"\"\"Calculate word-level precision, recall, and F1-score\"\"\"\n",
    "    pred_words = set(clean_text(predicted).split())\n",
    "    gt_words = set(clean_text(ground_truth).split())\n",
    "    \n",
    "    if not gt_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    if not pred_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    intersection = pred_words.intersection(gt_words)\n",
    "    \n",
    "    precision = len(intersection) / len(pred_words) if pred_words else 0.0\n",
    "    recall = len(intersection) / len(gt_words) if gt_words else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_result(predicted: str, ground_truth: str) -> Dict[str, float]:\n",
    "    \"\"\"Comprehensive evaluation of OCR result\"\"\"\n",
    "    similarity = calculate_similarity(predicted, ground_truth)\n",
    "    word_metrics = calculate_word_accuracy(predicted, ground_truth)\n",
    "    \n",
    "    return {\n",
    "        'similarity': similarity,\n",
    "        'precision': word_metrics['precision'],\n",
    "        'recall': word_metrics['recall'],\n",
    "        'f1': word_metrics['f1'],\n",
    "        'predicted_length': len(predicted),\n",
    "        'ground_truth_length': len(ground_truth)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run OCR Evaluation on All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_single_ocr_evaluation(samples, ocr_model_name):\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'tesseract': tesseract_ocr,\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'paddleocr': paddleocr_ocr,\n",
    "        'kraken': kraken_ocr,\n",
    "        'google_vision': google_vision_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} imgs\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate results\n",
    "        metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        print(f\"  Similarity: {metrics['similarity']:.3f}, P: {metrics['precision']:.3f}, R: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "\n",
    "        \n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_single_ocr_evaluation_with_spatial(samples, ocr_model_name):\n",
    "    \"\"\"Run OCR evaluation with both text and spatial metrics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'tesseract': tesseract_ocr,\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'paddleocr': paddleocr_ocr,\n",
    "        'kraken': kraken_ocr,\n",
    "        'google_vision': google_vision_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    # Check if model supports spatial evaluation\n",
    "    spatial_supported = ocr_model_name in ['easyocr', 'paddleocr', 'google_vision']\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} samples...\")\n",
    "    if spatial_supported:\n",
    "        print(f\"Spatial evaluation supported\")\n",
    "    else:\n",
    "        print(f\"Text-only evaluation (no bounding boxes)\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ground_truth_boxes': sample.get('ground_truth_boxes', []),\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get OCR text\n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Get OCR boxes if supported\n",
    "        predicted_boxes = []\n",
    "        if spatial_supported:\n",
    "            predicted_boxes = get_ocr_boxes_with_text(ocr_model_name, sample['image_path'])\n",
    "        \n",
    "        # Evaluate with spatial metrics if available\n",
    "        if spatial_supported and sample.get('ground_truth_boxes'):\n",
    "            metrics = evaluate_ocr_with_spatial(\n",
    "                predicted_text, \n",
    "                sample['ground_truth'],\n",
    "                predicted_boxes,\n",
    "                sample['ground_truth_boxes']\n",
    "            )\n",
    "            evaluation_type = 'spatial'\n",
    "        else:\n",
    "            # Fall back to text-only evaluation\n",
    "            metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "            evaluation_type = 'text_only'\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'predicted_boxes': predicted_boxes,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics,\n",
    "            'evaluation_type': evaluation_type\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        if evaluation_type == 'spatial':\n",
    "            print(f\"  Text F1: {metrics['text_f1']:.3f}, Detection F1: {metrics['detection_f1']:.3f}, Combined: {metrics['combined_f1']:.3f}, IoU: {metrics['avg_iou']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Similarity: {metrics['similarity']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OCR models...\n",
      "✓ EasyOCR initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "which: no ccache in (/home/duyle/.pyenv/versions/3.10.16/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/home/duyle/.local/share/flatpak/exports/bin:/var/lib/flatpak/exports/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin:/home/duyle/.local/share/bin)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 82510.90it/s]\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 15797.76it/s]\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 12473.77it/s]\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 8747.25it/s]\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /home/duyle/.paddlex/official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 57587.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PaddleOCR initialized\n",
      "✓ Google Cloud Vision initialized\n",
      "\n",
      "Available models: ['tesseract', 'easyocr', 'paddleocr', 'kraken', 'google_vision']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tesseract', 'easyocr', 'paddleocr', 'kraken', 'google_vision']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_ocr_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running paddleocr on 20 imgs\n",
      "Processing 1/20: 14_e26ee_default.jpg\n",
      "  Similarity: 0.000, P: 0.000, R: 0.000, F1: 0.000, Time: 267.55s\n",
      "Processing 2/20: 4_938dc_default.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m paddleocr_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_ocr_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaddleocr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m, in \u001b[0;36mrun_single_ocr_evaluation\u001b[0;34m(samples, ocr_model_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m sample_result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m: i,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m: sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mocr_result\u001b[39m\u001b[38;5;124m'\u001b[39m: {}\n\u001b[1;32m     27\u001b[0m }\n\u001b[1;32m     29\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 31\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m \u001b[43mocr_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m processing_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 97\u001b[0m, in \u001b[0;36mpaddleocr_ocr\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     image_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[0;32m---> 97\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpaddleocr_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     results \u001b[38;5;241m=\u001b[39m paddleocr_reader\u001b[38;5;241m.\u001b[39mocr(image_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/typing_extensions.py:2853\u001b[0m, in \u001b[0;36mdeprecated.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2852\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddleocr/_pipelines/ocr.py:225\u001b[0m, in \u001b[0;36mPaddleOCR.ocr\u001b[0;34m(self, img, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `predict` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mocr\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddleocr/_pipelines/ocr.py:208\u001b[0m, in \u001b[0;36mPaddleOCR.predict\u001b[0;34m(self, input, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_rec_score_thresh)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     text_rec_score_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_doc_orientation_classify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_doc_orientation_classify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_doc_unwarping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_doc_unwarping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_textline_orientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_textline_orientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_limit_side_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_limit_side_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_limit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_limit_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_box_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_box_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_unclip_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_unclip_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_rec_score_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_rec_score_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/pipelines/_parallel.py:129\u001b[0m, in \u001b[0;36mAutoParallelSimpleInferencePipeline.predict\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    133\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/pipelines/ocr/pipeline.py:433\u001b[0m, in \u001b[0;36m_OCRPipeline.predict\u001b[0;34m(self, input, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_max_side_limit, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_rec_score_thresh)\u001b[0m\n\u001b[1;32m    427\u001b[0m sorted_subs_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    428\u001b[0m     sub_img_info_list, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    430\u001b[0m sorted_subs_of_img \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    431\u001b[0m     all_subs_of_img[x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sorted_subs_info\n\u001b[1;32m    432\u001b[0m ]\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, rec_res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_rec_model(sorted_subs_of_img)\n\u001b[1;32m    435\u001b[0m ):\n\u001b[1;32m    436\u001b[0m     sub_img_id \u001b[38;5;241m=\u001b[39m sorted_subs_info[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    437\u001b[0m     sub_img_info_list[sub_img_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec_res\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m rec_res\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/base/predictor/base_predictor.py:211\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, input, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/base/predictor/base_predictor.py:267\u001b[0m, in \u001b[0;36mBasePredictor.apply\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m--> 267\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m PredictionWrap(prediction, \u001b[38;5;28mlen\u001b[39m(batch_data))\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_data)):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/text_recognition/predictor.py:75\u001b[0m, in \u001b[0;36mTextRecPredictor.process\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m     73\u001b[0m batch_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_tfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReisizeNorm\u001b[39m\u001b[38;5;124m\"\u001b[39m](imgs\u001b[38;5;241m=\u001b[39mbatch_raw_imgs)\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_tfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m](imgs\u001b[38;5;241m=\u001b[39mbatch_imgs)\n\u001b[0;32m---> 75\u001b[0m batch_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m texts, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_op(batch_preds)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_data\u001b[38;5;241m.\u001b[39minput_paths,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_data\u001b[38;5;241m.\u001b[39mpage_indexes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvis_font\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvis_font] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_raw_imgs),\n\u001b[1;32m     84\u001b[0m }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/common/static_infer.py:287\u001b[0m, in \u001b[0;36mPaddleInfer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    285\u001b[0m x \u001b[38;5;241m=\u001b[39m _sort_inputs(x, names)\n\u001b[1;32m    286\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(np\u001b[38;5;241m.\u001b[39mascontiguousarray, x))\n\u001b[0;32m--> 287\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/common/static_infer.py:252\u001b[0m, in \u001b[0;36mPaddleInferChainLegacy.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m     input_handle\u001b[38;5;241m.\u001b[39mreshape(input_\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    251\u001b[0m     input_handle\u001b[38;5;241m.\u001b[39mcopy_from_cpu(input_)\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mcopy_to_cpu() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_handles]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paddleocr_results = run_single_ocr_evaluation(all_samples, 'paddleocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running paddleocr on 20 samples...\n",
      "Spatial evaluation supported\n",
      "Processing 1/20: 14_e26ee_default.jpg\n",
      "PaddleOCR error on /home/duyle/Documents/Case-Study2/French_OCR_dataset/14_e26ee_default.jpg: PaddleOCR.predict() got an unexpected keyword argument 'cls'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mResized image size (4582x3131) exceeds max_side_limit of 4000. Resizing to fit within limit.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spatial_paddleocr_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single_ocr_evaluation_with_spatial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaddleocr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 95\u001b[0m, in \u001b[0;36mrun_single_ocr_evaluation_with_spatial\u001b[0;34m(samples, ocr_model_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m predicted_boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spatial_supported:\n\u001b[0;32m---> 95\u001b[0m     predicted_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mget_ocr_boxes_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocr_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Evaluate with spatial metrics if available\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spatial_supported \u001b[38;5;129;01mand\u001b[39;00m sample\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m, in \u001b[0;36mget_ocr_boxes_with_text\u001b[0;34m(ocr_model_name, image_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m paddleocr_reader\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m paddleocr_reader:\n\u001b[0;32m---> 45\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpaddleocr_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/typing_extensions.py:2853\u001b[0m, in \u001b[0;36mdeprecated.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2852\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddleocr/_pipelines/ocr.py:225\u001b[0m, in \u001b[0;36mPaddleOCR.ocr\u001b[0;34m(self, img, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `predict` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mocr\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddleocr/_pipelines/ocr.py:208\u001b[0m, in \u001b[0;36mPaddleOCR.predict\u001b[0;34m(self, input, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_rec_score_thresh)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     text_rec_score_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_doc_orientation_classify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_doc_orientation_classify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_doc_unwarping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_doc_unwarping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_textline_orientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_textline_orientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_limit_side_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_limit_side_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_limit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_limit_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_box_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_box_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_det_unclip_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_det_unclip_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_rec_score_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_rec_score_thresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/pipelines/_parallel.py:129\u001b[0m, in \u001b[0;36mAutoParallelSimpleInferencePipeline.predict\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    133\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/pipelines/ocr/pipeline.py:433\u001b[0m, in \u001b[0;36m_OCRPipeline.predict\u001b[0;34m(self, input, use_doc_orientation_classify, use_doc_unwarping, use_textline_orientation, text_det_limit_side_len, text_det_limit_type, text_det_max_side_limit, text_det_thresh, text_det_box_thresh, text_det_unclip_ratio, text_rec_score_thresh)\u001b[0m\n\u001b[1;32m    427\u001b[0m sorted_subs_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    428\u001b[0m     sub_img_info_list, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    430\u001b[0m sorted_subs_of_img \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    431\u001b[0m     all_subs_of_img[x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m sorted_subs_info\n\u001b[1;32m    432\u001b[0m ]\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, rec_res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_rec_model(sorted_subs_of_img)\n\u001b[1;32m    435\u001b[0m ):\n\u001b[1;32m    436\u001b[0m     sub_img_id \u001b[38;5;241m=\u001b[39m sorted_subs_info[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_img_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    437\u001b[0m     sub_img_info_list[sub_img_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec_res\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m rec_res\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/base/predictor/base_predictor.py:211\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, input, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/base/predictor/base_predictor.py:267\u001b[0m, in \u001b[0;36mBasePredictor.apply\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m--> 267\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m PredictionWrap(prediction, \u001b[38;5;28mlen\u001b[39m(batch_data))\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch_data)):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/text_recognition/predictor.py:75\u001b[0m, in \u001b[0;36mTextRecPredictor.process\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m     73\u001b[0m batch_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_tfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReisizeNorm\u001b[39m\u001b[38;5;124m\"\u001b[39m](imgs\u001b[38;5;241m=\u001b[39mbatch_raw_imgs)\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_tfs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m](imgs\u001b[38;5;241m=\u001b[39mbatch_imgs)\n\u001b[0;32m---> 75\u001b[0m batch_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m texts, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_op(batch_preds)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_data\u001b[38;5;241m.\u001b[39minput_paths,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_data\u001b[38;5;241m.\u001b[39mpage_indexes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvis_font\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvis_font] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_raw_imgs),\n\u001b[1;32m     84\u001b[0m }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/common/static_infer.py:287\u001b[0m, in \u001b[0;36mPaddleInfer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    285\u001b[0m x \u001b[38;5;241m=\u001b[39m _sort_inputs(x, names)\n\u001b[1;32m    286\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(np\u001b[38;5;241m.\u001b[39mascontiguousarray, x))\n\u001b[0;32m--> 287\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.16/lib/python3.10/site-packages/paddlex/inference/models/common/static_infer.py:252\u001b[0m, in \u001b[0;36mPaddleInferChainLegacy.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m     input_handle\u001b[38;5;241m.\u001b[39mreshape(input_\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    251\u001b[0m     input_handle\u001b[38;5;241m.\u001b[39mcopy_from_cpu(input_)\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mcopy_to_cpu() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_handles]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spatial_paddleocr_results = run_single_ocr_evaluation_with_spatial(all_samples, 'paddleocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_results = run_single_ocr_evaluation(all_samples, 'google_vision')\n",
    "spatial_google_results = run_single_ocr_evaluation_with_spatial(all_samples, 'google_vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_results = run_single_ocr_evaluation(all_samples, 'tesseract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Spatial Evaluation with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bounding box extraction on a sample\n",
    "sample = all_samples[0]\n",
    "print(f\"Testing on: {sample['image_name']} ({sample['dataset']})\")\n",
    "print(f\"Ground truth boxes: {len(sample.get('ground_truth_boxes', []))}\")\n",
    "\n",
    "# Show first few ground truth boxes\n",
    "for i, box in enumerate(sample.get('ground_truth_boxes', [])[:3]):\n",
    "    print(f\"  Box {i+1}: {box['box']} -> '{box['text'][:50]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with spatial evaluation (EasyOCR supports bounding boxes)\n",
    "print(\"=== Testing EasyOCR with Spatial Evaluation ===\")\n",
    "easyocr_spatial_results = run_single_ocr_evaluation_with_spatial(all_samples[:3], 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare text-only vs spatial evaluation\n",
    "print(\"\\n=== Comparison: Text-only vs Spatial Evaluation ===\")\n",
    "sample_result = easyocr_spatial_results[0]\n",
    "metrics = sample_result['ocr_result']['metrics']\n",
    "\n",
    "if sample_result['ocr_result']['evaluation_type'] == 'spatial':\n",
    "    print(f\"Sample: {sample_result['image_name']}\")\n",
    "    print(f\"Text Metrics:\")\n",
    "    print(f\"  - Text F1: {metrics['text_f1']:.3f}\")\n",
    "    print(f\"  - Text Precision: {metrics['text_precision']:.3f}\")\n",
    "    print(f\"  - Text Recall: {metrics['text_recall']:.3f}\")\n",
    "    print(f\"Spatial Metrics:\")\n",
    "    print(f\"  - Detection F1: {metrics['detection_f1']:.3f}\")\n",
    "    print(f\"  - Detection Precision: {metrics['detection_precision']:.3f}\")\n",
    "    print(f\"  - Detection Recall: {metrics['detection_recall']:.3f}\")\n",
    "    print(f\"  - Average IoU: {metrics['avg_iou']:.3f}\")\n",
    "    print(f\"  - Matched Boxes: {metrics['matched_boxes']}/{metrics['total_ground_truth_boxes']}\")\n",
    "    print(f\"Combined F1 Score: {metrics['combined_f1']:.3f}\")\n",
    "else:\n",
    "    print(\"Spatial evaluation not available for this sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Google Vision with spatial evaluation\n",
    "print(\"=== Testing Google Vision with Spatial Evaluation ===\")\n",
    "google_spatial_results = run_single_ocr_evaluation_with_spatial(all_samples[:2], 'google_vision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_dataframe(results: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Convert results to pandas DataFrame for analysis\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for result in results:\n",
    "        for model_name, model_result in result['ocr_results'].items():\n",
    "            row = {\n",
    "                'sample_id': result['sample_id'],\n",
    "                'dataset': result['dataset'],\n",
    "                'image_name': result['image_name'],\n",
    "                'model': model_name,\n",
    "                'processing_time': model_result.get('processing_time', 0),\n",
    "                'similarity': model_result['metrics']['similarity'],\n",
    "                'precision': model_result['metrics']['precision'],\n",
    "                'recall': model_result['metrics']['recall'],\n",
    "                'f1': model_result['metrics']['f1'],\n",
    "                'predicted_length': model_result['metrics'].get('predicted_length', 0),\n",
    "                'ground_truth_length': model_result['metrics'].get('ground_truth_length', 0),\n",
    "                'has_error': 'error' in model_result\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = create_results_dataframe(evaluation_results)\n",
    "print(f\"Results DataFrame shape: {df_results.shape}\")\n",
    "print(f\"Available models: {df_results['model'].unique()}\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by model\n",
    "summary_stats = df_results.groupby('model').agg({\n",
    "    'similarity': ['mean', 'std', 'min', 'max'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'f1': ['mean', 'std'],\n",
    "    'processing_time': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"=== OCR Model Performance Summary ===\")\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by dataset (French vs English)\n",
    "dataset_performance = df_results.groupby(['dataset', 'model']).agg({\n",
    "    'similarity': 'mean',\n",
    "    'f1': 'mean',\n",
    "    'processing_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"=== Performance by Dataset ===\")\n",
    "dataset_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. F1 Score comparison\n",
    "sns.boxplot(data=df_results, x='model', y='f1', ax=axes[0,0])\n",
    "axes[0,0].set_title('F1 Score by OCR Model')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Similarity comparison\n",
    "sns.boxplot(data=df_results, x='model', y='similarity', ax=axes[0,1])\n",
    "axes[0,1].set_title('Text Similarity by OCR Model')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Processing time comparison\n",
    "sns.boxplot(data=df_results, x='model', y='processing_time', ax=axes[1,0])\n",
    "axes[1,0].set_title('Processing Time by OCR Model')\n",
    "axes[1,0].set_ylabel('Time (seconds)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Performance by dataset\n",
    "sns.barplot(data=df_results, x='model', y='f1', hue='dataset', ax=axes[1,1])\n",
    "axes[1,1].set_title('F1 Score by Model and Dataset')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ranking and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall ranking\n",
    "model_ranking = df_results.groupby('model').agg({\n",
    "    'f1': 'mean',\n",
    "    'similarity': 'mean',\n",
    "    'precision': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'processing_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Add pricing information\n",
    "pricing_info = {\n",
    "    'tesseract': 'Free',\n",
    "    'easyocr': 'Free',\n",
    "    'paddleocr': 'Free',\n",
    "    'kraken': 'Free',\n",
    "    'google_vision': '$1.50 per 1,000 images'\n",
    "}\n",
    "\n",
    "model_ranking['pricing'] = model_ranking.index.map(pricing_info)\n",
    "\n",
    "# Sort by F1 score\n",
    "model_ranking_sorted = model_ranking.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"=== OCR Model Ranking (by F1 Score) ===\")\n",
    "model_ranking_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results report\n",
    "final_report = {\n",
    "    'evaluation_summary': {\n",
    "        'total_samples': len(all_samples),\n",
    "        'french_samples': len(french_samples),\n",
    "        'english_samples': len(english_samples),\n",
    "        'models_tested': list(df_results['model'].unique()),\n",
    "        'evaluation_date': pd.Timestamp.now().isoformat()\n",
    "    },\n",
    "    'model_performance': model_ranking_sorted.to_dict('index'),\n",
    "    'dataset_performance': dataset_performance.to_dict(),\n",
    "    'pricing_information': pricing_info,\n",
    "    'detailed_results': evaluation_results\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = 'ocr_evaluation_results.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(final_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / 1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
