{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Models Evaluation on French and English Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:40:52.782395Z",
     "iopub.status.busy": "2025-07-09T18:40:52.781624Z",
     "iopub.status.idle": "2025-07-09T18:40:52.786753Z",
     "shell.execute_reply": "2025-07-09T18:40:52.786014Z",
     "shell.execute_reply.started": "2025-07-09T18:40:52.782369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # OCR Libraries\n",
    "# import pytesseract\n",
    "# import easyocr\n",
    "\n",
    "# from google.cloud import vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install python-doctr[torch]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "doctr_available = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install surya-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade surya-ocr torch torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install surya-ocr torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:37:14.368196Z",
     "iopub.status.busy": "2025-07-09T18:37:14.367199Z",
     "iopub.status.idle": "2025-07-09T18:38:00.798413Z",
     "shell.execute_reply": "2025-07-09T18:38:00.797657Z",
     "shell.execute_reply.started": "2025-07-09T18:37:14.368161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 18:37:20.465173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752086240.644285     151 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752086240.696560     151 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Downloading text_recognition model to /root/.cache/datalab/models/text_recognition/2025_05_16: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n",
      "Downloading text_detection model to /root/.cache/datalab/models/text_detection/2025_05_07: 100%|██████████| 6/6 [00:02<00:00,  2.08it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "Recognizing Text: 100%|██████████| 43/43 [00:03<00:00, 11.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from surya.recognition import RecognitionPredictor\n",
    "from surya.detection import DetectionPredictor\n",
    "\n",
    "image = Image.open('/kaggle/input/ocr-dataset/English_OCR_dataset/English_OCR_dataset/images/0001123541.png')\n",
    "recognition_predictor = RecognitionPredictor()\n",
    "detection_predictor = DetectionPredictor()\n",
    "\n",
    "predictions = recognition_predictor([image], det_predictor=detection_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/json-data/active-sun-455914-a1-141238abf96c.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:40:42.284658Z",
     "iopub.status.busy": "2025-07-09T18:40:42.283732Z",
     "iopub.status.idle": "2025-07-09T18:40:42.288994Z",
     "shell.execute_reply": "2025-07-09T18:40:42.288138Z",
     "shell.execute_reply.started": "2025-07-09T18:40:42.284630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ENGLISH_DATASET_PATH = \"/kaggle/input/ocr-dataset/English_OCR_dataset/English_OCR_dataset\"\n",
    "FRENCH_DATASET_PATH = \"/kaggle/input/ocr-dataset/French_OCR_dataset/French_OCR_dataset\"\n",
    "\n",
    "SAMPLES_PER_DATASET = 10\n",
    "TOTAL_SAMPLES = SAMPLES_PER_DATASET * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:40:44.078234Z",
     "iopub.status.busy": "2025-07-09T18:40:44.077937Z",
     "iopub.status.idle": "2025-07-09T18:40:44.086916Z",
     "shell.execute_reply": "2025-07-09T18:40:44.086006Z",
     "shell.execute_reply.started": "2025-07-09T18:40:44.078210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_french_samples(dataset_path, num_samples):\n",
    "    samples = []\n",
    "    \n",
    "    image_files = [f for f in os.listdir(dataset_path) if f.endswith('.jpg')]\n",
    "    \n",
    "    valid_pairs = []\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        base_name = img_file.replace('_default.jpg', '')  \n",
    "        xml_file = None\n",
    "        \n",
    "        possible_xml = [\n",
    "            f\"{base_name}_default.xml\",  \n",
    "            f\"{base_name[:-1]}g_default.xml\",\n",
    "        ]\n",
    "        \n",
    "        for xml_name in possible_xml:\n",
    "            if os.path.exists(os.path.join(dataset_path, xml_name)):\n",
    "                xml_file = xml_name\n",
    "                break\n",
    "        \n",
    "        if xml_file:\n",
    "            valid_pairs.append((img_file, xml_file))\n",
    "    \n",
    "    \n",
    "    selected_pairs = random.sample(valid_pairs, min(num_samples, len(valid_pairs)))\n",
    "    \n",
    "    for img_file, xml_file in selected_pairs:\n",
    "        samples.append({\n",
    "            'dataset': 'French',\n",
    "            'image_path': os.path.join(dataset_path, img_file),\n",
    "            'annotation_path': os.path.join(dataset_path, xml_file),\n",
    "            'image_name': img_file,\n",
    "            'annotation_name': xml_file\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def get_english_samples(dataset_path, num_samples):\n",
    "    samples = []\n",
    "    \n",
    "    images_path = os.path.join(dataset_path, 'images')\n",
    "    annotations_path = os.path.join(dataset_path, 'annotations')\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith('.png')]\n",
    "    selected_files = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "    \n",
    "    for img_file in selected_files:\n",
    "        # Find corresponding json\n",
    "        base_name = img_file.replace('.png', '')\n",
    "        json_file = f\"{base_name}.json\"\n",
    "        \n",
    "        json_path = os.path.join(annotations_path, json_file)\n",
    "        if os.path.exists(json_path):\n",
    "            samples.append({\n",
    "                'dataset': 'English',\n",
    "                'image_path': os.path.join(images_path, img_file),\n",
    "                'annotation_path': json_path,\n",
    "                'image_name': img_file,\n",
    "                'annotation_name': json_file\n",
    "            })\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:40:47.348654Z",
     "iopub.status.busy": "2025-07-09T18:40:47.347888Z",
     "iopub.status.idle": "2025-07-09T18:40:47.354849Z",
     "shell.execute_reply": "2025-07-09T18:40:47.354013Z",
     "shell.execute_reply.started": "2025-07-09T18:40:47.348623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_text_from_french_xml(xml_path):\n",
    "    \"\"\"Extract ground truth text from French dataset XML file\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Find all String elements with CONTENT attribute\n",
    "        texts = []\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            if content:\n",
    "                texts.append(content)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML {xml_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_english_json(json_path):\n",
    "    \"\"\"Extract ground truth text from English dataset JSON file\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        texts = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "        \n",
    "        return ' '.join(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON {json_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:40:48.706720Z",
     "iopub.status.busy": "2025-07-09T18:40:48.706415Z",
     "iopub.status.idle": "2025-07-09T18:40:48.714219Z",
     "shell.execute_reply": "2025-07-09T18:40:48.713401Z",
     "shell.execute_reply.started": "2025-07-09T18:40:48.706697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_boxes_from_french_xml(xml_path):\n",
    "    \"\"\"Extract bounding boxes and text from French dataset XML file\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        for string_elem in root.findall('.//{http://www.loc.gov/standards/alto/ns-v4#}String'):\n",
    "            content = string_elem.get('CONTENT')\n",
    "            hpos = string_elem.get('HPOS')\n",
    "            vpos = string_elem.get('VPOS')\n",
    "            width = string_elem.get('WIDTH')\n",
    "            height = string_elem.get('HEIGHT')\n",
    "            \n",
    "            if content and hpos and vpos and width and height:\n",
    "                x1 = int(float(hpos))\n",
    "                y1 = int(float(vpos))\n",
    "                x2 = x1 + int(float(width))\n",
    "                y2 = y1 + int(float(height))\n",
    "                \n",
    "                boxes.append({\n",
    "                    'text': content,\n",
    "                    'box': [x1, y1, x2, y2]\n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML boxes {xml_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_boxes_from_english_json(json_path):\n",
    "    \"\"\"Extract bounding boxes and text from English dataset JSON file\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        boxes = []\n",
    "        for form_item in data.get('form', []):\n",
    "            text = form_item.get('text', '').strip()\n",
    "            box = form_item.get('box', [])\n",
    "            \n",
    "            if text and len(box) == 4:\n",
    "                boxes.append({\n",
    "                    'text': text,\n",
    "                    'box': box  # Already in [x1, y1, x2, y2] format\n",
    "                })\n",
    "        \n",
    "        return boxes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON boxes {json_path}: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:40:58.138077Z",
     "iopub.status.busy": "2025-07-09T18:40:58.137535Z",
     "iopub.status.idle": "2025-07-09T18:40:58.401976Z",
     "shell.execute_reply": "2025-07-09T18:40:58.401301Z",
     "shell.execute_reply.started": "2025-07-09T18:40:58.138050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "french_samples = get_french_samples(FRENCH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "english_samples = get_english_samples(ENGLISH_DATASET_PATH, SAMPLES_PER_DATASET)\n",
    "\n",
    "all_samples = french_samples + english_samples\n",
    "\n",
    "print(f\"French samples: {len(french_samples)}\")\n",
    "print(f\"English samples: {len(english_samples)}\")\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "\n",
    "for sample in all_samples:\n",
    "    if sample['dataset'] == 'French':\n",
    "        sample['ground_truth'] = extract_text_from_french_xml(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_french_xml(sample['annotation_path'])\n",
    "    else:\n",
    "        sample['ground_truth'] = extract_text_from_english_json(sample['annotation_path'])\n",
    "        sample['ground_truth_boxes'] = extract_boxes_from_english_json(sample['annotation_path'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:44:52.484895Z",
     "iopub.status.busy": "2025-07-09T18:44:52.484249Z",
     "iopub.status.idle": "2025-07-09T18:44:52.498831Z",
     "shell.execute_reply": "2025-07-09T18:44:52.498109Z",
     "shell.execute_reply.started": "2025-07-09T18:44:52.484869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "easyocr_reader = None\n",
    "google_client = None\n",
    "doctr_model = None\n",
    "surya_recognition_predictor = None\n",
    "surya_detection_predictor = None\n",
    "\n",
    "models_available = {\n",
    "    'tesseract': True,\n",
    "    'easyocr': True,\n",
    "    'surya': True,\n",
    "    'doctr': True,\n",
    "    'google_vision':True\n",
    "}\n",
    "\n",
    "def initialize_ocr_models():\n",
    "    global easyocr_reader, google_client, doctr_model, surya_recognition_predictor, surya_detection_predictor\n",
    "    \n",
    "    print(\"Initializing models\")\n",
    "    \n",
    "    try:\n",
    "        if models_available['easyocr']:\n",
    "            easyocr_reader = easyocr.Reader(['en', 'fr'])\n",
    "    except Exception as e:\n",
    "        print(f\"EasyOCR initialization failed: {e}\")\n",
    "        models_available['easyocr'] = False\n",
    "    \n",
    "    #DocTR\n",
    "    try:\n",
    "        if models_available['doctr']:\n",
    "            doctr_model = ocr_predictor(pretrained=True)\n",
    "    except Exception as e:\n",
    "        print(f\"DocTR initialization failed: {e}\")\n",
    "        models_available['doctr'] = False\n",
    "    #Surya\n",
    "    try:\n",
    "        if models_available['surya']:\n",
    "            surya_recognition_predictor = RecognitionPredictor()\n",
    "            surya_detection_predictor = DetectionPredictor()\n",
    "    except Exception as e:\n",
    "        print(f\"Surya OCR initialization failed: {e}\")\n",
    "        models_available['surya'] = False\n",
    "        \n",
    "    # Google Cloud Vision\n",
    "    try:\n",
    "        if models_available['google_vision']:\n",
    "            google_client = vision.ImageAnnotatorClient()\n",
    "    except Exception as e:\n",
    "        print(f\"Google Cloud Vision initialization failed: {e}\")\n",
    "        models_available['google_vision'] = False\n",
    "    \n",
    "    available_models = [k for k, v in models_available.items() if v]\n",
    "    return available_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tesseract_ocr(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Tesseract error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def easyocr_ocr(image_path):\n",
    "    global easyocr_reader\n",
    "    try:\n",
    "        if not easyocr_reader:\n",
    "            print(\"easyocr not initialized.\")\n",
    "            return \"\"\n",
    "        results = easyocr_reader.readtext(image_path)\n",
    "        text = ' '.join([result[1] for result in results])\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"easyocr error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "def surya_ocr(image_path):\n",
    "    global surya_recognition_predictor, surya_detection_predictor\n",
    "    try:\n",
    "        if not surya_recognition_predictor or not surya_detection_predictor:\n",
    "            print(\"Surya not initialized\")\n",
    "            return \"\"\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        predictions = surya_recognition_predictor([image], det_predictor=surya_detection_predictor)\n",
    "        \n",
    "        # Extract text from predictions\n",
    "        text_parts = []\n",
    "        for text_line in predictions[0].text_lines:\n",
    "            text_parts.append(text_line.text)\n",
    "        \n",
    "        return ' '.join(text_parts).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Surya error: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "def doctr_ocr(image_path):\n",
    "    global doctr_model\n",
    "    try:\n",
    "        if not doctr_model:\n",
    "            print(\"DocTR not initialized\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Load document\n",
    "        doc = DocumentFile.from_images(image_path)\n",
    "        \n",
    "        # Run OCR\n",
    "        result = doctr_model(doc)\n",
    "        \n",
    "        # Extract text\n",
    "        text_parts = []\n",
    "        for page in result.pages:\n",
    "            for block in page.blocks:\n",
    "                for line in block.lines:\n",
    "                    for word in line.words:\n",
    "                        text_parts.append(word.value)\n",
    "        \n",
    "        return ' '.join(text_parts).strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"doctr error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def google_vision_ocr(image_path):\n",
    "    global google_client\n",
    "    try:\n",
    "        if not google_client:\n",
    "            print(\"gg vsion not initialized\")\n",
    "            return \"\"\n",
    "        \n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        response = google_client.text_detection(image=image)\n",
    "        texts = response.text_annotations\n",
    "        \n",
    "        if texts:\n",
    "            return texts[0].description.strip()\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"gg vision error on {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run OCR eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_single_ocr_evaluation(samples, ocr_model_name):\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'tesseract': tesseract_ocr,\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'doctr':doctr_ocr,\n",
    "        'surya':surya_ocr,\n",
    "        'google_vision': google_vision_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} imgs\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate results\n",
    "        metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        print(f\"  Similarity: {metrics['similarity']:.3f}, P: {metrics['precision']:.3f}, R: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "\n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:48:24.052420Z",
     "iopub.status.busy": "2025-07-09T18:48:24.052179Z",
     "iopub.status.idle": "2025-07-09T18:48:24.071868Z",
     "shell.execute_reply": "2025-07-09T18:48:24.071166Z",
     "shell.execute_reply.started": "2025-07-09T18:48:24.052397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_single_ocr_evaluation_with_spatial(samples, ocr_model_name):\n",
    "    results = []\n",
    "    \n",
    "    ocr_functions = {\n",
    "        'easyocr': easyocr_ocr,\n",
    "        'doctr':doctr_ocr,\n",
    "        'google_vision': google_vision_ocr,\n",
    "        'surya':surya_ocr\n",
    "    }\n",
    "    \n",
    "    ocr_function = ocr_functions[ocr_model_name]\n",
    "    \n",
    "    spatial_supported = ocr_model_name in ['easyocr', 'doctr', 'google_vision','surya']\n",
    "    \n",
    "    print(f\"Running {ocr_model_name} on {len(samples)} samples\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Processing {i+1}/{len(samples)}: {sample['image_name']}\")\n",
    "        \n",
    "        sample_result = {\n",
    "            'sample_id': i,\n",
    "            'dataset': sample['dataset'],\n",
    "            'image_name': sample['image_name'],\n",
    "            'ground_truth': sample['ground_truth'],\n",
    "            'ground_truth_boxes': sample.get('ground_truth_boxes', []),\n",
    "            'ocr_result': {}\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get OCR text\n",
    "        predicted_text = ocr_function(sample['image_path'])\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Get OCR boxes\n",
    "        predicted_boxes = []\n",
    "        if spatial_supported:\n",
    "            predicted_boxes = get_ocr_boxes_with_text(ocr_model_name, sample['image_path'])\n",
    "        \n",
    "        if spatial_supported and sample.get('ground_truth_boxes'):\n",
    "            metrics = evaluate_ocr_with_spatial(\n",
    "                predicted_text, \n",
    "                sample['ground_truth'],\n",
    "                predicted_boxes,\n",
    "                sample['ground_truth_boxes']\n",
    "            )\n",
    "            evaluation_type = 'spatial'\n",
    "        else:\n",
    "            metrics = evaluate_ocr_result(predicted_text, sample['ground_truth'])\n",
    "            evaluation_type = 'text_only'\n",
    "        \n",
    "        sample_result['ocr_result'] = {\n",
    "            'model': ocr_model_name,\n",
    "            'predicted_text': predicted_text,\n",
    "            'predicted_boxes': predicted_boxes,\n",
    "            'processing_time': processing_time,\n",
    "            'metrics': metrics,\n",
    "            'evaluation_type': evaluation_type\n",
    "        }\n",
    "        \n",
    "        if evaluation_type == 'spatial':\n",
    "            print(f\"  Text F1: {metrics['text_f1']:.3f}, Detection F1: {metrics['detection_f1']:.3f}, Combined: {metrics['combined_f1']:.3f}, IoU: {metrics['avg_iou']:.3f}\")\n",
    "        else:\n",
    "            print(f\"  Similarity: {metrics['similarity']:.3f}, F1: {metrics['f1']:.3f}, Time: {processing_time:.2f}s\")\n",
    "        \n",
    "        results.append(sample_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    if x1_inter >= x2_inter or y1_inter >= y2_inter:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_ocr_boxes_with_text(ocr_model_name, image_path):\n",
    "    boxes = []\n",
    "    \n",
    "    try:\n",
    "        if ocr_model_name == 'easyocr':\n",
    "            global easyocr_reader\n",
    "            if easyocr_reader:\n",
    "                results = easyocr_reader.readtext(image_path)\n",
    "                for result in results:\n",
    "                    bbox_points = result[0]\n",
    "                    text = result[1]\n",
    "                    # Convert bbox points \n",
    "                    x_coords = [point[0] for point in bbox_points]\n",
    "                    y_coords = [point[1] for point in bbox_points]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "        \n",
    "        elif ocr_model_name == 'doctr':  \n",
    "            global doctr_model\n",
    "            if doctr_model:\n",
    "                # Load document\n",
    "                doc = DocumentFile.from_images(image_path)\n",
    "                result = doctr_model(doc)\n",
    "                \n",
    "                from PIL import Image\n",
    "                img = Image.open(image_path)\n",
    "                img_width, img_height = img.size\n",
    "                \n",
    "                for page in result.pages:\n",
    "                    for block in page.blocks:\n",
    "                        for line in block.lines:\n",
    "                            for word in line.words:\n",
    "                                # Convert \n",
    "                                bbox = word.geometry\n",
    "                                x1 = int(bbox[0][0] * img_width)\n",
    "                                y1 = int(bbox[0][1] * img_height)\n",
    "                                x2 = int(bbox[1][0] * img_width)\n",
    "                                y2 = int(bbox[1][1] * img_height)\n",
    "                                \n",
    "                                boxes.append({'text': word.value,'box': [x1, y1, x2, y2]})\n",
    "\n",
    "        elif ocr_model_name == 'surya':  \n",
    "            from PIL import Image\n",
    "            global surya_recognition_predictor, surya_detection_predictor\n",
    "            if surya_recognition_predictor and surya_detection_predictor:\n",
    "                image = Image.open(image_path)\n",
    "                predictions = surya_recognition_predictor([image], det_predictor=surya_detection_predictor)\n",
    "                \n",
    "                for text_line in predictions[0].text_lines:\n",
    "                    text = text_line.text\n",
    "                    bbox = text_line.bbox\n",
    "                    boxes.append({'text': text, 'box': bbox})\n",
    "                    \n",
    "        elif ocr_model_name == 'google_vision':\n",
    "            global google_client\n",
    "            if google_client:\n",
    "                with open(image_path, 'rb') as image_file:\n",
    "                    content = image_file.read()\n",
    "                \n",
    "                image = vision.Image(content=content)\n",
    "                response = google_client.text_detection(image=image)\n",
    "                texts = response.text_annotations\n",
    "                \n",
    "                for annotation in texts[1:]:\n",
    "                    text = annotation.description\n",
    "                    vertices = annotation.bounding_poly.vertices\n",
    "                    \n",
    "                    # Convert vertices to [x1, y1, x2, y2]\n",
    "                    x_coords = [vertex.x for vertex in vertices]\n",
    "                    y_coords = [vertex.y for vertex in vertices]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append({'text': text, 'box': [x1, y1, x2, y2]})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting boxes from {ocr_model_name}: {e}\")\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes, iou_threshold=0.5):\n",
    "    \"\"\"Evaluate spatial accuracy using bounding box matching\"\"\"\n",
    "    if not predicted_boxes or not ground_truth_boxes:\n",
    "        return {\n",
    "            'detection_precision': 0.0,\n",
    "            'detection_recall': 0.0,\n",
    "            'detection_f1': 0.0,\n",
    "            'avg_iou': 0.0,\n",
    "            'matched_pairs': 0,\n",
    "            'total_predicted': len(predicted_boxes),\n",
    "            'total_ground_truth': len(ground_truth_boxes)\n",
    "        }\n",
    "    \n",
    "    # Find best matches between predicted and ground truth boxes\n",
    "    matched_pairs = []\n",
    "    used_gt_indices = set()\n",
    "    \n",
    "    for pred_idx, pred_box in enumerate(predicted_boxes):\n",
    "        best_iou = 0.0\n",
    "        best_gt_idx = -1\n",
    "        \n",
    "        for gt_idx, gt_box in enumerate(ground_truth_boxes):\n",
    "            if gt_idx in used_gt_indices:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(pred_box['box'], gt_box['box'])\n",
    "            if iou > best_iou and iou >= iou_threshold:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "        \n",
    "        if best_gt_idx != -1:\n",
    "            matched_pairs.append({\n",
    "                'pred_idx': pred_idx,\n",
    "                'gt_idx': best_gt_idx,\n",
    "                'iou': best_iou,\n",
    "                'pred_text': pred_box['text'],\n",
    "                'gt_text': ground_truth_boxes[best_gt_idx]['text']\n",
    "            })\n",
    "            used_gt_indices.add(best_gt_idx)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    num_matches = len(matched_pairs)\n",
    "    detection_precision = num_matches / len(predicted_boxes) if predicted_boxes else 0.0\n",
    "    detection_recall = num_matches / len(ground_truth_boxes) if ground_truth_boxes else 0.0\n",
    "    detection_f1 = 2 * (detection_precision * detection_recall) / (detection_precision + detection_recall) if (detection_precision + detection_recall) > 0 else 0.0\n",
    "    avg_iou = sum(pair['iou'] for pair in matched_pairs) / num_matches if num_matches > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'detection_precision': detection_precision,\n",
    "        'detection_recall': detection_recall,\n",
    "        'detection_f1': detection_f1,\n",
    "        'avg_iou': avg_iou,\n",
    "        'matched_pairs': num_matches,\n",
    "        'total_predicted': len(predicted_boxes),\n",
    "        'total_ground_truth': len(ground_truth_boxes),\n",
    "        'matches': matched_pairs\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_with_spatial(predicted_text, ground_truth_text, predicted_boxes, ground_truth_boxes):\n",
    "    # Text based evaluation \n",
    "    text_metrics = evaluate_ocr_result(predicted_text, ground_truth_text)\n",
    "    \n",
    "    # Spatial evaluation \n",
    "    spatial_metrics = evaluate_spatial_accuracy(predicted_boxes, ground_truth_boxes)\n",
    "    \n",
    "    # Combined metrics\n",
    "    combined_metrics = {\n",
    "        # Text metrics\n",
    "        'text_similarity': text_metrics['similarity'],\n",
    "        'text_precision': text_metrics['precision'],\n",
    "        'text_recall': text_metrics['recall'],\n",
    "        'text_f1': text_metrics['f1'],\n",
    "        \n",
    "        # Spatial metrics\n",
    "        'detection_precision': spatial_metrics['detection_precision'],\n",
    "        'detection_recall': spatial_metrics['detection_recall'],\n",
    "        'detection_f1': spatial_metrics['detection_f1'],\n",
    "        'avg_iou': spatial_metrics['avg_iou'],\n",
    "        'matched_boxes': spatial_metrics['matched_pairs'],\n",
    "        'total_predicted_boxes': spatial_metrics['total_predicted'],\n",
    "        'total_ground_truth_boxes': spatial_metrics['total_ground_truth'],\n",
    "        \n",
    "        # Combined score (average of text F1 and detection F1)\n",
    "        'combined_f1': (text_metrics['f1'] + spatial_metrics['detection_f1']) / 2,\n",
    "        \n",
    "        # Additional info\n",
    "        'predicted_length': text_metrics.get('predicted_length', 0),\n",
    "        'ground_truth_length': text_metrics.get('ground_truth_length', 0),\n",
    "        'spatial_matches': spatial_metrics.get('matches', [])\n",
    "    }\n",
    "    \n",
    "    return combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    return text.lower()\n",
    "\n",
    "def calculate_similarity(text1,text2):\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "    return SequenceMatcher(None, clean1, clean2).ratio()\n",
    "\n",
    "def calculate_word_accuracy(predicted, ground_truth):\n",
    "    pred_words = set(clean_text(predicted).split())\n",
    "    gt_words = set(clean_text(ground_truth).split())\n",
    "    \n",
    "    if not gt_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    if not pred_words:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "    \n",
    "    intersection = pred_words.intersection(gt_words)\n",
    "    \n",
    "    precision = len(intersection) / len(pred_words) if pred_words else 0.0\n",
    "    recall = len(intersection) / len(gt_words) if gt_words else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def evaluate_ocr_result(predicted, ground_truth):\n",
    "    similarity = calculate_similarity(predicted, ground_truth)\n",
    "    word_metrics = calculate_word_accuracy(predicted, ground_truth)\n",
    "    \n",
    "    return {\n",
    "        'similarity': similarity,\n",
    "        'precision': word_metrics['precision'],\n",
    "        'recall': word_metrics['recall'],\n",
    "        'f1': word_metrics['f1'],\n",
    "        'predicted_length': len(predicted),\n",
    "        'ground_truth_length': len(ground_truth)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:45:03.227343Z",
     "iopub.status.busy": "2025-07-09T18:45:03.227035Z",
     "iopub.status.idle": "2025-07-09T18:45:05.536758Z",
     "shell.execute_reply": "2025-07-09T18:45:05.535985Z",
     "shell.execute_reply.started": "2025-07-09T18:45:03.227323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OCR models...\n",
      "✗ EasyOCR initialization failed: name 'easyocr' is not defined\n",
      "✗ DocTR initialization failed: name 'ocr_predictor' is not defined\n",
      "✓ Surya OCR initialized\n",
      "✗ Google Cloud Vision initialization failed: name 'vision' is not defined\n",
      "\n",
      "Available models: ['tesseract', 'surya']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tesseract', 'surya']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_ocr_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:46:03.505956Z",
     "iopub.status.busy": "2025-07-09T18:46:03.505674Z",
     "iopub.status.idle": "2025-07-09T18:48:24.030970Z",
     "shell.execute_reply": "2025-07-09T18:48:24.030148Z",
     "shell.execute_reply.started": "2025-07-09T18:46:03.505935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running surya on 20 imgs\n",
      "Processing 1/20: 12_0bb90_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:06<00:00, 17.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.335, P: 0.817, R: 0.801, F1: 0.809, Time: 8.89s\n",
      "Processing 2/20: 14_e26ee_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Recognizing Text: 100%|██████████| 127/127 [00:07<00:00, 16.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.140, P: 0.976, R: 0.971, F1: 0.974, Time: 9.24s\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Recognizing Text: 100%|██████████| 22/22 [00:03<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.272, P: 0.557, R: 0.576, F1: 0.567, Time: 5.05s\n",
      "Processing 4/20: 8_5eeab_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "Recognizing Text: 100%|██████████| 120/120 [00:07<00:00, 16.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.347, P: 0.777, R: 0.770, F1: 0.774, Time: 8.66s\n",
      "Processing 5/20: 10_10fe5_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Recognizing Text: 100%|██████████| 125/125 [00:07<00:00, 17.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.366, P: 0.821, R: 0.802, F1: 0.811, Time: 8.60s\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Recognizing Text: 100%|██████████| 115/115 [00:08<00:00, 13.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.228, P: 0.937, R: 0.952, F1: 0.944, Time: 10.07s\n",
      "Processing 7/20: 17_24e64_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Recognizing Text: 100%|██████████| 123/123 [00:08<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.168, P: 0.944, R: 0.959, F1: 0.952, Time: 9.78s\n",
      "Processing 8/20: 11_ef202_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Recognizing Text: 100%|██████████| 124/124 [00:06<00:00, 18.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.233, P: 0.814, R: 0.812, F1: 0.813, Time: 8.20s\n",
      "Processing 9/20: 5_1df9d_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:08<00:00, 14.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.225, P: 0.837, R: 0.812, F1: 0.824, Time: 9.94s\n",
      "Processing 10/20: 18_ac1da_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:09<00:00, 13.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.165, P: 0.918, R: 0.931, F1: 0.925, Time: 11.39s\n",
      "Processing 11/20: 0060080406.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\n",
      "Recognizing Text: 100%|██████████| 76/76 [00:07<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.062, P: 0.662, R: 0.718, F1: 0.689, Time: 8.04s\n",
      "Processing 12/20: 0001477983.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "Recognizing Text: 100%|██████████| 42/42 [00:03<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.185, P: 0.856, R: 0.896, F1: 0.876, Time: 4.22s\n",
      "Processing 13/20: 88547278_88547279.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "Recognizing Text: 100%|██████████| 56/56 [00:08<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.471, P: 0.545, R: 0.808, F1: 0.651, Time: 8.92s\n",
      "Processing 14/20: 92094746.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Recognizing Text: 100%|██████████| 35/35 [00:02<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.060, P: 0.703, R: 0.735, F1: 0.719, Time: 2.89s\n",
      "Processing 15/20: 0001239897.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.01it/s]\n",
      "Recognizing Text: 100%|██████████| 56/56 [00:03<00:00, 17.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.048, P: 0.494, R: 0.472, F1: 0.483, Time: 3.46s\n",
      "Processing 16/20: 00836816.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "Recognizing Text: 100%|██████████| 36/36 [00:04<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.238, P: 0.699, R: 0.711, F1: 0.705, Time: 4.50s\n",
      "Processing 17/20: 00860012_00860014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
      "Recognizing Text: 100%|██████████| 68/68 [00:04<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.024, P: 0.722, R: 0.712, F1: 0.717, Time: 4.81s\n",
      "Processing 18/20: 71108371.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\n",
      "Recognizing Text: 100%|██████████| 63/63 [00:04<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.236, P: 0.915, R: 0.910, F1: 0.912, Time: 4.74s\n",
      "Processing 19/20: 01122115.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\n",
      "Recognizing Text: 100%|██████████| 62/62 [00:03<00:00, 16.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.124, P: 0.683, R: 0.658, F1: 0.670, Time: 4.14s\n",
      "Processing 20/20: 00836244.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Recognizing Text: 100%|██████████| 87/87 [00:04<00:00, 19.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Similarity: 0.094, P: 0.532, R: 0.517, F1: 0.524, Time: 4.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "surya_results = run_single_ocr_evaluation(all_samples,'surya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:50:15.922099Z",
     "iopub.status.busy": "2025-07-09T18:50:15.921811Z",
     "iopub.status.idle": "2025-07-09T18:54:56.753591Z",
     "shell.execute_reply": "2025-07-09T18:54:56.752555Z",
     "shell.execute_reply.started": "2025-07-09T18:50:15.922080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running surya on 20 samples...\n",
      "Processing 1/20: 12_0bb90_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Recognizing Text:   0%|          | 0/122 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:   1%|          | 1/122 [00:04<09:39,  4.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:   6%|▌         | 7/122 [00:05<01:01,  1.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:   7%|▋         | 9/122 [00:05<00:45,  2.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  12%|█▏        | 15/122 [00:05<00:19,  5.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  15%|█▍        | 18/122 [00:05<00:14,  6.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  17%|█▋        | 21/122 [00:05<00:11,  8.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  20%|█▉        | 24/122 [00:05<00:08, 10.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  22%|██▏       | 27/122 [00:05<00:07, 13.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  25%|██▌       | 31/122 [00:05<00:05, 16.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  31%|███       | 38/122 [00:05<00:03, 25.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  35%|███▌      | 43/122 [00:06<00:02, 26.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  39%|███▊      | 47/122 [00:06<00:02, 28.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  44%|████▍     | 54/122 [00:06<00:01, 35.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  58%|█████▊    | 71/122 [00:06<00:00, 63.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  78%|███████▊  | 95/122 [00:06<00:00, 101.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  90%|█████████ | 110/122 [00:06<00:00, 109.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:06<00:00, 17.60it/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Detecting bboxes:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Recognizing Text:   0%|          | 0/122 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:   1%|          | 1/122 [00:04<09:40,  4.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:   6%|▌         | 7/122 [00:05<01:01,  1.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:   7%|▋         | 9/122 [00:05<00:45,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  12%|█▏        | 15/122 [00:05<00:19,  5.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  15%|█▍        | 18/122 [00:05<00:14,  6.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  17%|█▋        | 21/122 [00:05<00:11,  8.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  20%|█▉        | 24/122 [00:05<00:08, 10.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  22%|██▏       | 27/122 [00:05<00:07, 13.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  25%|██▌       | 31/122 [00:05<00:05, 16.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  31%|███       | 38/122 [00:06<00:03, 25.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  35%|███▌      | 43/122 [00:06<00:02, 26.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  39%|███▊      | 47/122 [00:06<00:02, 28.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  44%|████▍     | 54/122 [00:06<00:01, 35.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  58%|█████▊    | 71/122 [00:06<00:00, 63.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  78%|███████▊  | 95/122 [00:06<00:00, 101.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text:  90%|█████████ | 110/122 [00:06<00:00, 109.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:06<00:00, 17.58it/s] \u001b[A\u001b[A\n",
      "Recognizing Text:   0%|          | 0/22 [01:20<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.809, Detection F1: 0.343, Combined: 0.576, IoU: 0.569\n",
      "Processing 2/20: 14_e26ee_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Recognizing Text: 100%|██████████| 127/127 [00:07<00:00, 16.52it/s] \n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Recognizing Text: 100%|██████████| 127/127 [00:07<00:00, 16.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.974, Detection F1: 0.768, Combined: 0.871, IoU: 0.622\n",
      "Processing 3/20: 2_f1a3f_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Recognizing Text: 100%|██████████| 22/22 [00:03<00:00,  5.80it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Recognizing Text: 100%|██████████| 22/22 [00:03<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.567, Detection F1: 0.622, Combined: 0.594, IoU: 0.594\n",
      "Processing 4/20: 8_5eeab_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Recognizing Text: 100%|██████████| 120/120 [00:07<00:00, 16.89it/s] \n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Recognizing Text: 100%|██████████| 120/120 [00:07<00:00, 16.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.774, Detection F1: 0.492, Combined: 0.633, IoU: 0.569\n",
      "Processing 5/20: 10_10fe5_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Recognizing Text: 100%|██████████| 125/125 [00:07<00:00, 17.59it/s] \n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Recognizing Text: 100%|██████████| 125/125 [00:07<00:00, 17.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.811, Detection F1: 0.258, Combined: 0.535, IoU: 0.551\n",
      "Processing 6/20: 16_e8fb7_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Recognizing Text: 100%|██████████| 115/115 [00:08<00:00, 13.33it/s] \n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Recognizing Text: 100%|██████████| 115/115 [00:08<00:00, 13.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.944, Detection F1: 0.678, Combined: 0.811, IoU: 0.558\n",
      "Processing 7/20: 17_24e64_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Recognizing Text: 100%|██████████| 123/123 [00:08<00:00, 14.96it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Recognizing Text: 100%|██████████| 123/123 [00:08<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.952, Detection F1: 0.854, Combined: 0.903, IoU: 0.619\n",
      "Processing 8/20: 11_ef202_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Recognizing Text: 100%|██████████| 124/124 [00:06<00:00, 18.34it/s] \n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Recognizing Text: 100%|██████████| 124/124 [00:06<00:00, 18.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.813, Detection F1: 0.178, Combined: 0.495, IoU: 0.557\n",
      "Processing 9/20: 5_1df9d_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:08<00:00, 14.57it/s] \n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:08<00:00, 14.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.824, Detection F1: 0.813, Combined: 0.818, IoU: 0.628\n",
      "Processing 10/20: 18_ac1da_default.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:09<00:00, 13.02it/s] \n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Recognizing Text: 100%|██████████| 122/122 [00:09<00:00, 13.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.925, Detection F1: 0.959, Combined: 0.942, IoU: 0.662\n",
      "Processing 11/20: 0060080406.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.67it/s]\n",
      "Recognizing Text: 100%|██████████| 76/76 [00:07<00:00,  9.80it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\n",
      "Recognizing Text: 100%|██████████| 76/76 [00:07<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.689, Detection F1: 0.622, Combined: 0.656, IoU: 0.704\n",
      "Processing 12/20: 0001477983.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Recognizing Text: 100%|██████████| 42/42 [00:03<00:00, 10.74it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.86it/s]\n",
      "Recognizing Text: 100%|██████████| 42/42 [00:03<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.876, Detection F1: 0.447, Combined: 0.661, IoU: 0.731\n",
      "Processing 13/20: 88547278_88547279.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.96it/s]\n",
      "Recognizing Text: 100%|██████████| 56/56 [00:08<00:00,  6.50it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "Recognizing Text: 100%|██████████| 56/56 [00:08<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.651, Detection F1: 0.521, Combined: 0.586, IoU: 0.733\n",
      "Processing 14/20: 92094746.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "Recognizing Text: 100%|██████████| 35/35 [00:02<00:00, 13.39it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "Recognizing Text: 100%|██████████| 35/35 [00:02<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.719, Detection F1: 0.194, Combined: 0.456, IoU: 0.657\n",
      "Processing 15/20: 0001239897.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n",
      "Recognizing Text: 100%|██████████| 56/56 [00:03<00:00, 17.64it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n",
      "Recognizing Text: 100%|██████████| 56/56 [00:03<00:00, 17.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.483, Detection F1: 0.621, Combined: 0.552, IoU: 0.771\n",
      "Processing 16/20: 00836816.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "Recognizing Text: 100%|██████████| 36/36 [00:04<00:00,  8.59it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n",
      "Recognizing Text: 100%|██████████| 36/36 [00:04<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.705, Detection F1: 0.585, Combined: 0.645, IoU: 0.729\n",
      "Processing 17/20: 00860012_00860014.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "Recognizing Text: 100%|██████████| 68/68 [00:04<00:00, 15.14it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Recognizing Text: 100%|██████████| 68/68 [00:04<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.717, Detection F1: 0.744, Combined: 0.731, IoU: 0.717\n",
      "Processing 18/20: 71108371.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.79it/s]\n",
      "Recognizing Text: 100%|██████████| 63/63 [00:04<00:00, 14.21it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\n",
      "Recognizing Text: 100%|██████████| 63/63 [00:04<00:00, 14.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.912, Detection F1: 0.694, Combined: 0.803, IoU: 0.757\n",
      "Processing 19/20: 01122115.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.43it/s]\n",
      "Recognizing Text: 100%|██████████| 62/62 [00:03<00:00, 16.21it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "Recognizing Text: 100%|██████████| 62/62 [00:03<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.670, Detection F1: 0.179, Combined: 0.425, IoU: 0.557\n",
      "Processing 20/20: 00836244.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
      "Recognizing Text: 100%|██████████| 87/87 [00:04<00:00, 19.56it/s]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\n",
      "Recognizing Text: 100%|██████████| 87/87 [00:04<00:00, 19.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text F1: 0.524, Detection F1: 0.859, Combined: 0.692, IoU: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spatial_surya_results = run_single_ocr_evaluation_with_spatial(all_samples,'surya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "doctr_results = run_single_ocr_evaluation(all_samples, 'doctr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spatial_doctr_results = run_single_ocr_evaluation_with_spatial(all_samples,'doctr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "easyocr_results = run_single_ocr_evaluation(all_samples, 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spatial_easyocr_results = run_single_ocr_evaluation_with_spatial(all_samples, 'easyocr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tesseract_results = run_single_ocr_evaluation(all_samples, 'tesseract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gg_vision_results = run_single_ocr_evaluation(all_samples,'google_vision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spatial_gg_vision_results = run_single_ocr_evaluation_with_spatial(all_samples,'google_vision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T18:56:38.439791Z",
     "iopub.status.busy": "2025-07-09T18:56:38.439223Z",
     "iopub.status.idle": "2025-07-09T18:56:38.508440Z",
     "shell.execute_reply": "2025-07-09T18:56:38.507875Z",
     "shell.execute_reply.started": "2025-07-09T18:56:38.439750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert all results\n",
    "all_ocr_results = {\n",
    "    # 'easyocr': easyocr_results,\n",
    "    # 'tesseract': tesseract_results,\n",
    "    # 'google_vision': gg_vision_results, \n",
    "    # 'spatial_easyocr': spatial_easyocr_results,\n",
    "    # 'spatial_google_vision': spatial_gg_vision_results,\n",
    "    # 'doctr': doctr_results,\n",
    "    # 'spatial_doctr': spatial_doctr_results\n",
    "    'surya':surya_results,\n",
    "    'spatial_surya':spatial_surya_results\n",
    "}\n",
    "\n",
    "converted_results = convert_numpy_types(all_ocr_results)\n",
    "\n",
    "with open('all_ocr_results_surya.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(converted_results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display resuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_metrics(json_file_path, output_csv_path=None):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_name, samples in data.items():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "        \n",
    "        # Initialize metric lists\n",
    "        metrics_lists = {\n",
    "            'similarity': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': [],\n",
    "            'predicted_length': [],\n",
    "            'ground_truth_length': [],\n",
    "            'processing_time': [],\n",
    "            'detection_precision': [],\n",
    "            'detection_recall': [],\n",
    "            'detection_f1': [],\n",
    "            'avg_iou': []\n",
    "        }\n",
    "        \n",
    "        # Collect metrics\n",
    "        for sample in samples:\n",
    "            metrics = sample['ocr_result']['metrics']\n",
    "            processing_time = sample['ocr_result']['processing_time']\n",
    "            \n",
    "            # Add processing time\n",
    "            metrics_lists['processing_time'].append(processing_time)\n",
    "            \n",
    "            # text metrics \n",
    "            similarity = metrics.get('similarity', metrics.get('text_similarity'))\n",
    "            precision = metrics.get('precision', metrics.get('text_precision'))\n",
    "            recall = metrics.get('recall', metrics.get('text_recall'))\n",
    "            f1 = metrics.get('f1', metrics.get('text_f1'))\n",
    "            \n",
    "            if similarity is not None:\n",
    "                metrics_lists['similarity'].append(similarity)\n",
    "            if precision is not None:\n",
    "                metrics_lists['precision'].append(precision)\n",
    "            if recall is not None:\n",
    "                metrics_lists['recall'].append(recall)\n",
    "            if f1 is not None:\n",
    "                metrics_lists['f1'].append(f1)\n",
    "            \n",
    "            if 'predicted_length' in metrics:\n",
    "                metrics_lists['predicted_length'].append(metrics['predicted_length'])\n",
    "            if 'ground_truth_length' in metrics:\n",
    "                metrics_lists['ground_truth_length'].append(metrics['ground_truth_length'])\n",
    "            \n",
    "            # Detection metrics \n",
    "            for metric_name in ['detection_precision', 'detection_recall', 'detection_f1', 'avg_iou']:\n",
    "                if metric_name in metrics:\n",
    "                    metrics_lists[metric_name].append(metrics[metric_name])\n",
    "        \n",
    "        # Calculate averages \n",
    "        result = {'model': model_name, 'sample_count': len(samples)}\n",
    "        \n",
    "        for metric_name, values in metrics_lists.items():\n",
    "            if len(values) > 0:  \n",
    "                avg_value = sum(values) / len(values)\n",
    "                result[f'avg_{metric_name}'] = round(avg_value, 4)\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    df = df.sort_values('model').reset_index(drop=True)\n",
    "    \n",
    "    if output_csv_path:\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Results saved to {output_csv_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary_table(df):\n",
    "    text_df = df[~df['model'].str.contains('spatial', case=False)].copy()\n",
    "    text_cols = ['model', 'avg_similarity', 'avg_precision', 'avg_recall', 'avg_f1', 'avg_processing_time']\n",
    "    available_text_cols = [col for col in text_cols if col in text_df.columns]\n",
    "    text_table = text_df[available_text_cols].copy()\n",
    "    \n",
    "    spatial_df = df[df['model'].str.contains('spatial', case=False)].copy()\n",
    "    \n",
    "    if len(spatial_df) > 0:\n",
    "        detection_cols = ['model', 'avg_detection_precision', 'avg_detection_recall', 'avg_detection_f1', 'avg_avg_iou']\n",
    "        available_detection_cols = [col for col in detection_cols if col in spatial_df.columns]\n",
    "        spatial_detection_df = spatial_df[available_detection_cols].copy()\n",
    "    else:\n",
    "        spatial_detection_df = None\n",
    "    \n",
    "    return text_table, spatial_detection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing easyocr...\n",
      "Processing tesseract...\n",
      "Processing google_vision...\n",
      "Processing spatial_easyocr...\n",
      "Processing spatial_google_vision...\n",
      "Processing doctr...\n",
      "Processing spatial_doctr...\n",
      "Processing surya...\n",
      "Processing spatial_surya...\n",
      "Results saved to ocr_average_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "df = calculate_average_metrics(\"/home/duyle/Documents/Case-Study2/combined_ocr_results.json\", \"ocr_average_metrics.csv\")\n",
    "\n",
    "# Display summary\n",
    "text, spatial = display_summary_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_similarity</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>avg_processing_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doctr</td>\n",
       "      <td>0.2471</td>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>0.7780</td>\n",
       "      <td>11.3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>easyocr</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>0.5926</td>\n",
       "      <td>0.5945</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>5.1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google_vision</td>\n",
       "      <td>0.4560</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>0.8015</td>\n",
       "      <td>0.8025</td>\n",
       "      <td>0.6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>surya</td>\n",
       "      <td>0.2011</td>\n",
       "      <td>0.7606</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>0.7669</td>\n",
       "      <td>7.0168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tesseract</td>\n",
       "      <td>0.3203</td>\n",
       "      <td>0.6325</td>\n",
       "      <td>0.6487</td>\n",
       "      <td>0.6380</td>\n",
       "      <td>4.3286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  avg_similarity  avg_precision  avg_recall  avg_f1  \\\n",
       "0          doctr          0.2471         0.7736      0.7857  0.7780   \n",
       "1        easyocr          0.1811         0.5926      0.5945  0.5930   \n",
       "2  google_vision          0.4560         0.8065      0.8015  0.8025   \n",
       "7          surya          0.2011         0.7606      0.7762  0.7669   \n",
       "8      tesseract          0.3203         0.6325      0.6487  0.6380   \n",
       "\n",
       "   avg_processing_time  \n",
       "0              11.3139  \n",
       "1               5.1605  \n",
       "2               0.6742  \n",
       "7               7.0168  \n",
       "8               4.3286  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>avg_detection_precision</th>\n",
       "      <th>avg_detection_recall</th>\n",
       "      <th>avg_detection_f1</th>\n",
       "      <th>avg_avg_iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spatial_doctr</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.1501</td>\n",
       "      <td>0.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spatial_easyocr</td>\n",
       "      <td>0.2321</td>\n",
       "      <td>0.4128</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>0.6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spatial_google_vision</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.3493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spatial_surya</td>\n",
       "      <td>0.5751</td>\n",
       "      <td>0.5777</td>\n",
       "      <td>0.5716</td>\n",
       "      <td>0.6491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  avg_detection_precision  avg_detection_recall  \\\n",
       "3          spatial_doctr                   0.1034                0.3042   \n",
       "4        spatial_easyocr                   0.2321                0.4128   \n",
       "5  spatial_google_vision                   0.0405                0.1588   \n",
       "6          spatial_surya                   0.5751                0.5777   \n",
       "\n",
       "   avg_detection_f1  avg_avg_iou  \n",
       "3            0.1501       0.6706  \n",
       "4            0.2860       0.6900  \n",
       "5            0.0635       0.3493  \n",
       "6            0.5716       0.6491  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spatial"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7834418,
     "sourceId": 12421308,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7834439,
     "sourceId": 12421338,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
